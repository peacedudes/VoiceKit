//
//  VoiceIO.swift
//  VoiceKitCore
//
//  Generated by GPT-5 (OpenAI) â€” collaborator: robert
//  date: 09-15-2025
//
//  Notes:
//  - Public API is @MainActor to align with UI callbacks and AVFoundation/Speech usage.
//  - RealVoiceIO and ScriptedVoiceIO are @MainActor.
//  - AVSpeechSynthesizer delegate avoids sending AVSpeechUtterance across actors (uses ObjectIdentifier + copied String).
//  - Audio tap avoids capturing @MainActor self directly; uses a @unchecked Sendable LevelSink to post level updates to main safely.
//  - Boosted playback completion is driven by AVAudioPlayerNode callbacks or timeout and resumed on the main actor (no polling).
//  - Permission requests use nonisolated async bridges to avoid passing @MainActor closures into TCC/AVF (prevents libdispatch queue assertions).
//

import Foundation
import CoreGraphics
@preconcurrency import AVFoundation
#if canImport(AVFAudio)
@preconcurrency import AVFAudio
#endif
@preconcurrency import Speech
import Accelerate
import QuartzCore

// MARK: - Public types

public struct VoiceResult: Sendable {
    public let transcript: String
    public let recordingURL: URL?
    public init(transcript: String, recordingURL: URL?) {
        self.transcript = transcript
        self.recordingURL = recordingURL
    }
}

// Entire API is main-actor isolated.
@MainActor
public protocol VoiceIO: AnyObject {
    var onListeningChanged: ((Bool) -> Void)? { get set }
    var onTranscriptChanged: ((String) -> Void)? { get set }
    var onLevelChanged: ((CGFloat) -> Void)? { get set }
    var onTTSSpeakingChanged: ((Bool) -> Void)? { get set }
    var onTTSPulse: ((CGFloat) -> Void)? { get set }
    var onStatusMessageChanged: ((String?) -> Void)? { get set }

    func ensurePermissions() async throws
    func configureSessionIfNeeded() async throws

    func speak(_ text: String) async
    func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult

    func prepareBoosted(url: URL, gainDB: Float) async throws
    func startPreparedBoosted() async throws
    func playBoosted(url: URL, gainDB: Float) async throws

    func stopAll()
    func hardReset()
}

@MainActor
public extension VoiceIO {
    func prepareBoosted(url: URL) async throws { try await prepareBoosted(url: url, gainDB: 0) }
    func playBoosted(url: URL) async throws { try await playBoosted(url: url, gainDB: 0) }
    func queueSFX(url: URL, gainDB: Float = 0) async throws { try await prepareBoosted(url: url, gainDB: gainDB) }

    func listen(timeout: TimeInterval,
                inactivity: TimeInterval,
                record: Bool,
                context: RecognitionContext = .init()) async throws -> VoiceResult {
        if let real = self as? RealVoiceIO {
            real.setRecognitionContext(context)
        }
        return try await listen(timeout: timeout, inactivity: inactivity, record: record)
    }
}

public struct SimpleError: LocalizedError, Sendable {
    public var message: String
    public init(_ message: String) { self.message = message }
    public var errorDescription: String? { message }
}

// MARK: - TTS models

public struct TTSVoiceInfo: Identifiable, Hashable, Codable, Sendable {
    public let id: String
    public let name: String
    public let language: String
    public init(id: String, name: String, language: String) {
        self.id = id; self.name = name; self.language = language
    }
}

public struct TTSVoiceProfile: Identifiable, Hashable, Codable, Sendable {
    public let id: String
    public var displayName: String
    public var rate: Float
    public var pitch: Float
    public var volume: Float
    public var isSelected: Bool
    public var isHidden: Bool
    public init(id: String, displayName: String, rate: Float, pitch: Float, volume: Float, isSelected: Bool = false, isHidden: Bool = false) {
        self.id = id; self.displayName = displayName; self.rate = rate; self.pitch = pitch; self.volume = volume
        self.isSelected = isSelected; self.isHidden = isHidden
    }
}

public struct TTSMasterControl: Codable, Sendable {
    public var volume: Float
    public var pitchVariation: Float
    public var rateVariation: Float
    public init(volume: Float = 1.0, pitchVariation: Float = 0.0001, rateVariation: Float = 0.0001) {
        self.volume = volume; self.pitchVariation = pitchVariation; self.rateVariation = rateVariation
    }
}

@MainActor
public protocol TTSConfigurable: AnyObject {
    func availableVoices() -> [TTSVoiceInfo]
    func setVoiceProfile(_ profile: TTSVoiceProfile)
    func getVoiceProfile(id: String) -> TTSVoiceProfile?
    func setDefaultVoiceProfile(_ profile: TTSVoiceProfile)
    func getDefaultVoiceProfile() -> TTSVoiceProfile?
    func setMasterControl(_ master: TTSMasterControl)
    func getMasterControl() -> TTSMasterControl
    func speak(_ text: String, using voiceID: String?) async
    func stopSpeakingNow()
}

// MARK: - Recognition context

public struct RecognitionContext: Sendable {
    public enum Expectation: Sendable {
        case freeform
        case name(allowed: [String])
        case number
    }
    public var expectation: Expectation
    public init(expectation: Expectation = .freeform) { self.expectation = expectation }
}

public extension RecognitionContext {
    static var numericContextualStrings: [String] {
        let digits = (0...20).map { String($0) } + ["30","40","50","60","70","80","90","100"]
        let words = ["zero","one","two","three","four","five","six","seven","eight","nine",
                     "ten","eleven","twelve","thirteen","fourteen","fifteen",
                     "sixteen","seventeen","eighteen","nineteen","twenty",
                     "thirty","forty","fifty","sixty","seventy","eighty","ninety","hundred"]
        return digits + words
    }
}

// MARK: - Gate

public actor VoiceOpGate {
    private var busy = false
    public init() {}
    public func acquire() async {
        while busy { try? await Task.sleep(nanoseconds: 200_000) }
        busy = true
    }
    public func release() async { busy = false }
    public func forceClear() async { busy = false }
}

// MARK: - BoostQueue

private actor BoostQueue {
    private var scheduled = 0
    private var completed = 0
    func scheduleOne() { scheduled &+= 1 }
    func completeOne() { completed &+= 1 }
    func hasNothing() -> Bool { scheduled == 0 }
    func isDone() -> Bool { scheduled > 0 && completed >= scheduled }
    func reset() { scheduled = 0; completed = 0 }
}

// MARK: - permission bridge (nonisolated async wrappers)

private enum PermissionBridge {

    nonisolated static func awaitSpeechAuth() async -> SFSpeechRecognizerAuthorizationStatus {
        await withCheckedContinuation { (c: CheckedContinuation<SFSpeechRecognizerAuthorizationStatus, Never>) in
            SFSpeechRecognizer.requestAuthorization { status in
                c.resume(returning: status)
            }
        }
    }

    #if os(iOS)
    nonisolated static func awaitMicPermission() async -> Bool {
        await withCheckedContinuation { (c: CheckedContinuation<Bool, Never>) in
            AVAudioApplication.requestRecordPermission { granted in
                c.resume(returning: granted)
            }
        }
    }
    #elseif os(macOS)
    nonisolated static func awaitMicPermission() async -> Bool {
        await withCheckedContinuation { (c: CheckedContinuation<Bool, Never>) in
            AVCaptureDevice.requestAccess(for: .audio) { granted in
                c.resume(returning: granted)
            }
        }
    }
    #endif
}

// MARK: - RealVoiceIO

@MainActor
public final class RealVoiceIO: NSObject, VoiceIO, TTSConfigurable {

    public struct Config: Sendable {
        public var trimPrePad: Double = 0.02
        public var trimPostPad: Double = 0.50
        public var ttsSuppressAfterFinish: Double = 0.25
        public var boostedWaitTimeoutSeconds: Double = 3.0
        public init(trimPrePad: Double = 0.02, trimPostPad: Double = 0.50, ttsSuppressAfterFinish: Double = 0.25, boostedWaitTimeoutSeconds: Double = 3.0) {
            self.trimPrePad = trimPrePad
            self.trimPostPad = trimPostPad
            self.ttsSuppressAfterFinish = ttsSuppressAfterFinish
            self.boostedWaitTimeoutSeconds = boostedWaitTimeoutSeconds
        }
    }
    private let config: Config

    public var onListeningChanged: ((Bool) -> Void)?
    public var onTranscriptChanged: ((String) -> Void)?
    public var onLevelChanged: ((CGFloat) -> Void)?
    public var onTTSSpeakingChanged: ((Bool) -> Void)?
    public var onTTSPulse: ((CGFloat) -> Void)?
    public var onStatusMessageChanged: ((String?) -> Void)?

    private let synthesizer = AVSpeechSynthesizer()
    private var speakContinuations: [ObjectIdentifier: CheckedContinuation<Void, Never>] = [:]
    private lazy var speechDelegate: SpeechDelegate = SpeechDelegate(owner: self)

    private var profilesByID: [String: TTSVoiceProfile] = [:]
    private var defaultProfile: TTSVoiceProfile?
    private var master: TTSMasterControl = .init()

    private var ttsPulseTask: Task<Void, Never>?
    private var ttsPhase: CGFloat = 0
    private var ttsGlow: CGFloat = 0

    private let speechRecognizer = SFSpeechRecognizer(locale: .autoupdatingCurrent)
    private var audioEngine: AVAudioEngine?
    private var tapInstalled = false
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var hasFinishedRecognition = false

    private var recognitionContext: RecognitionContext = .init()

    private var firstSpeechStart: Double?
    private var lastSpeechEnd: Double?
    private var latestTranscript: String = ""

    private var boostEngine: AVAudioEngine?
    private var boostPlayer: AVAudioPlayerNode?
    private var boostEQ: AVAudioUnitEQ?
    private var autoStartPreparedAfterTTS = false
    private let boostQueue = BoostQueue()

    // Waiters for boosted playback completion (resumed on main)
    private var boostWaiters: [UUID: CheckedContinuation<Void, Never>] = [:]

    private let opGate = VoiceOpGate()
    private var ttsSuppressUntil: CFTimeInterval = 0

    private var listenCont: CheckedContinuation<VoiceResult, Error>?
    private var listenOutURL: URL?
    private var listenOutFile: AVAudioFile?
    private var finishQueued: VoiceResult?

    private var listenOverallTask: Task<Void, Never>?
    private var listenInactivityTask: Task<Void, Never>?

    private var wasInterrupted = false
    private var wasPlayingBoost = false

    public init(config: Config = Config()) {
        self.config = config
        super.init()
        synthesizer.delegate = speechDelegate
        installInterruptionObservers()
    }

    deinit {
        #if os(iOS)
        NotificationCenter.default.removeObserver(self)
        #endif
    }

    public func ensurePermissions() async throws {
        let micOK = await PermissionBridge.awaitMicPermission()
        guard micOK else { throw SimpleError("Microphone permission denied.") }

        let speechAuth = await PermissionBridge.awaitSpeechAuth()
        guard speechAuth == .authorized else { throw SimpleError("Speech permission denied.") }
        guard speechRecognizer?.isAvailable == true else { throw SimpleError("Speech recognizer unavailable.") }
    }

    public func configureSessionIfNeeded() async throws {
        #if os(iOS)
        let s = AVAudioSession.sharedInstance()
        try? s.setActive(false, options: [])
        var opts: AVAudioSession.CategoryOptions = [.defaultToSpeaker, .duckOthers]
        opts.insert(.allowBluetoothHFP)
        try s.setCategory(.playAndRecord, mode: .voiceChat, options: opts)
        try s.setPreferredSampleRate(44100)
        try s.setPreferredIOBufferDuration(0.01)
        try s.setActive(true, options: [])
        #endif
    }

    public func availableVoices() -> [TTSVoiceInfo] {
        AVSpeechSynthesisVoice.speechVoices().map { TTSVoiceInfo(id: $0.identifier, name: $0.name, language: $0.language) }
    }
    public func setVoiceProfile(_ profile: TTSVoiceProfile) { profilesByID[profile.id] = profile }
    public func getVoiceProfile(id: String) -> TTSVoiceProfile? { profilesByID[id] }
    public func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { profilesByID[profile.id] = profile; defaultProfile = profile }
    public func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
    public func setMasterControl(_ master: TTSMasterControl) { self.master = master }
    public func getMasterControl() -> TTSMasterControl { master }
    public func stopSpeakingNow() { synthesizer.stopSpeaking(at: .immediate) }

    public func setRecognitionContext(_ ctx: RecognitionContext) { recognitionContext = ctx }

    private func ttsStartPulse() {
        onTTSSpeakingChanged?(true)
        ttsPulseTask?.cancel()
        ttsPhase = 0
        ttsGlow = 0
        ttsPulseTask = Task { [weak self] in
            while let self, !Task.isCancelled {
                await MainActor.run {
                    self.ttsPhase += 0.14
                    if self.ttsPhase > .pi * 2 { self.ttsPhase -= .pi * 2 }
                    let base = (sin(self.ttsPhase) + 1) / 2
                    let baseAmp = 0.22 + 0.28 * base
                    self.ttsGlow *= 0.78
                    let level = min(1, max(0, baseAmp + self.ttsGlow))
                    self.onTTSPulse?(level)
                }
                try? await Task.sleep(nanoseconds: 33_333_333)
            }
        }
    }
    private func ttsStopPulse() {
        ttsPulseTask?.cancel()
        ttsPulseTask = nil
        ttsGlow = 0
        onTTSPulse?(0.0)
        onTTSSpeakingChanged?(false)
    }

    public func speak(_ text: String) async { await speak(text, using: nil) }

    public func speak(_ text: String, using voiceID: String?) async {
        if Task.isCancelled { return }
        await opGate.acquire()
        defer { Task { await opGate.release() } }
        if Task.isCancelled { return }
        await stopRecognitionAsync()

        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            if Task.isCancelled { cont.resume(); return }
            let utt = AVSpeechUtterance(string: text)
            applyProfile(to: utt, voiceID: voiceID)
            let key = ObjectIdentifier(utt)
            speakContinuations[key] = cont
            synthesizer.speak(utt)
        }
    }

    private func applyProfile(to utt: AVSpeechUtterance, voiceID: String?) {
        let profile = (voiceID.flatMap { profilesByID[$0] }) ?? defaultProfile
        if let pid = profile?.id, let v = AVSpeechSynthesisVoice(identifier: pid) { utt.voice = v }
        else { utt.voice = AVSpeechSynthesisVoice(language: Locale.autoupdatingCurrent.identifier) }

        let sysMin = AVSpeechUtteranceMinimumSpeechRate
        let sysMax = AVSpeechUtteranceMaximumSpeechRate
        let span = sysMax - sysMin
        var rate = sysMin + span * (profile?.rate ?? 0.5)
        if master.rateVariation > 0 {
            let jitter = (Float.random(in: -master.rateVariation...master.rateVariation)) * Float(span)
            rate = max(sysMin, min(sysMax, rate + jitter))
        }
        utt.rate = rate

        var pitch = profile?.pitch ?? 1.0
        if master.pitchVariation > 0 {
            pitch += Float.random(in: -master.pitchVariation...master.pitchVariation)
        }
        utt.pitchMultiplier = max(0.5, min(2.0, pitch))

        let baseVol = profile?.volume ?? 1.0
        utt.volume = max(0.0, min(1.0, baseVol * master.volume))
    }

    fileprivate func ttsDidStart(key: ObjectIdentifier) { ttsStartPulse() }
    fileprivate func ttsWillSpeak(range: NSRange, text: String) { ttsGlow = min(1.0, ttsGlow + 0.55) }
    fileprivate func ttsDidFinishOrCancel(key: ObjectIdentifier) {
        speakContinuations.removeValue(forKey: key)?.resume()
        ttsSuppressUntil = CACurrentMediaTime() + config.ttsSuppressAfterFinish
        if autoStartPreparedAfterTTS, let player = boostPlayer, !player.isPlaying {
            autoStartPreparedAfterTTS = false
            player.play()
        }
        ttsStopPulse()
    }

    public func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
        await opGate.acquire()
        defer { Task { await opGate.release() } }

        if Task.isCancelled { throw CancellationError() }
        await stopRecognitionAsync()

        #if os(iOS)
        if !AVAudioSession.sharedInstance().isInputAvailable { throw SimpleError("Mic unavailable.") }
        #endif

        latestTranscript = ""
        onTranscriptChanged?("")
        onLevelChanged?(0)
        hasFinishedRecognition = false
        firstSpeechStart = nil
        lastSpeechEnd = nil
        listenOutURL = nil
        listenOutFile = nil
        finishQueued = nil
        tapInstalled = false

        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil

        let engine = AVAudioEngine()
        audioEngine = engine

        let request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = true

        switch recognitionContext.expectation {
        case .freeform:
            request.taskHint = .dictation
        case .name(let allowed):
            request.taskHint = .dictation
            request.contextualStrings = allowed
        case .number:
            request.taskHint = .search
            request.contextualStrings = RecognitionContext.numericContextualStrings
        }

        if #available(iOS 13.0, macOS 10.15, *),
           let recognizer = speechRecognizer,
           recognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
        }
        recognitionRequest = request

        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw SimpleError("Recognizer unavailable.")
        }

        let inputNode = engine.inputNode
        let inputFormat = inputNode.inputFormat(forBus: 0)
        if inputFormat.channelCount == 0 || inputFormat.sampleRate <= 0 {
            throw SimpleError("Invalid input format.")
        }

        if record {
            let url = FileManager.default.temporaryDirectory.appendingPathComponent("rec-\(UUID().uuidString).caf")
            listenOutURL = url
            listenOutFile = try AVAudioFile(forWriting: url, settings: inputFormat.settings)
        }

        struct LevelSink: @unchecked Sendable {
            weak var owner: RealVoiceIO?
            func callAsFunction(_ value: CGFloat) {
                Task { @MainActor in owner?.onLevelChanged?(value) }
            }
        }
        let levelSink = LevelSink(owner: self)

        let reqRef = request
        let fileRef = listenOutFile

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { buffer, _ in
            let frames = Int(buffer.frameLength)
            guard frames > 0 else {
                levelSink(0)
                return
            }

            reqRef.append(buffer)
            if let fileRef { try? fileRef.write(from: buffer) }

            if let ch = buffer.floatChannelData?.pointee {
                var ms: Float = 0
                vDSP_measqv(ch, 1, &ms, vDSP_Length(frames))
                let db: Float = ms <= 0 ? -160 : 10 * log10f(ms)
                let norm = max(0, min(1, (db + 60) / 60))
                levelSink(CGFloat(norm))
            }
        }
        tapInstalled = true

        engine.prepare()
        try engine.start()
        onListeningChanged?(true)

        try await Task.sleep(nanoseconds: 50_000_000)
        if Task.isCancelled { throw CancellationError() }

        let task = recognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self else { return }
            if let r = result {
                let now = CACurrentMediaTime()
                var text = r.bestTranscription.formattedString
                let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                let segments = r.bestTranscription.segments

                if case .number = self.recognitionContext.expectation,
                   let normalized = Self.normalizeNumeric(from: text) {
                    text = normalized
                }

                Task { @MainActor in
                    if now >= self.ttsSuppressUntil, !trimmed.isEmpty {
                        self.latestTranscript = text
                        self.onTranscriptChanged?(text)
                        self.startInactivityTimer(seconds: inactivity)
                    }

                    if !segments.isEmpty {
                        var first = self.firstSpeechStart
                        var last = self.lastSpeechEnd
                        for seg in segments {
                            if first == nil { first = seg.timestamp }
                            last = max(last ?? 0, seg.timestamp + seg.duration)
                        }
                        self.firstSpeechStart = first
                        self.lastSpeechEnd = last
                    }

                    if r.isFinal { self.finishRecognition() }
                }
            }
            if error != nil {
                Task { @MainActor in self.finishRecognition() }
            }
        }
        recognitionTask = task

        startOverallTimer(seconds: timeout)

        let result: VoiceResult = try await withCheckedThrowingContinuation { (cont: CheckedContinuation<VoiceResult, Error>) in
            self.listenCont = cont
            if let queued = self.finishQueued {
                self.finishQueued = nil
                self.resumeListen(with: queued)
            }
        }

        if Task.isCancelled { throw CancellationError() }
        return result
    }

    private static func normalizeNumeric(from text: String) -> String? {
        if let int = Int(text.trimmingCharacters(in: .whitespacesAndNewlines)) { return String(int) }
        let fmt = NumberFormatter()
        fmt.locale = .autoupdatingCurrent
        fmt.numberStyle = .spellOut
        if let n = fmt.number(from: text.lowercased()) { return n.stringValue }
        return nil
    }

    private func startInactivityTimer(seconds: TimeInterval) {
        listenInactivityTask?.cancel()
        listenInactivityTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run { self?.finishRecognition() }
        }
    }
    private func startOverallTimer(seconds: TimeInterval) {
        listenOverallTask?.cancel()
        listenOverallTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run { self?.finishRecognition() }
        }
    }

    private func ensureBoostEngine(format: AVAudioFormat) throws {
        if boostEngine == nil {
            let engine = AVAudioEngine()
            let player = AVAudioPlayerNode()
            let eq = AVAudioUnitEQ(numberOfBands: 0)
            engine.attach(player); engine.attach(eq)
            engine.connect(player, to: eq, format: format)
            engine.connect(eq, to: engine.mainMixerNode, format: format)
            try engine.start()
            boostEngine = engine; boostPlayer = player; boostEQ = eq
        } else if boostEngine?.isRunning == false {
            try boostEngine?.start()
        }
    }

    public func prepareBoosted(url: URL, gainDB: Float) async throws {
        await opGate.acquire()
        defer { Task { await opGate.release() } }

        let file = try AVAudioFile(forReading: url)
        let format = file.processingFormat
        let totalFrames = AVAudioFrameCount(file.length)
        guard totalFrames > 0,
              let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: totalFrames) else {
            throw SimpleError("Failed to allocate audio buffer.")
        }
        try file.read(into: buffer)
        buffer.frameLength = totalFrames
        guard buffer.frameLength > 0 else { throw SimpleError("Audio buffer empty.") }

        try ensureBoostEngine(format: format)
        boostEQ?.globalGain = max(0, min(gainDB, 24))
        guard let player = boostPlayer else { return }

        await boostQueue.scheduleOne()
        autoStartPreparedAfterTTS = true

        let playerRef = player
        let bRef = buffer
        let boostQueueRef = boostQueue

        // Schedule directly; completion posts progress without detached tasks.
        playerRef.scheduleBuffer(bRef, at: nil, options: [], completionCallbackType: .dataPlayedBack) { _ in
            Task {
                await boostQueueRef.completeOne()
                let done = await boostQueueRef.isDone()
                if done {
                    await MainActor.run { [weak self] in
                        self?.completeBoostWaiters()
                    }
                }
            }
        }
    }

    public func startPreparedBoosted() async throws {
        if await boostQueue.hasNothing() {
            stopBoostedPlayback()
            return
        }
        guard let player = boostPlayer else {
            stopBoostedPlayback()
            return
        }
        if !autoStartPreparedAfterTTS, !player.isPlaying { player.play() }

        if await boostQueue.isDone() {
            stopBoostedPlayback()
            return
        }

        let timeout = max(0.25, config.boostedWaitTimeoutSeconds)
        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            let id = UUID()
            boostWaiters[id] = cont
            let boostQueueRef = boostQueue

            // Complete early if already done
            Task {
                let done = await boostQueueRef.isDone()
                if done {
                    await MainActor.run { [weak self] in self?.finishBoostWaiter(id: id) }
                }
            }

            // Timeout guard
            Task {
                try? await Task.sleep(nanoseconds: UInt64(timeout * 1_000_000_000))
                await MainActor.run { [weak self] in self?.finishBoostWaiter(id: id) }
            }
        }
    }

    public func playBoosted(url: URL, gainDB: Float) async throws {
        try await prepareBoosted(url: url, gainDB: gainDB)
        try await startPreparedBoosted()
    }

    public func stopAll() {
        synthesizer.stopSpeaking(at: .immediate)
        for (_, cont) in speakContinuations { cont.resume() }
        speakContinuations.removeAll()
        stopBoostedPlayback()
        stopRecognition()
        onListeningChanged?(false)
    }

    public func hardReset() {
        stopAll()
        hasFinishedRecognition = false
        firstSpeechStart = nil
        lastSpeechEnd = nil
        latestTranscript = ""
        autoStartPreparedAfterTTS = false
        onTranscriptChanged?("")
        onLevelChanged?(0)
        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil
        tapInstalled = false
        Task { await opGate.forceClear() }
    }

    private func completeBoostWaiters() {
        stopBoostedPlayback()
        let waiters = boostWaiters
        boostWaiters.removeAll()
        for (_, cont) in waiters { cont.resume() }
    }

    private func finishBoostWaiter(id: UUID) {
        if let c = boostWaiters.removeValue(forKey: id) {
            stopBoostedPlayback()
            c.resume()
        }
    }

    private func stopBoostedPlayback() {
        boostPlayer?.stop()
        boostEngine?.stop()
        boostEQ = nil
        boostPlayer = nil
        boostEngine = nil
        autoStartPreparedAfterTTS = false
        Task { await boostQueue.reset() }
    }

    private func finishRecognition() {
        guard !hasFinishedRecognition else { return }
        hasFinishedRecognition = true

        recognitionRequest?.endAudio()
        audioEngine?.stop()
        removeTapIfInstalled()

        var finalURL: URL? = listenOutURL
        if let url = listenOutURL {
            if let trimmed = trimAudioSmart(inputURL: url,
                                            sttStart: firstSpeechStart,
                                            sttEnd: lastSpeechEnd,
                                            prePad: config.trimPrePad,
                                            postPad: config.trimPostPad) {
                finalURL = trimmed
            }
        }
        let result = VoiceResult(transcript: self.latestTranscript, recordingURL: finalURL)

        cleanupRecognition()
        listenInactivityTask?.cancel(); listenInactivityTask = nil
        listenOverallTask?.cancel(); listenOverallTask = nil
        onListeningChanged?(false)

        if listenCont != nil {
            resumeListen(with: result)
        } else {
            finishQueued = result
        }
    }

    private func removeTapIfInstalled() {
        if tapInstalled {
            audioEngine?.inputNode.removeTap(onBus: 0)
            tapInstalled = false
        }
    }

    private func resumeListen(with result: VoiceResult) {
        guard let cont = listenCont else { return }
        listenCont = nil
        finishQueued = nil
        listenOutFile = nil
        listenOutURL = nil
        cont.resume(returning: result)
    }

    private func cleanupRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        audioEngine?.stop()
        audioEngine = nil
        tapInstalled = false
    }

    private func stopRecognition() {
        recognitionTask?.cancel()
        recognitionRequest?.endAudio()
        finishRecognition()
    }

    private func stopRecognitionAsync() async {
        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            stopRecognition()
            cont.resume()
        }
    }

    private func installInterruptionObservers() {
        #if os(iOS)
        NotificationCenter.default.addObserver(self, selector: #selector(handleInterruption(_:)),
                                               name: AVAudioSession.interruptionNotification, object: nil)
        NotificationCenter.default.addObserver(self, selector: #selector(handleRouteChange(_:)),
                                               name: AVAudioSession.routeChangeNotification, object: nil)
        #endif
    }

    @objc private func handleInterruption(_ note: Notification) {
        #if os(iOS)
        guard let info = note.userInfo,
              let typeValue = info[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else { return }
        switch type {
        case .began:
            wasInterrupted = true
            wasPlayingBoost = boostPlayer?.isPlaying ?? false
            boostPlayer?.pause()
            onListeningChanged?(false)
        case .ended:
            let optionsValue = info[AVAudioSessionInterruptionOptionKey] as? UInt
            let shouldResume = optionsValue.map { AVAudioSession.InterruptionOptions(rawValue: $0).contains(.shouldResume) } ?? true
            if shouldResume {
                try? AVAudioSession.sharedInstance().setActive(true, options: [])
                if wasPlayingBoost {
                    try? boostEngine?.start()
                    boostPlayer?.play()
                }
            }
            wasInterrupted = false
            wasPlayingBoost = false
        @unknown default: break
        }
        #endif
    }

    @objc private func handleRouteChange(_ note: Notification) {
        #if os(iOS)
        guard let info = note.userInfo,
              let reasonValue = info[AVAudioSessionRouteChangeReasonKey] as? UInt,
              let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else { return }
        switch reason {
        case .oldDeviceUnavailable, .categoryChange, .override, .wakeFromSleep, .noSuitableRouteForCategory:
            if audioEngine?.isRunning == true { finishRecognition() }
        default: break
        }
        #endif
    }

    private func trimAudioSmart(inputURL: URL,
                                sttStart: Double?,
                                sttEnd: Double?,
                                prePad: Double,
                                postPad: Double) -> URL? {
        do {
            let inFile = try AVAudioFile(forReading: inputURL)
            let sampleRate = inFile.fileFormat.sampleRate
            let totalFrames = inFile.length
            let duration = Double(totalFrames) / sampleRate

            let fallback = (sttStart == nil || sttEnd == nil || sttEnd! <= sttStart!)
            var start = sttStart ?? 0
            var end = sttEnd ?? duration

            if fallback {
                let targetFormat = inFile.processingFormat
                let chunk: AVAudioFrameCount = 8192
                inFile.framePosition = 0
                let threshDB: Float = -45
                var foundStart: Double?
                var lastNonSilent: Double = 0

                while inFile.framePosition < totalFrames {
                    let remaining = AVAudioFrameCount(totalFrames - inFile.framePosition)
                    let frames = min(chunk, remaining)
                    guard let buf = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: frames) else { break }
                    try inFile.read(into: buf, frameCount: frames)
                    if buf.frameLength == 0 { break }
                    let ts = Double(inFile.framePosition - Int64(frames)) / sampleRate

                    if let ch = buf.floatChannelData?.pointee {
                        var ms: Float = 0
                        vDSP_measqv(ch, 1, &ms, vDSP_Length(buf.frameLength))
                        let db: Float = ms <= 0 ? -160 : 10 * log10f(ms)
                        if db > threshDB {
                            if foundStart == nil { foundStart = ts }
                            lastNonSilent = ts + Double(buf.frameLength) / sampleRate
                        }
                    }
                }
                if let fs = foundStart {
                    start = fs
                    end = max(lastNonSilent, fs + 0.1)
                } else {
                    start = 0
                    end = duration
                }
            }

            start = max(0, start - prePad)
            end = min(duration, end + postPad)
            guard end > start else { return inputURL }

            let startFrame = AVAudioFramePosition(start * sampleRate)
            let endFrame = AVAudioFramePosition(end * sampleRate)
            let framesToRead = endFrame - startFrame
            guard framesToRead > 0 else { return inputURL }

            let outURL = inputURL.deletingPathExtension().appendingPathExtension("trim.caf")
            let outFile = try AVAudioFile(forWriting: outURL, settings: inFile.fileFormat.settings)

            inFile.framePosition = startFrame
            let chunkSize: AVAudioFrameCount = 8192
            while inFile.framePosition < endFrame {
                let remaining = AVAudioFrameCount(endFrame - inFile.framePosition)
                let frames = min(chunkSize, remaining)
                guard let buffer = AVAudioPCMBuffer(pcmFormat: inFile.processingFormat, frameCapacity: frames) else { break }
                try inFile.read(into: buffer, frameCount: frames)
                if buffer.frameLength == 0 { break }
                buffer.frameLength = frames
                try outFile.write(from: buffer)
            }
            return outURL
        } catch {
            return inputURL
        }
    }
}

// MARK: - AVSpeechSynthesizer delegate

private final class SpeechDelegate: NSObject, AVSpeechSynthesizerDelegate {
    private unowned(unsafe) let owner: RealVoiceIO
    init(owner: RealVoiceIO) {
        self.owner = owner
        super.init()
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didStart utterance: AVSpeechUtterance) {
        let key = ObjectIdentifier(utterance)
        Task { @MainActor in self.owner.ttsDidStart(key: key) }
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, willSpeakRangeOfSpeechString characterRange: NSRange, utterance: AVSpeechUtterance) {
        let text = utterance.speechString
        Task { @MainActor in self.owner.ttsWillSpeak(range: characterRange, text: text) }
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        let key = ObjectIdentifier(utterance)
        Task { @MainActor in self.owner.ttsDidFinishOrCancel(key: key) }
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didCancel utterance: AVSpeechUtterance) {
        let key = ObjectIdentifier(utterance)
        Task { @MainActor in self.owner.ttsDidFinishOrCancel(key: key) }
    }
}

// MARK: - ScriptedVoiceIO

@MainActor
public final class ScriptedVoiceIO: VoiceIO {
    public var onListeningChanged: ((Bool) -> Void)?
    public var onTranscriptChanged: ((String) -> Void)?
    public var onLevelChanged: ((CGFloat) -> Void)?
    public var onTTSSpeakingChanged: ((Bool) -> Void)?
    public var onTTSPulse: ((CGFloat) -> Void)?
    public var onStatusMessageChanged: ((String?) -> Void)?

    private var queue: [String]
    private var epoch: Int = 0

    // Convenience non-Base64 init for tests/demos
    public init(script: [String]) {
        self.queue = script
    }

    public init?(fromBase64 jsonBase64: String) {
        guard let data = Data(base64Encoded: jsonBase64),
              let arr = try? JSONSerialization.jsonObject(with: data) as? [String] else {
            return nil
        }
        self.queue = arr
    }

    public func ensurePermissions() async throws { }
    public func configureSessionIfNeeded() async throws { }

    public func speak(_ text: String) async {
        onTTSSpeakingChanged?(true)
        onTTSPulse?(0.35)
        let steps = 6
        for i in 0..<steps {
            let phase = Double(i) / Double(steps - 1)
            let level = 0.28 + 0.24 * sin(phase * .pi)
            onTTSPulse?(CGFloat(level))
            try? await Task.sleep(nanoseconds: 50_000_000)
        }
        onTTSPulse?(0.0)
        onTTSSpeakingChanged?(false)
    }

    public func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
        let myEpoch = epoch
        onListeningChanged?(true)
        onLevelChanged?(0.2)
        try? await Task.sleep(nanoseconds: 120_000_000)
        if myEpoch != epoch {
            onLevelChanged?(0.0)
            onListeningChanged?(false)
            return VoiceResult(transcript: "", recordingURL: nil)
        }
        let transcript = queue.isEmpty ? "" : queue.removeFirst()
        onTranscriptChanged?(transcript)
        try? await Task.sleep(nanoseconds: 40_000_000)
        onLevelChanged?(0.0)
        onListeningChanged?(false)
        return VoiceResult(transcript: transcript, recordingURL: nil)
    }

    public func prepareBoosted(url: URL, gainDB: Float) async throws { }
    public func startPreparedBoosted() async throws { }
    public func playBoosted(url: URL, gainDB: Float) async throws { }

    public func stopAll() {
        epoch &+= 1
        onListeningChanged?(false)
    }

    public func hardReset() {
        epoch &+= 1
        onListeningChanged?(false)
        onTranscriptChanged?("")
        onLevelChanged?(0)
        onTTSPulse?(0)
        onTTSSpeakingChanged?(false)
        onStatusMessageChanged?(nil)
    }
}
