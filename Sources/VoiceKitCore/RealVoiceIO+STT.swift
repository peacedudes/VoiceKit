//
//  RealVoiceIO+STT.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
import CoreGraphics
@preconcurrency import AVFoundation
@preconcurrency import Speech
import Accelerate
import QuartzCore

@MainActor
extension RealVoiceIO {

    // MARK: - Listen

    public func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
        await opGate.acquire()
        defer { Task { await opGate.release() } }

        if Task.isCancelled { throw CancellationError() }
        await stopRecognitionAsync()

        #if os(iOS)
        if !AVAudioSession.sharedInstance().isInputAvailable { throw SimpleError("Mic unavailable.") }
        #endif

        latestTranscript = ""
        onTranscriptChanged?("")
        onLevelChanged?(0)
        hasFinishedRecognition = false
        firstSpeechStart = nil
        lastSpeechEnd = nil
        listenOutURL = nil
        listenOutFile = nil
        finishQueued = nil
        tapInstalled = false

        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil

        let engine = AVAudioEngine()
        audioEngine = engine

        let request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = true

        switch recognitionContext.expectation {
        case .freeform:
            request.taskHint = .dictation
        case .name(let allowed):
            request.taskHint = .dictation
            request.contextualStrings = allowed
        case .number:
            request.taskHint = .search
            request.contextualStrings = RecognitionContext.numericContextualStrings
        }

        if #available(iOS 13.0, macOS 10.15, *),
           let recognizer = speechRecognizer,
           recognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
        }
        recognitionRequest = request

        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw SimpleError("Recognizer unavailable.")
        }

        let inputNode = engine.inputNode
        let inputFormat = inputNode.inputFormat(forBus: 0)
        if inputFormat.channelCount == 0 || inputFormat.sampleRate <= 0 {
            throw SimpleError("Invalid input format.")
        }

        if record {
            let url = FileManager.default.temporaryDirectory.appendingPathComponent("rec-\(UUID().uuidString).caf")
            listenOutURL = url
            listenOutFile = try AVAudioFile(forWriting: url, settings: inputFormat.settings)
        }

        struct LevelSink: @unchecked Sendable {
            weak var owner: RealVoiceIO?
            func callAsFunction(_ value: CGFloat) {
                Task { @MainActor in owner?.onLevelChanged?(value) }
            }
        }
        let levelSink = LevelSink(owner: self)

        let reqRef = request
        let fileRef = listenOutFile

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { buffer, _ in
            let frames = Int(buffer.frameLength)
            guard frames > 0 else {
                levelSink(0)
                return
            }

            reqRef.append(buffer)
            if let fileRef { try? fileRef.write(from: buffer) }

            if let ch = buffer.floatChannelData?.pointee {
                var ms: Float = 0
                vDSP_measqv(ch, 1, &ms, vDSP_Length(frames))
                let db: Float = ms <= 0 ? -160 : 10 * log10f(ms)
                let norm = max(0, min(1, (db + 60) / 60))
                levelSink(CGFloat(norm))
            }
        }
        tapInstalled = true

        engine.prepare()
        try engine.start()
        onListeningChanged?(true)

        try await Task.sleep(nanoseconds: 50_000_000)
        if Task.isCancelled { throw CancellationError() }

        let task = recognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self else { return }
            if let r = result {
                let now = CACurrentMediaTime()
                var text = r.bestTranscription.formattedString
                let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                let segments = r.bestTranscription.segments

                if case .number = self.recognitionContext.expectation,
                   let normalized = Self.normalizeNumeric(from: text) {
                    text = normalized
                }

                Task { @MainActor in
                    if now >= self.ttsSuppressUntil, !trimmed.isEmpty {
                        self.latestTranscript = text
                        self.onTranscriptChanged?(text)
                        self.startInactivityTimer(seconds: inactivity)
                    }

                    if !segments.isEmpty {
                        var first = self.firstSpeechStart
                        var last = self.lastSpeechEnd
                        for seg in segments {
                            if first == nil { first = seg.timestamp }
                            last = max(last ?? 0, seg.timestamp + seg.duration)
                        }
                        self.firstSpeechStart = first
                        self.lastSpeechEnd = last
                    }

                    if r.isFinal { self.finishRecognition() }
                }
            }
            if error != nil {
                Task { @MainActor in self.finishRecognition() }
            }
        }
        recognitionTask = task

        startOverallTimer(seconds: timeout)

        let result: VoiceResult = try await withCheckedThrowingContinuation { (cont: CheckedContinuation<VoiceResult, Error>) in
            self.listenCont = cont
            if let queued = self.finishQueued {
                self.finishQueued = nil
                self.resumeListen(with: queued)
            }
        }

        if Task.isCancelled { throw CancellationError() }
        return result
    }

    // MARK: - Stop/Cleanup

    func stopRecognition() {
        recognitionTask?.cancel()
        recognitionRequest?.endAudio()
        finishRecognition()
    }

    func stopRecognitionAsync() async {
        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            stopRecognition()
            cont.resume()
        }
    }
}

// MARK: - Timers and finish

@MainActor
extension RealVoiceIO {

    static func normalizeNumeric(from text: String) -> String? {
        if let int = Int(text.trimmingCharacters(in: .whitespacesAndNewlines)) { return String(int) }
        let fmt = NumberFormatter()
        fmt.locale = .autoupdatingCurrent
        fmt.numberStyle = .spellOut
        if let n = fmt.number(from: text.lowercased()) { return n.stringValue }
        return nil
    }

    func startInactivityTimer(seconds: TimeInterval) {
        listenInactivityTask?.cancel()
        listenInactivityTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run { self?.finishRecognition() }
        }
    }

    func startOverallTimer(seconds: TimeInterval) {
        listenOverallTask?.cancel()
        listenOverallTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run { self?.finishRecognition() }
        }
    }

    func finishRecognition() {
        guard !hasFinishedRecognition else { return }
        hasFinishedRecognition = true

        recognitionRequest?.endAudio()
        audioEngine?.stop()
        removeTapIfInstalled()

        var finalURL: URL? = listenOutURL
        if let url = listenOutURL {
            if let trimmed = trimAudioSmart(inputURL: url,
                                            sttStart: firstSpeechStart,
                                            sttEnd: lastSpeechEnd,
                                            prePad: config.trimPrePad,
                                            postPad: config.trimPostPad) {
                finalURL = trimmed
            }
        }
        let result = VoiceResult(transcript: self.latestTranscript, recordingURL: finalURL)

        cleanupRecognition()
        listenInactivityTask?.cancel(); listenInactivityTask = nil
        listenOverallTask?.cancel(); listenOverallTask = nil
        onListeningChanged?(false)

        if listenCont != nil {
            resumeListen(with: result)
        } else {
            finishQueued = result
        }
    }

    func removeTapIfInstalled() {
        if tapInstalled {
            audioEngine?.inputNode.removeTap(onBus: 0)
            tapInstalled = false
        }
    }

    func resumeListen(with result: VoiceResult) {
        guard let cont = listenCont else { return }
        listenCont = nil
        finishQueued = nil
        listenOutFile = nil
        listenOutURL = nil
        cont.resume(returning: result)
    }

    func cleanupRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        audioEngine?.stop()
        audioEngine = nil
        tapInstalled = false
    }
}
