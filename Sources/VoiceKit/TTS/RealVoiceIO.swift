//
//  RealVoiceIO.swift
//  VoiceKit
//
//  Main VoiceIO implementation with TTS and live STT.
//  - In CI/headless (IsCI.running): listen() uses a deterministic stub path.
//  - In apps on device/simulator: listen() uses the live AVAudioEngine + SFSpeechRecognizer pipeline.
//
//  Generated by GPT-5 (OpenAI) - collaborator: rdoggett
//  date: 09-18-2025
//

import Foundation
@preconcurrency import AVFoundation
import CoreGraphics
@preconcurrency import Speech

@MainActor
public final class RealVoiceIO: NSObject, TTSConfigurable, VoiceIO {

    // MARK: - Public callbacks
    public var onListeningChanged: ((Bool) -> Void)?
    public var onTranscriptChanged: ((String) -> Void)?
    public var onLevelChanged: ((CGFloat) -> Void)?
    public var onTTSSpeakingChanged: ((Bool) -> Void)?
    public var onTTSPulse: ((CGFloat) -> Void)?
    public var onStatusMessageChanged: ((String?) -> Void)?

    // MARK: - Debug logging (opt-in)
    public enum LogLevel: Sendable {
        case info, warn, error
    }
    /// Optional logger. Set from the app to trace VoiceKit events.
    /// Example:
    ///   io.logger = { level, msg in print("[VoiceKit][\(level)] \(msg)") }
    public var logger: ((LogLevel, String) -> Void)?

    @inline(__always)
    internal func log(_ level: LogLevel = .info, _ message: @autoclosure () -> String) {
        logger?(level, message())
    }

    // MARK: - Public config
    public private(set) var config: VoiceIOConfig

    // MARK: - TTS state
    internal var profilesByID: [String: TTSVoiceProfile] = [:]
    internal var defaultProfile: TTSVoiceProfile?
    internal var tuning: Tuning = .init()

    // AVSpeechSynthesizer (lazy optional)
    internal var synthesizer: AVSpeechSynthesizer?

    // Continuations keyed by utterance
    internal var speakContinuations: [ObjectIdentifier: CheckedContinuation<Void, Error>] = [:]

    // Per-utterance timing (for duration measurement/calibration)
    internal var ttsStartTimes: [ObjectIdentifier: TimeInterval] = [:]
    internal var measureContinuations: [ObjectIdentifier: CheckedContinuation<TimeInterval, Never>] = [:]

    // Simple pulse animation state
    internal var ttsPhase: CGFloat = 0
    internal var ttsGlow: CGFloat = 0

    // MARK: - Environment helpers

    /// Treat the iOS simulator like CI for live STT:
    /// many simulator/runtime combinations do not reliably support
    /// SFSpeechRecognizer + AVAudioEngine, and repeatedly fail with
    /// kAFAssistantErrorDomain recording errors. On real devices we
    /// still use the full live STT pipeline.
    internal static var isSimulator: Bool {
        #if targetEnvironment(simulator)
        return true
        #else
        return false
        #endif
    }

    // MARK: - Test/STT shim state

    // Very lightweight transcript store used by tests
    private static var _latestTranscriptStore = [ObjectIdentifier: String]()
    public var latestTranscript: String {
        get { RealVoiceIO._latestTranscriptStore[ObjectIdentifier(self)] ?? "" }
        set { RealVoiceIO._latestTranscriptStore[ObjectIdentifier(self)] = newValue }
    }

    // Recognition context captured for listen shim + live STT.
    // Internal so STT extension can read it.
    internal var recognitionContext: RecognitionContext = .init()

    // MARK: - STT live state

    internal var speechRecognizer: SFSpeechRecognizer? = SFSpeechRecognizer(locale: .autoupdatingCurrent)
    internal var audioEngine: AVAudioEngine?
    internal var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    internal var recognitionTask: SFSpeechRecognitionTask?

    internal var hasFinishedRecognition = false
    internal var firstSpeechStart: Double?
    internal var lastSpeechEnd: Double?

    internal var listenOverallTask: Task<Void, Never>?
    internal var listenInactivityTask: Task<Void, Never>?

    // Tracks recent speech activity based on input energy.
    internal let sttActivityTracker = STTActivityTracker()

    // Per-listen recording state for live STT
    internal var rawRecordingURL: URL?
    internal var currentListenShouldRecord = false

    // Continuation for the current live listen (non-CI path)
    internal var listenCont: CheckedContinuation<VoiceResult, Error>?

    // MARK: - Init

    override public init() {
        self.config = VoiceIOConfig()
        super.init()
        // Opt-in default logger via env flag
        let env = ProcessInfo.processInfo.environment
        if let logEnv = env["VOICEKIT_LOG"]?.lowercased(),
           logEnv == "1" || logEnv == "true" || logEnv == "yes" {
            self.logger = { level, msg in
                print("[VoiceKit][\(level)] \(msg)")
            }
        }
    }

    public init(config: VoiceIOConfig) {
        self.config = config
        super.init()
        let env = ProcessInfo.processInfo.environment
        if let logEnv = env["VOICEKIT_LOG"]?.lowercased(),
           logEnv == "1" || logEnv == "true" || logEnv == "yes" {
            self.logger = { level, msg in
                print("[VoiceKit][\(level)] \(msg)")
            }
        }
    }

    public static func makeLive(config: VoiceIOConfig = VoiceIOConfig()) -> RealVoiceIO {
        RealVoiceIO(config: config)
    }

    // MARK: - TTSConfigurable

    public func setVoiceProfile(_ profile: TTSVoiceProfile) { profilesByID[profile.id] = profile }
    public func getVoiceProfile(id: String) -> TTSVoiceProfile? { profilesByID[id] }
    public func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile; profilesByID[profile.id] = profile }
    public func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }

    // Transitional convenience: prefer Tuning in new code.
    // These are not required by TTSConfigurable yet, but available on RealVoiceIO.
    public func setTuning(_ tuning: Tuning) {
        self.tuning = tuning
    }
    public func getTuning() -> Tuning { tuning }
    // MARK: - VoiceIO basics

    public func ensurePermissions() async throws {
        try await ensureSpeechAndMicPermissions()
    }
    public func configureSessionIfNeeded() async throws {
        try await configureAudioSessionIfNeeded()
    }

    public func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
        log(.info, "listen(start) timeout=\(timeout), inactivity=\(inactivity), record=\(record)")
        onListeningChanged?(true)
        defer { onListeningChanged?(false) }

        // CI/headless path: keep the stub behavior so tests remain deterministic
        // and never require real hardware or permissions.
        if IsCI.running {
            if timeout > 0 {
                let nanos = UInt64(min(timeout, 0.05) * 1_000_000_000)
                if nanos > 0 { try? await Task.sleep(nanoseconds: nanos) }
            }

            // If context expects a number, synthesize a final "42"
            if recognitionContext.expectNumber {
                let transcript = "42"
                log(.info, "listen(result/ci) synthesized numeric: \(transcript)")
                latestTranscript = transcript
                onTranscriptChanged?(transcript)
                return VoiceResult(transcript: transcript, recordingURL: nil)
            }

            // Otherwise, return whatever has been set externally (default empty)
            let stub = VoiceResult(transcript: latestTranscript, recordingURL: nil)
            log(.info, "listen(result/ci) transcript='\(stub.transcript)' record=\(record)")
            return stub
        }

        // App path: configure per-listen recording intent; the STT pipeline
        // will honor this and emit a trimmed recording URL when possible.
        currentListenShouldRecord = record
        rawRecordingURL = nil

        // App path: use the live STT pipeline defined in RealVoiceIO+STT.swift.
        let live = try await performSTTListen(timeout: timeout,
                                              inactivity: inactivity,
                                              record: record)
        log(.info, "listen(result/live) transcript='\(live.transcript)' record=\(record)")
        return live
    }

    // Called by tests; store context for listen shim
    public func setRecognitionContext(_ context: RecognitionContext) {
        self.recognitionContext = context
    }

    public func stopAll() {
        synthesizer?.stopSpeaking(at: .immediate)
    }

    public func hardReset() {
        // Stop any TTS in progress.
        stopAll()

        // Tear down any in-flight STT listen.
        listenOverallTask?.cancel()
        listenOverallTask = nil
        listenInactivityTask?.cancel()
        listenInactivityTask = nil

        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine = nil

        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil

        // If a live listen was waiting on a continuation, resume it with
        // cancellation so callers can unwind cleanly.
        if !hasFinishedRecognition {
            hasFinishedRecognition = true
            listenCont?.resume(throwing: CancellationError())
        }
        listenCont = nil

        rawRecordingURL = nil
        currentListenShouldRecord = false
        firstSpeechStart = nil
        lastSpeechEnd = nil
        latestTranscript = ""

        // Clear TTS bookkeeping.
        speakContinuations.removeAll()
        ttsStartTimes.removeAll()
        measureContinuations.removeAll()
    }
}

// MARK: - RecognitionContext helpers used by the listen shim

public extension RecognitionContext {
    var expectNumber: Bool {
        if case .number = expectation { return true }
        return false
    }
}
