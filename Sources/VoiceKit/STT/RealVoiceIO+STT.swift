//
//  RealVoiceIO+STT.swift
//  VoiceKit
//
//  Minimal STT helpers to satisfy tests without full STT implementation.
//  Now also includes a live STT pipeline used in app contexts.
//  - In CI/headless (IsCI.running): RealVoiceIO.listen() keeps using a stub.
//  - In app: RealVoiceIO.listen() calls performSTTListen(..) below.
//
//  Generated by GPT-5.1 (OpenAI) — collaborator: rdoggett
//  date: 11-23-2025
//

import Foundation
@preconcurrency import AVFoundation
@preconcurrency import Speech
import Accelerate
import CoreGraphics

// Tracks when we last observed "loud" audio in the mic stream.
// Used to decide when the user has gone quiet, independent of STT partials.
internal actor STTActivityTracker {
    private var lastLoudTime: TimeInterval?
    // Simple adaptive estimate of background noise in dB.
    // Starts at nil; seeded from early observations.
    private var baselineDB: Float?

    /// Observe a new buffer loudness in dB at the given uptime.
    /// - Updates the baseline (noise floor) over time.
    /// - Updates lastLoudTime only when the buffer is sufficiently louder
    ///   than the baseline (speech vs. background).
    func observe(db: Float, at time: TimeInterval) {
        // Seed baseline from first sample.
        if baselineDB == nil {
            baselineDB = db
        }

        guard let currentBaseline = baselineDB else { return }

        // Exponential moving average toward the observed db.
        // Use a small alpha so the baseline moves slowly and reflects
        // ambient conditions rather than every transient spike.
        let alpha: Float = 0.05
        let newBaseline = currentBaseline + alpha * (db - currentBaseline)
        baselineDB = newBaseline

        // Margin above baseline to consider this "speech" instead of noise.
        // This is deliberately modest; we still have an inactivity timeout
        // and trimming on the raw recording to refine bounds.
        let margin: Float = 12.0
        if db >= newBaseline + margin {
            markLoud(at: time)
        }
    }

    func markLoud(at time: TimeInterval) {
        // Only move forward in time.
        if let current = lastLoudTime {
            if time > current { lastLoudTime = time }
        } else {
            lastLoudTime = time
        }
    }

    func lastLoud() -> TimeInterval? { lastLoudTime }

    func reset(now: TimeInterval? = nil) {
        baselineDB = nil
        lastLoudTime = now
    }
}

@MainActor
extension RealVoiceIO {

    // MARK: - Legacy test hook

    /// Legacy no-op used by older tests to simulate end-of-speech.
    ///
    /// The modern live STT pipeline does **not** depend on this hook; it
    /// completes listens via recognizer callbacks, inactivity timers, and
    /// overall timeouts. The method remains only to avoid breaking existing
    /// callers and can be treated as a harmless stub in new code.
    public func finishRecognition() {
        // no-op stub; present for legacy test compatibility
    }

    // MARK: - Live STT implementation (app path)

    /// Ensure mic + speech permissions are granted; throws on failure.
    ///
    /// - Behavior:
    ///   - On devices and simulators (normal app runs), this:
    ///     - Requests microphone access using the appropriate API per platform.
    ///     - Awaits speech recognizer authorization via `PermissionBridge`.
    ///     - Verifies that the `SFSpeechRecognizer` is currently available.
    ///   - Under CI or forced test mode (`IsCI.running == true` / `VOICEKIT_FORCE_CI`):
    ///     - `PermissionBridge` helpers short-circuit and return “granted”
    ///       to keep headless tests deterministic and free of TCC prompts.
    internal func ensureSpeechAndMicPermissions() async throws {
        // Short-circuit in CI/headless; tests never hit real STT.
        if IsCI.running { return }

        let micOK: Bool
        #if os(iOS)
        if #available(iOS 17.0, *) {
            micOK = await withCheckedContinuation { (continuation: CheckedContinuation<Bool, Never>) in
                AVAudioApplication.requestRecordPermission { continuation.resume(returning: $0) }
            }
        } else {
            micOK = await withCheckedContinuation { (continuation: CheckedContinuation<Bool, Never>) in
                AVAudioSession.sharedInstance().requestRecordPermission { continuation.resume(returning: $0) }
            }
        }
        #elseif os(macOS)
        micOK = await withCheckedContinuation { (continuation: CheckedContinuation<Bool, Never>) in
            AVCaptureDevice.requestAccess(for: .audio) { continuation.resume(returning: $0) }
        }
        #else
        micOK = false
        #endif

        guard micOK else { throw SimpleError("Microphone permission denied.") }

        // Speech permission
        let status = await PermissionBridge.awaitSpeechAuth()
        guard status == .authorized else { throw SimpleError("Speech permission denied.") }
        guard speechRecognizer?.isAvailable == true else { throw SimpleError("Speech recognizer unavailable.") }
    }

    /// Configure audio session for voice I/O where applicable.
    internal func configureAudioSessionIfNeeded() async throws {
        #if os(iOS)
        let session = AVAudioSession.sharedInstance()
        try? session.setActive(false, options: [])
        var opts: AVAudioSession.CategoryOptions = [.defaultToSpeaker, .duckOthers]
        opts.insert(.allowBluetoothHFP)
        try session.setCategory(.playAndRecord, mode: .voiceChat, options: opts)
        try session.setPreferredSampleRate(44100)
        try session.setPreferredIOBufferDuration(0.01)
        try session.setActive(true, options: [])
        #else
        // macOS: rely on default session.
        _ = ()
        #endif
    }

    /// Install an AVAudioEngine input tap that forwards audio buffers into the
    /// given SFSpeechAudioBufferRecognitionRequest.
    ///
    /// This helper is explicitly nonisolated so the tap closure does **not**
    /// inherit @MainActor isolation. AVFoundation invokes the closure on a
    /// realtime audio queue (e.g., RealtimeMessenger.mServiceQueue); if it were
    /// @MainActor-isolated, Swift 6 would trap with `_swift_task_checkIsolatedSwift`.
    ///
    /// The activityTracker is updated from the realtime audio queue based on
    /// buffer energy and later consulted by the inactivity timer to decide
    /// when the user has gone quiet.
    nonisolated internal static func installRecognitionTap(
        engine: AVAudioEngine,
        request: SFSpeechAudioBufferRecognitionRequest,
        recordingFile: AVAudioFile?,
        activityTracker: STTActivityTracker
    ) throws {
        let inputNode = engine.inputNode
        let format = inputNode.inputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
            let frames = Int(buffer.frameLength)
            guard frames > 0 else { return }
            // Feed STT only; never touch @MainActor state from this queue.
            request.append(buffer)
            // Optionally mirror into a raw recording file. Any error here should
            // not break recognition; ignore failures.
            if let file = recordingFile {
                try? file.write(from: buffer)
            }

            // Approximate loudness in dB for this buffer and feed it into the
            // activity tracker so it can adapt to ambient noise and mark
            // "loud enough" buffers as speech activity.
            if let ch = buffer.floatChannelData?.pointee {
                var ms: Float = 0
                vDSP_measqv(ch, 1, &ms, vDSP_Length(buffer.frameLength))
                let db: Float = ms <= 0 ? -160 : 10 * log10f(ms)
                let now = ProcessInfo.processInfo.systemUptime
                Task.detached {
                    await activityTracker.observe(db: db, at: now)
                }
            }
        }
    }

    /// Real STT listen implementation used when not in CI.
    internal func performSTTListen(timeout: TimeInterval,
                                   inactivity: TimeInterval,
                                   record: Bool) async throws -> VoiceResult {
        if Task.isCancelled { throw CancellationError() }

        #if os(iOS)
        if !AVAudioSession.sharedInstance().isInputAvailable {
            throw SimpleError("Mic unavailable.")
        }
        #endif

        latestTranscript = ""
        onTranscriptChanged?("")
        onLevelChanged?(0)
        hasFinishedRecognition = false
        firstSpeechStart = nil
        lastSpeechEnd = nil

        // Reset activity tracker for this listen. Until we observe loud input
        // from the tap, inactivity is measured relative to this moment; once
        // loud buffers arrive, the tracker will be updated from the realtime
        // queue.
        await sttActivityTracker.reset(now: ProcessInfo.processInfo.systemUptime)

        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil

        let engine = AVAudioEngine()
        audioEngine = engine

        let (request, recognizer) = try prepareRecognitionRequest()

        let inputNode = engine.inputNode
        let inputFormat = inputNode.inputFormat(forBus: 0)
        if inputFormat.channelCount == 0 || inputFormat.sampleRate <= 0 {
            throw SimpleError("Invalid input format.")
        }

        let recordingFile = makeRecordingFileIfNeeded(for: inputFormat, record: record)

        // The tap itself must not be @MainActor-isolated; install it via a
        // nonisolated helper so the closure can run safely on the realtime queue.
        try Self.installRecognitionTap(
            engine: engine,
            request: request,
            recordingFile: recordingFile,
            activityTracker: sttActivityTracker
        )

        engine.prepare()
        try engine.start()
        onListeningChanged?(true)

        // Tiny startup delay to avoid treating engine warm-up as inactivity.
        if timeout > 0 {
            try await Task.sleep(nanoseconds: 50_000_000)
        }
        if Task.isCancelled { throw CancellationError() }

        let expectation = recognitionContext.expectation

        log(.info, "listen(stt) recognition task starting")
        recognitionTask = recognizer.recognitionTask(
            with: request
        ) { [weak self] (result: SFSpeechRecognitionResult?, error: Error?) in
            guard let self else { return }
            if let result {
                self.handleRecognitionSuccess(
                    result: result,
                    expectation: expectation,
                    inactivity: inactivity
                )
            }
            if error != nil {
                Task { @MainActor in
                    self.log(.warn, "listen(stt) complete via recognizer error: \(error!)")
                    self.completeCurrentListen()
                }
            }
        }

        startOverallTimer(seconds: timeout)

        let result: VoiceResult = try await withCheckedThrowingContinuation { (cont: CheckedContinuation<VoiceResult, Error>) in
            self.listenCont = cont
        }

        if Task.isCancelled { throw CancellationError() }
        return result
    }

    // MARK: - Recognition helpers

    private func handleRecognitionSuccess(
        result: SFSpeechRecognitionResult,
        expectation: RecognitionContext.Expectation,
        inactivity: TimeInterval
    ) {
        var text = result.bestTranscription.formattedString
        let trimmed = text.trimmingCharacters(
            in: CharacterSet.whitespacesAndNewlines
        )
        let segments = result.bestTranscription.segments

        if case .number = expectation {
            text = Self.normalizeNumeric(from: text)
        }

        Task { @MainActor in
            if !trimmed.isEmpty {
                self.latestTranscript = text
                self.onTranscriptChanged?(text)
                self.startInactivityTimer(seconds: inactivity)
            }

            if !segments.isEmpty {
                var first = self.firstSpeechStart
                var last = self.lastSpeechEnd
                for seg in segments {
                    if first == nil { first = seg.timestamp }
                    last = max(last ?? 0, seg.timestamp + seg.duration)
                }
                self.firstSpeechStart = first
                self.lastSpeechEnd = last
            }

            if result.isFinal {
                self.log(.info, "listen(stt) complete via recognizer isFinal")
                self.completeCurrentListen()
            }
        }
    }

    // MARK: - Private helpers (STT listen setup)

    /// Build and configure the speech recognition request and recognizer
    /// based on the current recognitionContext.
    private func prepareRecognitionRequest() throws -> (SFSpeechAudioBufferRecognitionRequest, SFSpeechRecognizer) {
        let request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = true

        switch recognitionContext.expectation {
        case .freeform:
            request.taskHint = .dictation
        case .name(let allowed):
            request.taskHint = .dictation
            request.contextualStrings = allowed
        case .number:
            request.taskHint = .search
            request.contextualStrings = RecognitionContext.numericContextualStrings
        }

        if #available(iOS 13.0, macOS 10.15, *),
           let recognizer = speechRecognizer,
           recognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
        }

        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw SimpleError("Recognizer unavailable.")
        }

        recognitionRequest = request
        return (request, recognizer)
    }

    /// Create an optional raw recording file for this listen. We never fail the overall
    /// listen if file creation fails; STT still runs normally.
    private func makeRecordingFileIfNeeded(for format: AVAudioFormat,
                                           record: Bool) -> AVAudioFile? {
        guard record else {
            rawRecordingURL = nil
            return nil
        }
        do {
            let base = "vk-listen-\(UUID().uuidString).caf"
            let url = FileManager.default.temporaryDirectory.appendingPathComponent(base)
            rawRecordingURL = url
            return try AVAudioFile(forWriting: url, settings: format.settings)
        } catch {
            // Fall back to STT-only; recordingURL will remain nil.
            rawRecordingURL = nil
            return nil
        }
    }

    // MARK: - Listen completion & timers (live path)
    private func startInactivityTimer(seconds: TimeInterval) {
        // seconds <= 0 means: disable inactivity timeout for this listen.
        guard seconds > 0 else {
            listenInactivityTask?.cancel()
            listenInactivityTask = nil
            return
        }

        listenInactivityTask?.cancel()
        listenInactivityTask = Task { [weak self] in
            // Poll activity until we've seen `seconds` of silence since the last
            // loud buffer, or until cancelled. The overall timer still enforces
            // a hard cap on total listen duration.
            let anchor = ProcessInfo.processInfo.systemUptime
            while !Task.isCancelled {
                try? await Task.sleep(nanoseconds: 250_000_000) // ~0.25s resolution
                guard let self else { return }

                let now = ProcessInfo.processInfo.systemUptime
                let last = await self.sttActivityTracker.lastLoud()
                // Fallback: if we've never seen a "loud" buffer, treat the
                // timer start as the reference point. We only start this timer
                // once we have a non-empty transcript, so `anchor` is already
                // "after the user has spoken at least once".
                let reference = last ?? anchor

                if now - reference >= seconds {
                    await MainActor.run {
                        self.log(.info, "listen(stt) complete via inactivity timeout \(seconds)s")
                        self.completeCurrentListen()
                    }
                    return
                }
            }
        }
    }

    private func startOverallTimer(seconds: TimeInterval) {
        listenOverallTask?.cancel()
        listenOverallTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run {
                self?.log(.info, "listen(stt) complete via overall timeout \(seconds)s")
                self?.completeCurrentListen()
            }
        }
    }

    private func completeCurrentListen() {
        guard !hasFinishedRecognition else { return }
        hasFinishedRecognition = true

        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil

        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()

        let transcript = latestTranscript
        var recordingURL: URL?

        if currentListenShouldRecord, let raw = rawRecordingURL {
            // Best-effort trim around detected speech; fall back to raw file.
            let trimmed = trimAudioSmart(
                inputURL: raw,
                sttStart: firstSpeechStart,
                sttEnd: lastSpeechEnd,
                prePad: config.trimPrePad,
                postPad: config.trimPostPad
            )
            recordingURL = trimmed ?? raw
        }

        currentListenShouldRecord = false
        rawRecordingURL = nil

        let res = VoiceResult(transcript: transcript, recordingURL: recordingURL)
        listenCont?.resume(returning: res)
        listenCont = nil
    }
}
