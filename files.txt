Concatenate all files in  'git ls-files', with header/separaters
.github/workflows/ci.yml
```yaml
name: CI

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

concurrency:
  group: ci-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-and-test:
    runs-on: macos-14
    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Select Xcode (Swift 6)
        uses: maxim-lobanov/setup-xcode@v1
        with:
          # Allow any 16.x to improve scheduling availability
          xcode-version: '16.*'

      - name: Tool versions
        run: |
          xcodebuild -version
          swift --version

      - name: Cache SPM
        uses: actions/cache@v4
        with:
          path: |
            ~/.swiftpm
            .build
          key: spm-${{ runner.os }}-xcode-16-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            spm-${{ runner.os }}-xcode-16-

      - name: Resolve
        run: swift package resolve

      - name: Build (debug)
        run: swift build -v

      - name: Test
        run: swift test -v
```
.gitignore
```
.DS_Store
/.build
/Packages
xcuserdata/
DerivedData/
.swiftpm/configuration/registries.json
.swiftpm/xcode/package.xcworkspace/contents.xcworkspacedata
.netrc
```
.swiftpm/xcode/xcshareddata/xcschemes/VoiceKit-Package.xcscheme
```
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "2600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitCore"
               BuildableName = "VoiceKitCore"
               BlueprintName = "VoiceKitCore"
               ReferencedContainer = "container:">
            </BuildableReference>
         </BuildActionEntry>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitUI"
               BuildableName = "VoiceKitUI"
               BlueprintName = "VoiceKitUI"
               ReferencedContainer = "container:">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
      <Testables>
         <TestableReference
            skipped = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitCoreTests"
               BuildableName = "VoiceKitCoreTests"
               BlueprintName = "VoiceKitCoreTests"
               ReferencedContainer = "container:">
            </BuildableReference>
         </TestableReference>
         <TestableReference
            skipped = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitUITests"
               BuildableName = "VoiceKitUITests"
               BlueprintName = "VoiceKitUITests"
               ReferencedContainer = "container:">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <MacroExpansion>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "VoiceKitCore"
            BuildableName = "VoiceKitCore"
            BlueprintName = "VoiceKitCore"
            ReferencedContainer = "container:">
         </BuildableReference>
      </MacroExpansion>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>
```
.swiftpm/xcode/xcshareddata/xcschemes/VoiceKitCore.xcscheme
```
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "2600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitCore"
               BuildableName = "VoiceKitCore"
               BlueprintName = "VoiceKitCore"
               ReferencedContainer = "container:">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <MacroExpansion>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "VoiceKitCore"
            BuildableName = "VoiceKitCore"
            BlueprintName = "VoiceKitCore"
            ReferencedContainer = "container:">
         </BuildableReference>
      </MacroExpansion>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>
```
.swiftpm/xcode/xcshareddata/xcschemes/VoiceKitUI.xcscheme
```
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "2600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitUI"
               BuildableName = "VoiceKitUI"
               BlueprintName = "VoiceKitUI"
               ReferencedContainer = "container:">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <MacroExpansion>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "VoiceKitUI"
            BuildableName = "VoiceKitUI"
            BlueprintName = "VoiceKitUI"
            ReferencedContainer = "container:">
         </BuildableReference>
      </MacroExpansion>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>
```
.swiftpm/xcode/xcshareddata/xcschemes/VoiceKitUITests.xcscheme
```
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "2600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES"
      buildArchitectures = "Automatic">
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      shouldAutocreateTestPlan = "YES">
      <Testables>
         <TestableReference
            skipped = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "VoiceKitUITests"
               BuildableName = "VoiceKitUITests"
               BlueprintName = "VoiceKitUITests"
               ReferencedContainer = "container:">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>
```
CHANGELOG.md
```md
# Changelog

## [0.1.0] — 2025-09-12
- Initial public extract: VoiceKitCore and VoiceKitUI
- RealVoiceIO with Swift 6 concurrency safety
- ScriptedVoiceIO for tests/demos
- VoicePickerView + VoiceProfilesStore (profiles, favorites, active/hidden, language filter, live previews)
- NameMatch utilities (normalizeKey + stringDistanceScore)
- Package tests: NameMatch and ScriptedVoiceIO
```
COLLABORATION.md
```md
# VoiceKit Collaboration & AI Co‑Author Statement

Project vision
- VoiceKit is a collaborative effort to demonstrate that an AI (OpenAI GPT‑5), guided by a human maintainer, can design, implement, document, and test a production‑friendly Swift package.
- The goal is not to replace people, but to accelerate human creativity and engineering with a careful, auditable AI workflow.

What “AI co‑author” means here
- Architecture and code were produced through iterative prompt/response sessions with GPT‑5, reviewed and integrated by the human maintainer (rdoggett).
- The AI contributed significant portions of:
  - Core engine (RealVoiceIO, ScriptedVoiceIO) with Swift 6 actor‑safety
  - TTS models and protocols
  - Name normalization and fuzzy matching
  - SwiftUI VoicePicker UI with persistence
  - Tests and developer documentation
- The human maintainer directed scope, validated design choices, resolved environment issues, and ensured code quality, correctness, and fit.

How we work (reproducible, incremental)
- Keep changes small, buildable, and testable. Iterate in green increments.
- Preserve a “working brief” (docs) to onboard a fresh assistant and reproduce context quickly.
- Prefer deterministic tests (ScriptedVoiceIO) for CI, and keep device‑dependent tests at the app level.
- Treat concurrency and platform APIs as first‑class constraints; document safety decisions.

Attribution
- Human maintainer: rdoggett (project owner), code review, integration, and releases.
- AI co‑author: GPT‑5 (OpenAI), architecture, implementation, docs, and tests.
- Code is licensed MIT; see LICENSE.

Contribution guidelines (short)
- Be clear if a change is primarily AI‑generated, human‑written, or pair‑authored:
  - Commit prefixes: ai:, human:, pair:
  - PR template: include “How produced” and “How verified” sections.
- Tests required for non‑trivial changes; prefer deterministic coverage in the package.
- Keep public APIs @MainActor where appropriate; avoid passing @MainActor closures to background-only callbacks; consult docs/Concurrency.md.
- Document notable design decisions in docs/ProgrammersGuide.md or inline with concise comments.

Scope and limits
- VoiceKit targets iOS 17+ and macOS 14+ with Swift 6 language mode.
- RealVoiceIO uses Apple frameworks (AVFoundation, Speech); ScriptedVoiceIO avoids hardware/permissions for CI.
- This repo does not ship or bundle proprietary model weights—only Swift code and documentation.

Ethics and transparency
- We disclose AI authorship because provenance matters. This repo is a case study in safe, auditable AI-assisted engineering.
- Review remains human‑led. Every merged change should be understandable and maintainable by human contributors.

Releases and versioning
- Versions derive from git tags (e.g., v0.1.0).
- For runtime diagnostics, you may expose a constant like `VoiceKitInfo.version` (optional).

Acknowledgements
- Thanks to everyone exploring sustainable human+AI collaboration.
- Special thanks to rdoggett for the vision and guidance—and to the Swift community for the ecosystem that makes packages like this possible.

— VoiceKit maintainers (human + AI)

```
Docs/Concurrency.md
```md
# Concurrency and Thread Safety (Swift 6)

Design
- Public API is @MainActor (VoiceIO protocol, RealVoiceIO, ScriptedVoiceIO).
- UI callbacks (onTranscriptChanged, onLevelChanged, etc.) are invoked on the main actor.

Permission callbacks (TCC)
- Apple’s TCC APIs may deliver on background queues. Passing @MainActor closures causes libdispatch assertions.
- Solution: PermissionBridge (nonisolated) wraps:
  - iOS 17+: AVAudioApplication.requestRecordPermission(completionHandler:)
  - macOS: AVCaptureDevice.requestAccess(for: .audio, completionHandler:)
  - Speech: SFSpeechRecognizer.requestAuthorization(_:)
- We await withCheckedContinuation and resume; execution returns to @MainActor after suspension safely.

Audio engine tap
- Core Audio calls back on a real-time thread.
- We avoid capturing @MainActor self; we compute meter levels and post to main via a tiny @unchecked Sendable LevelSink wrapper.

AVSpeechSynthesizer delegate
- We avoid sending AVSpeechUtterance across actors by:
  - Computing ObjectIdentifier(utterance) and copying Strings inside the delegate.
  - Hopping to @MainActor with those Sendable values.

Notifications (iOS)
- Interruption/route-change notifications are observed on the main queue to avoid queue assertions in handlers.

Do / Don’t
- Do call VoiceIO methods from @MainActor contexts (SwiftUI actions, @MainActor ViewModels).
- Don’t mutate UI/SwiftData from background contexts around VoiceIO callbacks.
- Don’t pass @MainActor closures into TCC/AVFoundation callbacks; use nonisolated bridges instead.
```
Docs/FAQ.md
```md
# FAQ

Q: “BUG IN CLIENT OF LIBDISPATCH: Block was expected to execute on queue […]”
- Cause: @MainActor closures passed to permission APIs (TCC) that callback on background queues.
- Fix: Use the provided PermissionBridge (nonisolated) and await continuations.

Q: “Sending 'utterance' risks causing data races”
- Fix: The delegate copies strings and uses ObjectIdentifier(utterance) before hopping to @MainActor; update to the latest VoiceIO.swift.

Q: “Ambiguous use of ScriptedVoiceIO.init(fromBase64:)”
- Cause: Duplicate type in app and package.
- Fix: Remove the app’s ScriptedVoiceIO from Target Membership; use the package type.

Q: Picker types not found (TTSVoiceProfile etc.)
- Cause: App compiling an old in-app picker.
- Fix: Use VoiceKitUI’s picker and import VoiceKitUI.

Q: Local vs remote package?
- During active dev, use local path (fastest).
- For releases, tag and point your app to the remote GitHub dependency. You can switch back to local later.

Q: Minimum OS / Swift?
- iOS 17.0+, macOS 14.0+, Swift 6 language mode.

Q: Where is the version defined?
- SPM uses git tags. If you need a runtime constant, add VoiceKitInfo.version in VoiceKitCore.
```
Docs/ProgrammersGuide.md
```md
# Programmer’s Guide

Architecture
- VoiceKitCore
  - VoiceIO protocol: speak/listen/boosted playback with UI callbacks.
  - RealVoiceIO: AVSpeechSynthesizer + SFSpeechRecognizer + AVAudioEngine implementation.
  - ScriptedVoiceIO: deterministic “fake mic” for tests/demos.
  - TTS models: TTSVoiceInfo, TTSVoiceProfile, TTSMasterControl; TTSConfigurable protocol.
  - RecognitionContext: STT hints (freeform, name(allowed:), number).
  - NameMatch: normalizeKey + stringDistanceScore.
- VoiceKitUI
  - VoicePickerView: SwiftUI picker with favorites, active/hidden, language filter, live previews.
  - VoiceProfilesStore: JSON persistence (profiles, default, active, master).

Data flow
1) ViewModel calls RealVoiceIO.ensurePermissions/configureSession.
2) speak() produces TTS with pulse callbacks for animation; listen() captures audio and streams to Speech for partial/final transcripts (with optional recording/trim).
3) VoicePickerView uses TTSConfigurable and VoiceProfilesStore to list/preview/update voices and master controls.

Key APIs (Core)
- VoiceIO (all @MainActor)
  - onListeningChanged/onTranscriptChanged/onLevelChanged
  - onTTSSpeakingChanged/onTTSPulse
  - ensurePermissions(), configureSessionIfNeeded()
  - speak(_ text:), listen(timeout:inactivity:record:)
  - prepareBoosted/startPreparedBoosted/playBoosted
  - stopAll(), hardReset()
- RealVoiceIO: implements VoiceIO + TTSConfigurable
- ScriptedVoiceIO: implements VoiceIO (deterministic scripts)
- NameMatch.normalizeKey(String) -> String
- NameMatch.stringDistanceScore(a:b) -> Double

STT hints (RecognitionContext)
- .freeform (default)
- .name(allowed: [String]) sets contextualStrings
- .number sets numeric contextualStrings and normalizes spelled numbers when possible

Persistence (VoicePicker)
- VoiceProfilesStore writes JSON to Application Support.
- Fields: defaultVoiceID, master, profilesByID, activeVoiceIDs.

Customization
- Replace RealVoiceIO with your own TTS engine by conforming to TTSConfigurable; the picker works unchanged.
- Build custom voice UI by reusing VoiceProfilesStore directly.

See also
- docs/VoiceIO.md for API details and examples.
- docs/Concurrency.md for Swift 6 isolation notes.
- docs/VoicePicker.md for UI details and customization.
```
Docs/QuickReference.md
```md
# VoiceKit Quick Reference

Import
- Core: import VoiceKitCore
- UI: import VoiceKitUI

Primary types
- RealVoiceIO (@MainActor): production speech + recognition engine.
- ScriptedVoiceIO (@MainActor): deterministic test engine; no hardware.
- VoiceQueue (@MainActor): kid‑friendly sequencing of speak/SFX/pause with optional parallel channels.
- VoicePickerView (SwiftUI): voices UI with profiles and live preview.

Common flows
- Permissions + session
```swift
let voice = RealVoiceIO()
try await voice.ensurePermissions()
try await voice.configureSessionIfNeeded()
```

- Speak
```swift
await voice.speak("Hello there!")
```

- Listen (with numeric hints)
```swift
let r = try await voice.listen(timeout: 6, inactivity: 2, record: false,
                               context: RecognitionContext(expectation: .number))
print(r.transcript)
```

- Play a short clip (boosted)
```swift
let url = Bundle.main.url(forResource: "ding", withExtension: "caf")!
try await voice.playBoosted(url: url, gainDB: 6)
```

- Sequence speak → sfx → speak (near‑zero gap)
```swift
let q = VoiceQueue(primary: voice)
q.enqueueSpeak("Thank you.", voiceID: nil)
q.enqueueSFX(url) // pre-scheduled; will auto-fire right after speak
q.enqueueSpeak("Next question.")
await q.play()
```

- Embedded SFX in text
```swift
let resolver: VoiceQueue.SFXResolver = { name in Bundle.main.url(forResource: name, withExtension: "caf") }
let q = VoiceQueue(primary: voice)
q.enqueueParsingSFX(text: "Say your name [sfx:ding] now.", resolver: resolver)
await q.play()
```

- Parallel channels (optional)
```swift
let q = VoiceQueue(primary: RealVoiceIO()) { RealVoiceIO() } // channel factory
q.enqueueSpeak("Left", on: 0)
q.enqueueSpeak("Right", on: 1)
await q.play() // channels 0 and 1 run concurrently
```

- Cancel everything
```swift
q.cancelAll()     // cancels queued playback
voice.stopAll()   // stops any active speech/recording/clip at engine level
```

Picker UI
```swift
import VoiceKitCore
import VoiceKitUI
let voice = RealVoiceIO()
VoicePickerView(tts: voice)
```

Diagnostics
```swift
print("VoiceKit", VoiceKitInfo.version)
```
```
Docs/QuickStart.md
```md
# Quick Start

1) Add the package
- Local path (fastest while iterating): File > Add Packages… > Add Local… and choose the VoiceKit folder.
- Remote URL (for releases): Add Packages… > paste the GitHub URL; select “Up to Next Major” starting at v0.1.0.

2) Link products
- In your app target, General > Frameworks, Libraries, and Embedded Content:
  - Add VoiceKitCore and VoiceKitUI (Do Not Embed).

3) App permissions
- Add to Info.plist:
  - NSMicrophoneUsageDescription
  - NSSpeechRecognitionUsageDescription

4) Use RealVoiceIO
```swift
import VoiceKitCore

@MainActor
final class MyVM: ObservableObject {
    let voice = RealVoiceIO()
    func go() {
        Task {
            try? await voice.ensurePermissions()
            try? await voice.configureSessionIfNeeded()
            await voice.speak("Hello!")
            let r = try? await voice.listen(timeout: 8, inactivity: 2, record: false)
            print("Transcript:", r?.transcript ?? "")
        }
    }
}
```

5) Add the picker UI
```swift
import VoiceKitCore
import VoiceKitUI

struct SettingsView: View {
    let voice = RealVoiceIO()
    var body: some View {
        VoicePickerView(tts: voice)
    }
}
```

6) Tests without hardware
- Use ScriptedVoiceIO:
```swift
import VoiceKitCore
import XCTest

@MainActor
final class ScriptTests: XCTestCase {
    func testListen() async throws {
        let b64 = try! JSONSerialization.data(withJSONObject: ["alpha","beta"]).base64EncodedString()
        let io = ScriptedVoiceIO(fromBase64: b64)!
        let r1 = try await io.listen(timeout: 1.5, inactivity: 0.4, record: false)
        XCTAssertEqual(r1.transcript, "alpha")
    }
}
```
```
Docs/Testing.md
```md
# Testing

Deterministic tests with ScriptedVoiceIO
```swift
import VoiceKitCore
import XCTest

@MainActor
final class ScriptedTests: XCTestCase {
    func testFlow() async throws {
        let data = try! JSONSerialization.data(withJSONObject: ["hello","world"])
        let io = ScriptedVoiceIO(fromBase64: data.base64EncodedString())!
        let r1 = try await io.listen(timeout: 1.5, inactivity: 0.4, record: false)
        XCTAssertEqual(r1.transcript, "hello")
    }
}
```

Package tests (included)
- NameMatchTests: normalization and distance
- ScriptedVoiceIOPackageTests: basic listen flow
- CoreSanityTests: public types reachable

RealVoiceIO tests (app-level)
- Require simulator/device permissions.
- Avoid calling ensurePermissions from non-main contexts.
- Prefer small integration tests; heavy audio verification belongs in manual QA or specialized harnesses.

CI
- Recommended GitHub Actions workflow:
  - macos-latest
  - Build VoiceKit
  - Run package tests
  - (Optional) lint Swift format
```
Docs/VoiceIO.md
```md
# VoiceIO API Reference

Import
```swift
import VoiceKitCore
```

VoiceIO (protocol, @MainActor)
```swift
public protocol VoiceIO: AnyObject {
    var onListeningChanged: ((Bool) -> Void)? { get set }
    var onTranscriptChanged: ((String) -> Void)? { get set }
    var onLevelChanged: ((CGFloat) -> Void)? { get set }
    var onTTSSpeakingChanged: ((Bool) -> Void)? { get set }
    var onTTSPulse: ((CGFloat) -> Void)? { get set }
    var onStatusMessageChanged: ((String?) -> Void)? { get set }

    func ensurePermissions() async throws
    func configureSessionIfNeeded() async throws

    func speak(_ text: String) async
    func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult

    func prepareBoosted(url: URL, gainDB: Float) async throws
    func startPreparedBoosted() async throws
    func playBoosted(url: URL, gainDB: Float) async throws

    func stopAll()
    func hardReset()
}
```

Implementations
- RealVoiceIO (@MainActor)
  - Uses AVSpeechSynthesizer for TTS; SFSpeechRecognizer + AVAudioEngine for STT.
  - Conforms to TTSConfigurable.
  - VoiceResult contains transcript and optional trimmed recording URL when record = true.
- ScriptedVoiceIO (@MainActor)
  - Base64-encoded JSON array of strings; listen() dequeues and returns them; TTS is mocked with pulse callbacks.

TTSConfigurable
```swift
@MainActor
public protocol TTSConfigurable: AnyObject {
    func availableVoices() -> [TTSVoiceInfo]
    func setVoiceProfile(_ profile: TTSVoiceProfile)
    func getVoiceProfile(id: String) -> TTSVoiceProfile?
    func setDefaultVoiceProfile(_ profile: TTSVoiceProfile)
    func getDefaultVoiceProfile() -> TTSVoiceProfile?
    func setMasterControl(_ master: TTSMasterControl)
    func getMasterControl() -> TTSMasterControl
    func speak(_ text: String, using voiceID: String?) async
    func stopSpeakingNow()
}
```

Models
```swift
public struct TTSVoiceInfo { public let id, name, language: String }
public struct TTSVoiceProfile { public let id: String; public var displayName: String; public var rate, pitch, volume: Float; public var isSelected, isHidden: Bool }
public struct TTSMasterControl { public var volume, pitchVariation, rateVariation: Float }
public struct VoiceResult { public let transcript: String; public let recordingURL: URL? }
public struct RecognitionContext { public enum Expectation { case freeform, name(allowed: [String]), number } }
```

Examples
Speak and listen
```swift
let voice = RealVoiceIO()
try await voice.ensurePermissions()
try await voice.configureSessionIfNeeded()
await voice.speak("Please say your favorite number.")
let r = try await voice.listen(timeout: 6, inactivity: 2, record: false)
print(r.transcript)
```

Use STT hints
```swift
let r = try await voice.listen(timeout: 6, inactivity: 2, record: false,
                               context: RecognitionContext(expectation: .number))
```

Boosted short-clip playback
```swift
let url = Bundle.main.url(forResource: "ding", withExtension: "caf")!
try await voice.prepareBoosted(url: url, gainDB: 6)
try await voice.startPreparedBoosted()
```
```
Docs/VoicePicker.md
```md
# VoicePickerView (VoiceKitUI)

Import
```swift
import VoiceKitCore
import VoiceKitUI
```

What it provides
- A ready-to-use SwiftUI screen for system voices with:
  - Favorites (default voice), Active toggles, Hide/Unhide
  - Current-language filter (or All)
  - Master sliders (Volume, Pitch range, Speed range) with immediate effect
  - Live preview when sliders change or when tapping a row
- Persistence via VoiceProfilesStore (JSON in Application Support).

Usage
```swift
let voice = RealVoiceIO()
VoicePickerView(tts: voice) // creates its own VoiceProfilesStore
```
Or supply a shared store:
```swift
let store = VoiceProfilesStore(filename: "voices.json")
VoicePickerView(tts: voice, store: store)
```

Persistence model (VoiceProfilesStore)
- defaultVoiceID: String?
- master: TTSMasterControl
- profilesByID: [String: TTSVoiceProfile]
- activeVoiceIDs: Set<String>
- Methods: load(), save(), profile(for:), setProfile(_:), toggleActive(_), setHidden(_:_:)

Customization tips
- Filter behavior: set languageFilter (.current or .all) and showHidden flag on the ViewModel (exposed internally by the view).
- Theming: wrap VoicePickerView in your own NavigationView; apply .tint or environment modifiers.
- Alternate TTS engine: implement TTSConfigurable; the picker will use your availableVoices, set/get profile methods, and speak(_:using:).

Sample integration
```swift
struct SettingsView: View {
    let voice = RealVoiceIO()
    var body: some View { VoicePickerView(tts: voice) }
}
```
```
LICENSE
```
MIT License

Copyright (c) 2025 rdoggett

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice (including the next
paragraph) shall be included in all copies or substantial portions of the
Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
Package.swift
```swift
// swift-tools-version: 6.0
//
//  Package.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import PackageDescription

let package = Package(
    name: "VoiceKit",
    defaultLocalization: "en",
    platforms: [
        .iOS("17.0"),
        .macOS("14.0")
    ],
    products: [
        .library(name: "VoiceKitCore", targets: ["VoiceKitCore"]),
        .library(name: "VoiceKitUI", targets: ["VoiceKitUI"])
    ],
    dependencies: [
    ],
    targets: [
        .target(
            name: "VoiceKitCore",
            path: "Sources/VoiceKitCore"
        ),
        .target(
            name: "VoiceKitUI",
            dependencies: ["VoiceKitCore"],
            path: "Sources/VoiceKitUI"
        ),
        .testTarget(
            name: "VoiceKitCoreTests",
            dependencies: ["VoiceKitCore"],
            path: "Tests/VoiceKitCoreTests"
        ),
        .testTarget(
            name: "VoiceKitUITests",
            dependencies: ["VoiceKitCore", "VoiceKitUI"],
            path: "Tests/VoiceKitUITests"
        )
    ],
    swiftLanguageModes: [
        .v6
    ]
)
```
README.md
```md
# VoiceKit

Reusable voice I/O for SwiftUI apps (iOS 17+, macOS 14+)
[![CI](https://github.com/rdoggett/VoiceKit/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/rdoggett/VoiceKit/actions/workflows/ci.yml?query=branch%3Amain)
Products
- VoiceKitCore: Voice I/O engine (RealVoiceIO), deterministic test engine (ScriptedVoiceIO), STT hints, TTS models, NameMatch utilities.
- VoiceKitUI: VoicePickerView with persistence (profiles, favorites, active/hidden, language filter, live previews).

Highlights
- Swift 6 actor-safety: main-actor public API, safe bridging for permission callbacks, audio tap isolation, TTS delegate isolation.
- Drop-in UI picker with user profiles and live “preview as you slide” behavior.
- Deterministic ScriptedVoiceIO for tests and demos.
- Lightweight utilities for name normalization and fuzzy matching.

Requirements
- Swift tools-version: 6.0 (swiftLanguageModes [.v6])
- iOS 17.0+ and/or macOS 14.0+
- App Info.plist keys:
  - NSMicrophoneUsageDescription
  - NSSpeechRecognitionUsageDescription

Quick start
```swift
import VoiceKitCore

@MainActor
final class DemoVM: ObservableObject {
    let voice = RealVoiceIO()
    func run() {
        Task {
            try? await voice.ensurePermissions()
            try? await voice.configureSessionIfNeeded()
            await voice.speak("Say your name after the beep.")
            let r = try? await voice.listen(timeout: 8, inactivity: 2, record: true)
            print("Heard:", r?.transcript ?? "(none)")
        }
    }
}
```

Voice picker UI
```swift
import VoiceKitCore
import VoiceKitUI
import SwiftUI

struct SettingsView: View {
    let voice = RealVoiceIO()
    var body: some View {
        VoicePickerView(tts: voice)
    }
}
```

Docs
- docs/QuickStart.md
- docs/ProgrammersGuide.md
- docs/VoiceIO.md
- docs/VoicePicker.md
- docs/Concurrency.md
- docs/Testing.md
- docs/FAQ.md
- CHANGELOG.md

Install
- Local (recommended while iterating): Add Package… > Add Local… and pick the VoiceKit folder; link VoiceKitCore and (optionally) VoiceKitUI (Do Not Embed).
- Remote (from GitHub): Add Packages… > enter the repo URL; rule “Up to Next Major” from your tag (e.g., v0.1.0).

License
- MIT — see LICENSE.
```
Sources/VoiceKitCore/NameMatch.swift
```swift
//
//  NameMatch.swift
//  VoiceKitCore
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-12-2025
//
//  Public normalization and fuzzy matching utilities for user-visible names.
//

import Foundation

public enum NameMatch: Sendable {

    /// Normalize a string for stable matching:
    /// - Expands common ligatures (Æ→AE, æ→ae, Œ→OE, œ→oe).
    /// - Folds case and removes diacritics.
    /// - Removes apostrophes (straight and curly).
    /// - Removes non-letter/number/space/hyphen characters.
    /// - Collapses whitespaces.
    /// - Converts to lowercase.
    public static func normalizeKey(_ s: String) -> String {
        // 1) Normalize common ligatures first
        var t = s
            .replacingOccurrences(of: "Æ", with: "AE")
            .replacingOccurrences(of: "æ", with: "ae")
            .replacingOccurrences(of: "Œ", with: "OE")
            .replacingOccurrences(of: "œ", with: "oe")

        // 2) Unify dash variants to ASCII hyphen-minus
        //    Includes: hyphen (2010), non-breaking hyphen (2011), figure dash (2012),
        //    en dash (2013), em dash (2014), minus sign (2212)
        let dashVariants = ["\u{2010}","\u{2011}","\u{2012}","\u{2013}","\u{2014}","\u{2212}"]
        for d in dashVariants { t = t.replacingOccurrences(of: d, with: "-") }

        // 3) Remove soft/zero-width characters that should not affect matching
        //    Soft hyphen (00AD), zero-width space/joiners (200B, 200C, 200D), word joiner (2060)
        let invisibles = ["\u{00AD}","\u{200B}","\u{200C}","\u{200D}","\u{2060}"]
        for ch in invisibles { t = t.replacingOccurrences(of: ch, with: "") }

        // 4) Fold diacritics and case
        t = t.folding(options: [.diacriticInsensitive, .caseInsensitive], locale: .autoupdatingCurrent)

        // 5) Remove apostrophes (straight and curly)
        t = t.replacingOccurrences(of: #"['’]"#, with: "", options: .regularExpression)

        // 6) Keep only letters, numbers, spaces, and ASCII hyphen
        t = t.replacingOccurrences(of: #"[^\p{L}\p{N} \-]"#, with: "", options: .regularExpression)

        // 7) Collapse repeated hyphens and spaces; trim ends
        t = t.replacingOccurrences(of: #"\-+"#, with: "-", options: .regularExpression)
        t = t.replacingOccurrences(of: #"\s+"#, with: " ", options: .regularExpression)
        t = t.trimmingCharacters(in: CharacterSet(charactersIn: " -"))

        // 8) Lowercase final key
        return t.lowercased()
    }

    /// Compute a token-aware normalized edit-distance score between two strings.
    ///
    /// - Splits inputs into tokens by space and pairs them to minimize distance.
    /// - Uses Levenshtein edit distance per token pair, normalized by token length.
    /// - Penalizes unmatched tokens to maintain a score reflecting missing tokens.
    public static func stringDistanceScore(a: String, b: String) -> Double {
        if a == b { return 0 }
        let at = a.split(separator: " ").map(String.init)
        let bt = b.split(separator: " ").map(String.init)
        if at.isEmpty || bt.isEmpty { return 1 }

        var used = Array(repeating: false, count: bt.count)
        var scores: [Double] = []

        for t in at {
            var best: Double = 1
            var bestIdx = -1
            for (i, u) in bt.enumerated() where !used[i] {
                let d = Double(levenshtein(t, u)) / Double(max(t.count, u.count))
                if d < best { best = d; bestIdx = i }
            }
            if bestIdx >= 0 { used[bestIdx] = true }
            scores.append(best)
        }
        let avg = scores.reduce(0, +) / Double(scores.count)
        let unmatched = bt.count - used.filter { $0 }.count
        let penalty = bt.isEmpty ? 0 : Double(unmatched) / Double(bt.count)
        return min(1.0, avg * 0.8 + penalty * 0.2)
    }

    // MARK: - Private helpers

    private static func levenshtein(_ aStr: String, _ bStr: String) -> Int {
        let a = Array(aStr), b = Array(bStr)
        let m = a.count, n = b.count
        if m == 0 { return n }
        if n == 0 { return m }
        var dp = Array(repeating: Array(repeating: 0, count: n + 1), count: m + 1)
        for i in 0...m { dp[i][0] = i }
        for j in 0...n { dp[0][j] = j }
        for i in 1...m {
            for j in 1...n {
                let cost = a[i - 1] == b[j - 1] ? 0 : 1
                dp[i][j] = min(
                    dp[i - 1][j] + 1,        // deletion
                    dp[i][j - 1] + 1,        // insertion
                    dp[i - 1][j - 1] + cost  // substitution
                )
            }
        }
        return dp[m][n]
    }
}
```
Sources/VoiceKitCore/NameResolver.swift
```swift
//
//  NameResolver.swift
//  VoiceKitCore
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//
//  Strict name resolution with a “clean edges” return when no allowed list,
//  and exact match after robust normalization when allowed is provided.
//

import Foundation

public struct NameResolver: Sendable {

    public enum Policy: Sendable {
        case strictExact // exact match after normalization; no nicknames/aliases
    }

    public var policy: Policy = .strictExact

    public init(policy: Policy = .strictExact) {
        self.policy = policy
    }

    /// Resolve a spoken name to an allowed list.
    /// - Behavior:
    ///   - If `allowed` is nil or empty: return the raw input with leading/trailing
    ///     whitespace and punctuation trimmed (preserving original case/diacritics).
    ///   - Else: normalize both transcript and allowed strings by folding case/diacritics,
    ///     removing zero-width/soft hyphens, converting punctuation (including hyphen/dash
    ///     variants) to spaces, and collapsing whitespace. Return the original allowed string
    ///     when a normalized exact match is found; otherwise nil.
    public func resolve(transcript raw: String, allowed: [String]?) -> String? {
        guard let allowed, !allowed.isEmpty else {
            return cleanEdgesPreservingCase(raw)
        }

        // Build normalized lookup → original allowed string
        var table: [String: String] = [:]
        for a in allowed {
            table[normalizeForMatch(a)] = a
        }

        switch policy {
        case .strictExact:
            let key = normalizeForMatch(raw)
            return table[key]
        }
    }

    // MARK: - Helpers

    /// Trim whitespace and leading/trailing punctuation without changing case/diacritics.
    private func cleanEdgesPreservingCase(_ s: String) -> String {
        let trimmed = s.trimmingCharacters(in: .whitespacesAndNewlines)
        // Also trim punctuation at the edges (e.g., "Max!" -> "Max")
        let edgeTrimmed = trimmed.trimmingCharacters(in: .punctuationCharacters)
        return edgeTrimmed
    }

    /// Normalize for strict matching:
    /// - Remove zero-width/soft hyphens.
    /// - Convert hyphen/dash variants and all punctuation to spaces.
    /// - Collapse whitespace to single spaces; trim.
    /// - Fold diacritics and case (lowercased result).
    private func normalizeForMatch(_ s: String) -> String {
        // Remove invisible/soft joiners that can sneak into transcripts
        var t = s
        let invisibles = ["\u{00AD}", // soft hyphen
                          "\u{200B}", // zero-width space
                          "\u{200C}", // zero-width non-joiner
                          "\u{200D}", // zero-width joiner
                          "\u{2060}"] // word joiner
        for ch in invisibles { t = t.replacingOccurrences(of: ch, with: "") }

        // Turn hyphen/dash variants (and ASCII '-') into spaces to align "Jean‑Luc" with "Jean Luc"
        let dashVariants = ["-", "\u{2010}", "\u{2011}", "\u{2012}", "\u{2013}", "\u{2014}", "\u{2212}"]
        for d in dashVariants { t = t.replacingOccurrences(of: d, with: " ") }

        // Replace all remaining punctuation with spaces (commas, periods, quotes, etc.)
        t = t.replacingOccurrences(of: #"[\p{P}]"#, with: " ", options: .regularExpression)

        // Collapse whitespace and trim
        t = t.replacingOccurrences(of: #"\s+"#, with: " ", options: .regularExpression)
            .trimmingCharacters(in: .whitespacesAndNewlines)

        // Fold diacritics and case for matching
        t = t.folding(options: [.diacriticInsensitive, .caseInsensitive], locale: .autoupdatingCurrent)

        return t
    }
}
```
Sources/VoiceKitCore/PermissionBridge.swift
```swift
//
//  PermissionBridge.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-16-2025
//

import Foundation
@preconcurrency import Speech
@preconcurrency import AVFoundation

enum PermissionBridge {

    nonisolated static func awaitSpeechAuth() async -> SFSpeechRecognizerAuthorizationStatus {
        await withCheckedContinuation { (c: CheckedContinuation<SFSpeechRecognizerAuthorizationStatus, Never>) in
            SFSpeechRecognizer.requestAuthorization { status in
                c.resume(returning: status)
            }
        }
    }

    #if os(iOS)
    nonisolated static func awaitMicPermission() async -> Bool {
        await withCheckedContinuation { (c: CheckedContinuation<Bool, Never>) in
            AVAudioApplication.requestRecordPermission { granted in
                c.resume(returning: granted)
            }
        }
    }
    #elseif os(macOS)
    nonisolated static func awaitMicPermission() async -> Bool {
        await withCheckedContinuation { (c: CheckedContinuation<Bool, Never>) in
            AVCaptureDevice.requestAccess(for: .audio) { granted in
                c.resume(returning: granted)
            }
        }
    }
    #endif
}
```
Sources/VoiceKitCore/RealVoiceIO+Boosted.swift
```swift
//
//  RealVoiceIO+Boosted.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
@preconcurrency import AVFoundation
#if canImport(AVFAudio)
@preconcurrency import AVFAudio
#endif

// MARK: - Private queue actor

actor BoostQueue {
    private var scheduled = 0
    private var completed = 0
    func scheduleOne() { scheduled &+= 1 }
    func completeOne() { completed &+= 1 }
    func hasNothing() -> Bool { scheduled == 0 }
    func isDone() -> Bool { scheduled > 0 && completed >= scheduled }
    func reset() { scheduled = 0; completed = 0 }
}

@MainActor
extension RealVoiceIO {

    // MARK: - Engine wiring

    func ensureBoostEngine(format: AVAudioFormat) throws {
        if boostEngine == nil {
            let engine = AVAudioEngine()
            let player = AVAudioPlayerNode()
            let eq = AVAudioUnitEQ(numberOfBands: 0)
            engine.attach(player); engine.attach(eq)
            engine.connect(player, to: eq, format: format)
            engine.connect(eq, to: engine.mainMixerNode, format: format)
            try engine.start()
            boostEngine = engine; boostPlayer = player; boostEQ = eq
        } else if boostEngine?.isRunning == false {
            try boostEngine?.start()
        }
    }

    // MARK: - API

    public func prepareBoosted(url: URL, gainDB: Float) async throws {
        await opGate.acquire()
        defer { Task { await opGate.release() } }

        let file = try AVAudioFile(forReading: url)
        let format = file.processingFormat
        let totalFrames = AVAudioFrameCount(file.length)
        guard totalFrames > 0,
              let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: totalFrames) else {
            throw SimpleError("Failed to allocate audio buffer.")
        }
        try file.read(into: buffer)
        buffer.frameLength = totalFrames
        guard buffer.frameLength > 0 else { throw SimpleError("Audio buffer empty.") }

        try ensureBoostEngine(format: format)
        boostEQ?.globalGain = max(0, min(gainDB, 24))
        guard let player = boostPlayer else { return }

        await boostQueue.scheduleOne()
        autoStartPreparedAfterTTS = true

        let playerRef = player
        let bRef = buffer
        let boostQueueRef = boostQueue

        playerRef.scheduleBuffer(bRef, at: nil, options: [], completionCallbackType: .dataPlayedBack) { _ in
            Task {
                await boostQueueRef.completeOne()
                let done = await boostQueueRef.isDone()
                if done {
                    await MainActor.run { [weak self] in
                        self?.completeBoostWaiters()
                    }
                }
            }
        }
    }

    public func startPreparedBoosted() async throws {
        if await boostQueue.hasNothing() {
            stopBoostedPlayback()
            return
        }
        guard let player = boostPlayer else {
            stopBoostedPlayback()
            return
        }
        if !autoStartPreparedAfterTTS, !player.isPlaying { player.play() }

        if await boostQueue.isDone() {
            stopBoostedPlayback()
            return
        }

        let timeout = max(0.25, config.boostedWaitTimeoutSeconds)
        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            let id = UUID()
            boostWaiters[id] = cont
            let boostQueueRef = boostQueue

            // Complete early if already done
            Task {
                let done = await boostQueueRef.isDone()
                if done {
                    await MainActor.run { [weak self] in self?.finishBoostWaiter(id: id) }
                }
            }

            // Timeout guard
            Task {
                try? await Task.sleep(nanoseconds: UInt64(timeout * 1_000_000_000))
                await MainActor.run { [weak self] in self?.finishBoostWaiter(id: id) }
            }
        }
    }

    public func playBoosted(url: URL, gainDB: Float) async throws {
        try await prepareBoosted(url: url, gainDB: gainDB)
        try await startPreparedBoosted()
    }

    // MARK: - Internals

    func completeBoostWaiters() {
        stopBoostedPlayback()
        let waiters = boostWaiters
        boostWaiters.removeAll()
        for (_, cont) in waiters { cont.resume() }
    }

    func finishBoostWaiter(id: UUID) {
        if let c = boostWaiters.removeValue(forKey: id) {
            stopBoostedPlayback()
            c.resume()
        }
    }

    func stopBoostedPlayback() {
        boostPlayer?.stop()
        boostEngine?.stop()
        boostEQ = nil
        boostPlayer = nil
        boostEngine = nil
        autoStartPreparedAfterTTS = false
        Task { await boostQueue.reset() }
    }
}
```
Sources/VoiceKitCore/RealVoiceIO+Interruption.swift
```swift
//
//  RealVoiceIO+Interruption.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
@preconcurrency import AVFoundation

@MainActor
extension RealVoiceIO {
    func installInterruptionObservers() {
        #if os(iOS)
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleInterruption(_:)),
            name: AVAudioSession.interruptionNotification,
            object: nil
        )
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleRouteChange(_:)),
            name: AVAudioSession.routeChangeNotification,
            object: nil
        )
        #endif
    }

    @objc func handleInterruption(_ note: Notification) {
        #if os(iOS)
        guard let info = note.userInfo,
              let typeValue = info[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else { return }
        switch type {
        case .began:
            wasInterrupted = true
            wasPlayingBoost = boostPlayer?.isPlaying ?? false
            boostPlayer?.pause()
            onListeningChanged?(false)
        case .ended:
            let optionsValue = info[AVAudioSessionInterruptionOptionKey] as? UInt
            let shouldResume = optionsValue.map { AVAudioSession.InterruptionOptions(rawValue: $0).contains(.shouldResume) } ?? true
            if shouldResume {
                try? AVAudioSession.sharedInstance().setActive(true, options: [])
                if wasPlayingBoost {
                    try? boostEngine?.start()
                    boostPlayer?.play()
                }
            }
            wasInterrupted = false
            wasPlayingBoost = false
        @unknown default:
            break
        }
        #endif
    }

    @objc func handleRouteChange(_ note: Notification) {
        #if os(iOS)
        guard let info = note.userInfo,
              let reasonValue = info[AVAudioSessionRouteChangeReasonKey] as? UInt,
              let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else { return }
        switch reason {
        case .oldDeviceUnavailable, .categoryChange, .override, .wakeFromSleep, .noSuitableRouteForCategory:
            if audioEngine?.isRunning == true { finishRecognition() }
        default:
            break
        }
        #endif
    }
}```
Sources/VoiceKitCore/RealVoiceIO+STT.swift
```swift
//
//  RealVoiceIO+STT.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
import CoreGraphics
@preconcurrency import AVFoundation
@preconcurrency import Speech
import Accelerate
import QuartzCore

@MainActor
extension RealVoiceIO {

    // MARK: - Listen

    public func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
        await opGate.acquire()
        defer { Task { await opGate.release() } }

        if Task.isCancelled { throw CancellationError() }
        await stopRecognitionAsync()

        #if os(iOS)
        if !AVAudioSession.sharedInstance().isInputAvailable { throw SimpleError("Mic unavailable.") }
        #endif

        latestTranscript = ""
        onTranscriptChanged?("")
        onLevelChanged?(0)
        hasFinishedRecognition = false
        firstSpeechStart = nil
        lastSpeechEnd = nil
        listenOutURL = nil
        listenOutFile = nil
        finishQueued = nil
        tapInstalled = false

        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil

        let engine = AVAudioEngine()
        audioEngine = engine

        let request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = true

        switch recognitionContext.expectation {
        case .freeform:
            request.taskHint = .dictation
        case .name(let allowed):
            request.taskHint = .dictation
            request.contextualStrings = allowed
        case .number:
            request.taskHint = .search
            request.contextualStrings = RecognitionContext.numericContextualStrings
        }

        if #available(iOS 13.0, macOS 10.15, *),
           let recognizer = speechRecognizer,
           recognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
        }
        recognitionRequest = request

        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw SimpleError("Recognizer unavailable.")
        }

        let inputNode = engine.inputNode
        let inputFormat = inputNode.inputFormat(forBus: 0)
        if inputFormat.channelCount == 0 || inputFormat.sampleRate <= 0 {
            throw SimpleError("Invalid input format.")
        }

        if record {
            let url = FileManager.default.temporaryDirectory.appendingPathComponent("rec-\(UUID().uuidString).caf")
            listenOutURL = url
            listenOutFile = try AVAudioFile(forWriting: url, settings: inputFormat.settings)
        }

        struct LevelSink: @unchecked Sendable {
            weak var owner: RealVoiceIO?
            func callAsFunction(_ value: CGFloat) {
                Task { @MainActor in owner?.onLevelChanged?(value) }
            }
        }
        let levelSink = LevelSink(owner: self)

        let reqRef = request
        let fileRef = listenOutFile

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { buffer, _ in
            let frames = Int(buffer.frameLength)
            guard frames > 0 else {
                levelSink(0)
                return
            }

            reqRef.append(buffer)
            if let fileRef { try? fileRef.write(from: buffer) }

            if let ch = buffer.floatChannelData?.pointee {
                var ms: Float = 0
                vDSP_measqv(ch, 1, &ms, vDSP_Length(frames))
                let db: Float = ms <= 0 ? -160 : 10 * log10f(ms)
                let norm = max(0, min(1, (db + 60) / 60))
                levelSink(CGFloat(norm))
            }
        }
        tapInstalled = true

        engine.prepare()
        try engine.start()
        onListeningChanged?(true)

        try await Task.sleep(nanoseconds: 50_000_000)
        if Task.isCancelled { throw CancellationError() }

        let task = recognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self else { return }
            if let r = result {
                let now = CACurrentMediaTime()
                var text = r.bestTranscription.formattedString
                let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                let segments = r.bestTranscription.segments

                if case .number = self.recognitionContext.expectation,
                   let normalized = Self.normalizeNumeric(from: text) {
                    text = normalized
                }

                Task { @MainActor in
                    if now >= self.ttsSuppressUntil, !trimmed.isEmpty {
                        self.latestTranscript = text
                        self.onTranscriptChanged?(text)
                        self.startInactivityTimer(seconds: inactivity)
                    }

                    if !segments.isEmpty {
                        var first = self.firstSpeechStart
                        var last = self.lastSpeechEnd
                        for seg in segments {
                            if first == nil { first = seg.timestamp }
                            last = max(last ?? 0, seg.timestamp + seg.duration)
                        }
                        self.firstSpeechStart = first
                        self.lastSpeechEnd = last
                    }

                    if r.isFinal { self.finishRecognition() }
                }
            }
            if error != nil {
                Task { @MainActor in self.finishRecognition() }
            }
        }
        recognitionTask = task

        startOverallTimer(seconds: timeout)

        let result: VoiceResult = try await withCheckedThrowingContinuation { (cont: CheckedContinuation<VoiceResult, Error>) in
            self.listenCont = cont
            if let queued = self.finishQueued {
                self.finishQueued = nil
                self.resumeListen(with: queued)
            }
        }

        if Task.isCancelled { throw CancellationError() }
        return result
    }

    // MARK: - Stop/Cleanup

    func stopRecognition() {
        recognitionTask?.cancel()
        recognitionRequest?.endAudio()
        finishRecognition()
    }

    func stopRecognitionAsync() async {
        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            stopRecognition()
            cont.resume()
        }
    }
}

// MARK: - Timers and finish

@MainActor
extension RealVoiceIO {

    static func normalizeNumeric(from text: String) -> String? {
        if let int = Int(text.trimmingCharacters(in: .whitespacesAndNewlines)) { return String(int) }
        let fmt = NumberFormatter()
        fmt.locale = .autoupdatingCurrent
        fmt.numberStyle = .spellOut
        if let n = fmt.number(from: text.lowercased()) { return n.stringValue }
        return nil
    }

    func startInactivityTimer(seconds: TimeInterval) {
        listenInactivityTask?.cancel()
        listenInactivityTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run { self?.finishRecognition() }
        }
    }

    func startOverallTimer(seconds: TimeInterval) {
        listenOverallTask?.cancel()
        listenOverallTask = Task { [weak self] in
            guard !Task.isCancelled else { return }
            try? await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            await MainActor.run { self?.finishRecognition() }
        }
    }

    func finishRecognition() {
        guard !hasFinishedRecognition else { return }
        hasFinishedRecognition = true

        recognitionRequest?.endAudio()
        audioEngine?.stop()
        removeTapIfInstalled()

        var finalURL: URL? = listenOutURL
        if let url = listenOutURL {
            if let trimmed = trimAudioSmart(inputURL: url,
                                            sttStart: firstSpeechStart,
                                            sttEnd: lastSpeechEnd,
                                            prePad: config.trimPrePad,
                                            postPad: config.trimPostPad) {
                finalURL = trimmed
            }
        }
        let result = VoiceResult(transcript: self.latestTranscript, recordingURL: finalURL)

        cleanupRecognition()
        listenInactivityTask?.cancel(); listenInactivityTask = nil
        listenOverallTask?.cancel(); listenOverallTask = nil
        onListeningChanged?(false)

        if listenCont != nil {
            resumeListen(with: result)
        } else {
            finishQueued = result
        }
    }

    func removeTapIfInstalled() {
        if tapInstalled {
            audioEngine?.inputNode.removeTap(onBus: 0)
            tapInstalled = false
        }
    }

    func resumeListen(with result: VoiceResult) {
        guard let cont = listenCont else { return }
        listenCont = nil
        finishQueued = nil
        listenOutFile = nil
        listenOutURL = nil
        cont.resume(returning: result)
    }

    func cleanupRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        audioEngine?.stop()
        audioEngine = nil
        tapInstalled = false
    }
}
```
Sources/VoiceKitCore/RealVoiceIO+TTS.swift
```swift
//
//  RealVoiceIO+TTS.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
import CoreGraphics
@preconcurrency import AVFoundation
import QuartzCore

@MainActor
extension RealVoiceIO: TTSConfigurable {

    // MARK: - TTSConfigurable

    public func availableVoices() -> [TTSVoiceInfo] {
        AVSpeechSynthesisVoice.speechVoices().map { TTSVoiceInfo(id: $0.identifier, name: $0.name, language: $0.language) }
    }

    public func setVoiceProfile(_ profile: TTSVoiceProfile) { profilesByID[profile.id] = profile }
    public func getVoiceProfile(id: String) -> TTSVoiceProfile? { profilesByID[id] }
    public func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { profilesByID[profile.id] = profile; defaultProfile = profile }
    public func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
    public func setMasterControl(_ master: TTSMasterControl) { self.master = master }
    public func getMasterControl() -> TTSMasterControl { master }
    public func stopSpeakingNow() { synthesizer.stopSpeaking(at: .immediate) }

    // MARK: - VoiceIO speak

    public func speak(_ text: String) async { await speak(text, using: nil) }

    public func speak(_ text: String, using voiceID: String?) async {
        if Task.isCancelled { return }
        await opGate.acquire()
        defer { Task { await opGate.release() } }
        if Task.isCancelled { return }
        await stopRecognitionAsync()

        await withCheckedContinuation { (cont: CheckedContinuation<Void, Never>) in
            if Task.isCancelled { cont.resume(); return }
            let utt = AVSpeechUtterance(string: text)
            applyProfile(to: utt, voiceID: voiceID)
            let key = ObjectIdentifier(utt)
            speakContinuations[key] = cont
            synthesizer.speak(utt)
        }
    }

    func setRecognitionContext(_ ctx: RecognitionContext) { recognitionContext = ctx }
}

// MARK: - TTS helpers

@MainActor
extension RealVoiceIO {

    func applyProfile(to utt: AVSpeechUtterance, voiceID: String?) {
        let profile = (voiceID.flatMap { profilesByID[$0] }) ?? defaultProfile
        if let pid = profile?.id, let v = AVSpeechSynthesisVoice(identifier: pid) { utt.voice = v }
        else { utt.voice = AVSpeechSynthesisVoice(language: Locale.autoupdatingCurrent.identifier) }

        let sysMin = AVSpeechUtteranceMinimumSpeechRate
        let sysMax = AVSpeechUtteranceMaximumSpeechRate
        let span = sysMax - sysMin
        var rate = sysMin + span * (profile?.rate ?? 0.5)
        if master.rateVariation > 0 {
            let jitter = (Float.random(in: -master.rateVariation...master.rateVariation)) * Float(span)
            rate = max(sysMin, min(sysMax, rate + jitter))
        }
        utt.rate = rate

        var pitch = profile?.pitch ?? 1.0
        if master.pitchVariation > 0 {
            pitch += Float.random(in: -master.pitchVariation...master.pitchVariation)
        }
        utt.pitchMultiplier = max(0.5, min(2.0, pitch))

        let baseVol = profile?.volume ?? 1.0
        utt.volume = max(0.0, min(1.0, baseVol * master.volume))
    }

    func ttsStartPulse() {
        onTTSSpeakingChanged?(true)
        ttsPulseTask?.cancel()
        ttsPhase = 0
        ttsGlow = 0
        ttsPulseTask = Task { [weak self] in
            while let self, !Task.isCancelled {
                await MainActor.run {
                    self.ttsPhase += 0.14
                    if self.ttsPhase > .pi * 2 { self.ttsPhase -= .pi * 2 }
                    let base = (sin(self.ttsPhase) + 1) / 2
                    let baseAmp = 0.22 + 0.28 * base
                    self.ttsGlow *= 0.78
                    let level = min(1, max(0, baseAmp + self.ttsGlow))
                    self.onTTSPulse?(level)
                }
                try? await Task.sleep(nanoseconds: 33_333_333)
            }
        }
    }

    func ttsStopPulse() {
        ttsPulseTask?.cancel()
        ttsPulseTask = nil
        ttsGlow = 0
        onTTSPulse?(0.0)
        onTTSSpeakingChanged?(false)
    }

    fileprivate func ttsDidStart(key: ObjectIdentifier) { ttsStartPulse() }
    fileprivate func ttsWillSpeak(range: NSRange, text: String) { ttsGlow = min(1.0, ttsGlow + 0.55) }
    fileprivate func ttsDidFinishOrCancel(key: ObjectIdentifier) {
        speakContinuations.removeValue(forKey: key)?.resume()
        ttsSuppressUntil = CACurrentMediaTime() + config.ttsSuppressAfterFinish
        if autoStartPreparedAfterTTS, let player = boostPlayer, !player.isPlaying {
            autoStartPreparedAfterTTS = false
            player.play()
        }
        ttsStopPulse()
    }
}

// MARK: - AVSpeechSynthesizer delegate

final class SpeechDelegate: NSObject, AVSpeechSynthesizerDelegate {
    private unowned(unsafe) let owner: RealVoiceIO
    init(owner: RealVoiceIO) {
        self.owner = owner
        super.init()
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didStart utterance: AVSpeechUtterance) {
        let key = ObjectIdentifier(utterance)
        Task { @MainActor in self.owner.ttsDidStart(key: key) }
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, willSpeakRangeOfSpeechString characterRange: NSRange, utterance: AVSpeechUtterance) {
        let text = utterance.speechString
        Task { @MainActor in self.owner.ttsWillSpeak(range: characterRange, text: text) }
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        let key = ObjectIdentifier(utterance)
        Task { @MainActor in self.owner.ttsDidFinishOrCancel(key: key) }
    }
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didCancel utterance: AVSpeechUtterance) {
        let key = ObjectIdentifier(utterance)
        Task { @MainActor in self.owner.ttsDidFinishOrCancel(key: key) }
    }
}
```
Sources/VoiceKitCore/RealVoiceIO+Trimming.swift
```swift
//
//  RealVoiceIO+Trimming.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
@preconcurrency import AVFoundation
import Accelerate

@MainActor
extension RealVoiceIO {

    func trimAudioSmart(inputURL: URL,
                        sttStart: Double?,
                        sttEnd: Double?,
                        prePad: Double,
                        postPad: Double) -> URL? {
        do {
            let inFile = try AVAudioFile(forReading: inputURL)
            let sampleRate = inFile.fileFormat.sampleRate
            let totalFrames = inFile.length
            let duration = Double(totalFrames) / sampleRate

            let fallback = (sttStart == nil || sttEnd == nil || sttEnd! <= sttStart!)
            var start = sttStart ?? 0
            var end = sttEnd ?? duration

            if fallback {
                let targetFormat = inFile.processingFormat
                let chunk: AVAudioFrameCount = 8192
                inFile.framePosition = 0
                let threshDB: Float = -45
                var foundStart: Double?
                var lastNonSilent: Double = 0

                while inFile.framePosition < totalFrames {
                    let remaining = AVAudioFrameCount(totalFrames - inFile.framePosition)
                    let frames = min(chunk, remaining)
                    guard let buf = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: frames) else { break }
                    try inFile.read(into: buf, frameCount: frames)
                    if buf.frameLength == 0 { break }
                    let ts = Double(inFile.framePosition - Int64(frames)) / sampleRate

                    if let ch = buf.floatChannelData?.pointee {
                        var ms: Float = 0
                        vDSP_measqv(ch, 1, &ms, vDSP_Length(buf.frameLength))
                        let db: Float = ms <= 0 ? -160 : 10 * log10f(ms)
                        if db > threshDB {
                            if foundStart == nil { foundStart = ts }
                            lastNonSilent = ts + Double(buf.frameLength) / sampleRate
                        }
                    }
                }
                if let fs = foundStart {
                    start = fs
                    end = max(lastNonSilent, fs + 0.1)
                } else {
                    start = 0
                    end = duration
                }
            }

            start = max(0, start - prePad)
            end = min(duration, end + postPad)
            guard end > start else { return inputURL }

            let startFrame = AVAudioFramePosition(start * sampleRate)
            let endFrame = AVAudioFramePosition(end * sampleRate)
            let framesToRead = endFrame - startFrame
            guard framesToRead > 0 else { return inputURL }

            let outURL = inputURL.deletingPathExtension().appendingPathExtension("trim.caf")
            let outFile = try AVAudioFile(forWriting: outURL, settings: inFile.fileFormat.settings)

            inFile.framePosition = startFrame
            let chunkSize: AVAudioFrameCount = 8192
            while inFile.framePosition < endFrame {
                let remaining = AVAudioFrameCount(endFrame - inFile.framePosition)
                let frames = min(chunkSize, remaining)
                guard let buffer = AVAudioPCMBuffer(pcmFormat: inFile.processingFormat, frameCapacity: frames) else { break }
                try inFile.read(into: buffer, frameCount: frames)
                if buffer.frameLength == 0 { break }
                buffer.frameLength = frames
                try outFile.write(from: buffer)
            }
            return outURL
        } catch {
            return inputURL
        }
    }
}```
Sources/VoiceKitCore/RealVoiceIO.swift
```swift
//
//  RealVoiceIO.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
import CoreGraphics
@preconcurrency import AVFoundation
@preconcurrency import Speech

@MainActor
public final class RealVoiceIO: NSObject, VoiceIO {

    // MARK: - Config

    public struct Config: Sendable {
        public var trimPrePad: Double = 0.02
        public var trimPostPad: Double = 0.50
        public var ttsSuppressAfterFinish: Double = 0.25
        public var boostedWaitTimeoutSeconds: Double = 3.0
        public init(trimPrePad: Double = 0.02, trimPostPad: Double = 0.50, ttsSuppressAfterFinish: Double = 0.25, boostedWaitTimeoutSeconds: Double = 3.0) {
            self.trimPrePad = trimPrePad
            self.trimPostPad = trimPostPad
            self.ttsSuppressAfterFinish = ttsSuppressAfterFinish
            self.boostedWaitTimeoutSeconds = boostedWaitTimeoutSeconds
        }
    }
    let config: Config

    // MARK: - Callbacks

    public var onListeningChanged: ((Bool) -> Void)?
    public var onTranscriptChanged: ((String) -> Void)?
    public var onLevelChanged: ((CGFloat) -> Void)?
    public var onTTSSpeakingChanged: ((Bool) -> Void)?
    public var onTTSPulse: ((CGFloat) -> Void)?
    public var onStatusMessageChanged: ((String?) -> Void)?

    // MARK: - TTS

    let synthesizer = AVSpeechSynthesizer()
    var speakContinuations: [ObjectIdentifier: CheckedContinuation<Void, Never>] = [:]
    lazy var speechDelegate: SpeechDelegate = SpeechDelegate(owner: self)
    var ttsPulseTask: Task<Void, Never>?
    var ttsPhase: CGFloat = 0
    var ttsGlow: CGFloat = 0
    var ttsSuppressUntil: CFTimeInterval = 0

    var profilesByID: [String: TTSVoiceProfile] = [:]
    var defaultProfile: TTSVoiceProfile?
    var master: TTSMasterControl = .init()

    // MARK: - STT

    let speechRecognizer = SFSpeechRecognizer(locale: .autoupdatingCurrent)
    var audioEngine: AVAudioEngine?
    var tapInstalled = false
    var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    var recognitionTask: SFSpeechRecognitionTask?
    var hasFinishedRecognition = false

    var recognitionContext: RecognitionContext = .init()
    var firstSpeechStart: Double?
    var lastSpeechEnd: Double?
    var latestTranscript: String = ""

    var listenCont: CheckedContinuation<VoiceResult, Error>?
    var listenOutURL: URL?
    var listenOutFile: AVAudioFile?
    var finishQueued: VoiceResult?

    var listenOverallTask: Task<Void, Never>?
    var listenInactivityTask: Task<Void, Never>?

    var wasInterrupted = false
    var wasPlayingBoost = false

    // MARK: - Boosted SFX

    var boostEngine: AVAudioEngine?
    var boostPlayer: AVAudioPlayerNode?
    var boostEQ: AVAudioUnitEQ?
    var autoStartPreparedAfterTTS = false
    let boostQueue = BoostQueue()
    var boostWaiters: [UUID: CheckedContinuation<Void, Never>] = [:]

    // MARK: - Op gate

    let opGate = VoiceOpGate()

    // MARK: - Lifecycle

    public init(config: Config = Config()) {
        self.config = config
        super.init()
        synthesizer.delegate = speechDelegate
        installInterruptionObservers()
    }

    deinit {
        #if os(iOS)
        NotificationCenter.default.removeObserver(self)
        #endif
    }

    // MARK: - Permissions / Session

    public func ensurePermissions() async throws {
        let micOK = await PermissionBridge.awaitMicPermission()
        guard micOK else { throw SimpleError("Microphone permission denied.") }

        let speechAuth = await PermissionBridge.awaitSpeechAuth()
        guard speechAuth == .authorized else { throw SimpleError("Speech permission denied.") }
        guard speechRecognizer?.isAvailable == true else { throw SimpleError("Speech recognizer unavailable.") }
    }

    public func configureSessionIfNeeded() async throws {
        #if os(iOS)
        let s = AVAudioSession.sharedInstance()
        try? s.setActive(false, options: [])
        var opts: AVAudioSession.CategoryOptions = [.defaultToSpeaker, .duckOthers]
        opts.insert(.allowBluetoothHFP)
        try s.setCategory(.playAndRecord, mode: .voiceChat, options: opts)
        try s.setPreferredSampleRate(44100)
        try s.setPreferredIOBufferDuration(0.01)
        try s.setActive(true, options: [])
        #endif
    }

    // MARK: - Public reset/stop

    public func stopAll() {
        synthesizer.stopSpeaking(at: .immediate)
        for (_, cont) in speakContinuations { cont.resume() }
        speakContinuations.removeAll()
        stopBoostedPlayback()
        stopRecognition()
        onListeningChanged?(false)
    }

    public func hardReset() {
        stopAll()
        hasFinishedRecognition = false
        firstSpeechStart = nil
        lastSpeechEnd = nil
        latestTranscript = ""
        autoStartPreparedAfterTTS = false
        onTranscriptChanged?("")
        onLevelChanged?(0)
        listenOverallTask?.cancel(); listenOverallTask = nil
        listenInactivityTask?.cancel(); listenInactivityTask = nil
        tapInstalled = false
        Task { await opGate.forceClear() }
    }
}
```
Sources/VoiceKitCore/ScriptedVoiceIO.swift
```swift
//
//  ScriptedVoiceIO.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-15-2025
//

import Foundation
import CoreGraphics

@MainActor
public final class ScriptedVoiceIO: VoiceIO {
    public var onListeningChanged: ((Bool) -> Void)?
    public var onTranscriptChanged: ((String) -> Void)?
    public var onLevelChanged: ((CGFloat) -> Void)?
    public var onTTSSpeakingChanged: ((Bool) -> Void)?
    public var onTTSPulse: ((CGFloat) -> Void)?
    public var onStatusMessageChanged: ((String?) -> Void)?

    private var queue: [String]
    private var epoch: Int = 0

    // Convenience non-Base64 init for tests/demos
    public init(script: [String]) {
        self.queue = script
    }

    public init?(fromBase64 jsonBase64: String) {
        guard let data = Data(base64Encoded: jsonBase64),
              let arr = try? JSONSerialization.jsonObject(with: data) as? [String] else {
            return nil
        }
        self.queue = arr
    }

    public func ensurePermissions() async throws { }
    public func configureSessionIfNeeded() async throws { }

    public func speak(_ text: String) async {
        onTTSSpeakingChanged?(true)
        onTTSPulse?(0.35)
        let steps = 6
        for i in 0..<steps {
            let phase = Double(i) / Double(steps - 1)
            let level = 0.28 + 0.24 * sin(phase * .pi)
            onTTSPulse?(CGFloat(level))
            try? await Task.sleep(nanoseconds: 50_000_000)
        }
        onTTSPulse?(0.0)
        onTTSSpeakingChanged?(false)
    }

    public func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
        let myEpoch = epoch
        onListeningChanged?(true)
        onLevelChanged?(0.2)
        try? await Task.sleep(nanoseconds: 120_000_000)
        if myEpoch != epoch {
            onLevelChanged?(0.0)
            onListeningChanged?(false)
            return VoiceResult(transcript: "", recordingURL: nil)
        }
        let transcript = queue.isEmpty ? "" : queue.removeFirst()
        onTranscriptChanged?(transcript)
        try? await Task.sleep(nanoseconds: 40_000_000)
        onLevelChanged?(0.0)
        onListeningChanged?(false)
        return VoiceResult(transcript: transcript, recordingURL: nil)
    }

    public func prepareBoosted(url: URL, gainDB: Float) async throws { }
    public func startPreparedBoosted() async throws { }
    public func playBoosted(url: URL, gainDB: Float) async throws { }

    public func stopAll() {
        epoch &+= 1
        onListeningChanged?(false)
    }

    public func hardReset() {
        epoch &+= 1
        onListeningChanged?(false)
        onTranscriptChanged?("")
        onLevelChanged?(0)
        onTTSPulse?(0)
        onTTSSpeakingChanged?(false)
        onStatusMessageChanged?(nil)
    }
}
```
Sources/VoiceKitCore/VoiceChorus.swift
```swift
//
//  VoiceChorus.swift
//  VoiceKitCore
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-16-2025
//
//  Sing with multiple voices in parallel using predefined profiles or system voice IDs.
//  - Accepts system voice ID or TTSVoiceProfile.
//  - Supports concurrent instances of the same voice with different settings.
//  - sstop() to cancel.
//
import Foundation

@MainActor
public final class VoiceChorus {

    public typealias Engine = any VoiceIO & TTSConfigurable

    private let makeEngine: () -> Engine
    private var engines: [Engine] = []
    private var tasks: [Task<Void, Never>] = []
    private var isRunning = false

    public init(makeEngine: @escaping () -> Engine) {
        self.makeEngine = makeEngine
    }

    /// Ensure we have at least n engines ready.
    private func ensureEngines(count n: Int) {
        while engines.count < n {
            engines.append(makeEngine())
        }
    }

    /// Sing the same text with multiple voices in parallel.
    /// Enable custom settings for each engine.
    public func sing(_ text: String, withVoiceProfiles profiles: [TTSVoiceProfile]) async {
        guard !profiles.isEmpty else { return }
        stop() // reset any previous run
        isRunning = true

        ensureEngines(count: profiles.count)
        tasks = []

        for (index, profile) in profiles.enumerated() {
            let engine = engines[index]
            engine.setVoiceProfile(profile)
            let task = Task { @MainActor in
                if Task.isCancelled { return }
                await engine.speak(text, using: profile.id)
            }
            tasks.append(task)
        }

        for task in tasks { _ = await task.result }
        isRunning = false
        tasks.removeAll()
    }

    /// Stop all voices and cancel any in-flight chorus.
    public func stop() {
        guard isRunning || !tasks.isEmpty else { return }
        tasks.forEach { $0.cancel() }
        engines.forEach { $0.stopAll() }
        tasks.removeAll()
        isRunning = false
    }
}

public struct SingableVoice {
    var voiceID: String?
    var profile: TTSVoiceProfile?

    public init(voiceID: String) { self.voiceID = voiceID }
    public init(profile: TTSVoiceProfile) { self.profile = profile }
}
```
Sources/VoiceKitCore/VoiceKitInfo.swift
```swift
//
//  VoiceKitInfo.swift
//  VoiceKitCore
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-16-2025
//

import Foundation

public enum VoiceKitInfo {
    // Bump this when you tag a new release
    public static let version = "0.1.1"

    public static var buildTimestampISO8601: String {
        ISO8601DateFormatter().string(from: Date())
    }
}
```
Sources/VoiceKitCore/VoicePublic.swift
```swift
//
//  VoicePublic.swift
//  VoiceKit
//
//  Generated by GPT-5 (OpenAI)
//  collaborator: robert
//  date: 09-16-2025
//

import Foundation
import CoreGraphics

// MARK: - Public results and errors

/// Result of a listen() operation.
public struct VoiceResult: Sendable {
    public let transcript: String
    public let recordingURL: URL?
    public init(transcript: String, recordingURL: URL?) {
        self.transcript = transcript
        self.recordingURL = recordingURL
    }
}

/// Simple LocalizedError wrapper for user-friendly messages.
public struct SimpleError: LocalizedError, Sendable {
    public var message: String
    public init(_ message: String) { self.message = message }
    public var errorDescription: String? { message }
}

// MARK: - VoiceIO API (main-actor)

/// Primary voice I/O interface. All methods must be called on the main actor.
@MainActor
public protocol VoiceIO: AnyObject {
    // UI callbacks (main-actor)
    var onListeningChanged: ((Bool) -> Void)? { get set }
    var onTranscriptChanged: ((String) -> Void)? { get set }
    var onLevelChanged: ((CGFloat) -> Void)? { get set }
    var onTTSSpeakingChanged: ((Bool) -> Void)? { get set }
    var onTTSPulse: ((CGFloat) -> Void)? { get set }
    var onStatusMessageChanged: ((String?) -> Void)? { get set }

    // Setup
    func ensurePermissions() async throws
    func configureSessionIfNeeded() async throws

    // TTS and STT
    func speak(_ text: String) async
    func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult

    // Short-clip SFX with optional gain
    func prepareBoosted(url: URL, gainDB: Float) async throws
    func startPreparedBoosted() async throws
    func playBoosted(url: URL, gainDB: Float) async throws

    // Stop/cancel
    func stopAll()
    func hardReset()
}

@MainActor
public extension VoiceIO {
    func prepareBoosted(url: URL) async throws { try await prepareBoosted(url: url, gainDB: 0) }
    func playBoosted(url: URL) async throws { try await playBoosted(url: url, gainDB: 0) }
    func queueSFX(url: URL, gainDB: Float = 0) async throws { try await prepareBoosted(url: url, gainDB: gainDB) }

    /// Convenience listen with context when using RealVoiceIO.
    func listen(timeout: TimeInterval,
                inactivity: TimeInterval,
                record: Bool,
                context: RecognitionContext = .init()) async throws -> VoiceResult {
        if let real = self as? RealVoiceIO {
            real.setRecognitionContext(context)
        }
        return try await listen(timeout: timeout, inactivity: inactivity, record: record)
    }
}

// MARK: - TTS models and configuration

/// Basic system voice info.
public struct TTSVoiceInfo: Identifiable, Hashable, Codable, Sendable {
    public let id: String
    public let name: String
    public let language: String
    public init(id: String, name: String, language: String) {
        self.id = id; self.name = name; self.language = language
    }
}

/// A user-tweakable voice profile.
public struct TTSVoiceProfile: Identifiable, Hashable, Codable, Sendable {
    public let id: String
    public var displayName: String
    public var rate: Float
    public var pitch: Float
    public var volume: Float
    public var isSelected: Bool
    public var isHidden: Bool
    public init(id: String, displayName: String, rate: Float, pitch: Float, volume: Float, isSelected: Bool = false, isHidden: Bool = false) {
        self.id = id; self.displayName = displayName; self.rate = rate; self.pitch = pitch; self.volume = volume
        self.isSelected = isSelected; self.isHidden = isHidden
    }
}

/// Global master controls applied to speech.
public struct TTSMasterControl: Codable, Sendable {
    public var volume: Float
    public var pitchVariation: Float
    public var rateVariation: Float
    public init(volume: Float = 1.0, pitchVariation: Float = 0.0001, rateVariation: Float = 0.0001) {
        self.volume = volume; self.pitchVariation = pitchVariation; self.rateVariation = rateVariation
    }
}

/// TTS-specific configuration and targeted speak with selected voice.
@MainActor
public protocol TTSConfigurable: AnyObject {
    func availableVoices() -> [TTSVoiceInfo]
    func setVoiceProfile(_ profile: TTSVoiceProfile)
    func getVoiceProfile(id: String) -> TTSVoiceProfile?
    func setDefaultVoiceProfile(_ profile: TTSVoiceProfile)
    func getDefaultVoiceProfile() -> TTSVoiceProfile?
    func setMasterControl(_ master: TTSMasterControl)
    func getMasterControl() -> TTSMasterControl
    func speak(_ text: String, using voiceID: String?) async
    func stopSpeakingNow()
}

// MARK: - Recognition context

/// Hints for speech recognition to improve accuracy.
public struct RecognitionContext: Sendable {
    public enum Expectation: Sendable {
        case freeform
        case name(allowed: [String])
        case number
    }
    public var expectation: Expectation
    public init(expectation: Expectation = .freeform) { self.expectation = expectation }
}

public extension RecognitionContext {
    static var numericContextualStrings: [String] {
        let digits = (0...20).map { String($0) } + ["30","40","50","60","70","80","90","100"]
        let words = ["zero","one","two","three","four","five","six","seven","eight","nine",
                     "ten","eleven","twelve","thirteen","fourteen","fifteen",
                     "sixteen","seventeen","eighteen","nineteen","twenty",
                     "thirty","forty","fifty","sixty","seventy","eighty","ninety","hundred"]
        return digits + words
    }
}

// MARK: - Operation gate

/// A minimal async gate to serialize higher-level operations.
public actor VoiceOpGate {
    private var busy = false
    public init() {}
    public func acquire() async {
        while busy { try? await Task.sleep(nanoseconds: 200_000) }
        busy = true
    }
    public func release() async { busy = false }
    public func forceClear() async { busy = false }
}
```
Sources/VoiceKitCore/VoiceQueue.swift
```swift
//
//  VoiceQueue.swift
//  VoiceKitCore
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-16-2025
//
//  A tiny orchestration layer for kid‑friendly sequencing of voice + SFX.
//  - Linear queues per channel; channels run in parallel.
//  - Smart SFX near-zero gap: pre-schedules the next clip before speak().
//  - Embedded SFX parsing: "Hello [sfx:ding] world" with resolver(name)->URL.
//  - Cancel-all stops everything immediately.
//
//  Notes:
//  - Uses any VoiceIO; when available, it calls TTSConfigurable.speak(_:using:)
//    to select specific voices. Otherwise it falls back to VoiceIO.speak(_).
//  - For near-zero SFX after speech, we pre-call prepareBoosted() when the next
//    item is .sfx; after speak returns, we await startPreparedBoosted().
//  - Parallel playback requires separate channels; by default, channel 0 uses
//    the provided VoiceIO; additional channels can be provided via factory.
//

import Foundation
import CoreGraphics

@MainActor
public final class VoiceQueue {

    public enum Item: Sendable, Equatable {
        case speak(text: String, voiceID: String? = nil)
        case sfx(url: URL, gainDB: Float = 0)
        case pause(seconds: TimeInterval)

        public static func speak(_ text: String, voiceID: String? = nil) -> Item { .speak(text: text, voiceID: voiceID) }
        public static func sfx(_ url: URL, gainDB: Float = 0) -> Item { .sfx(url: url, gainDB: gainDB) }
        public static func pause(_ seconds: TimeInterval) -> Item { .pause(seconds: seconds) }
    }

    public typealias ChannelID = Int
    public typealias SFXResolver = (_ name: String) -> URL?

    private struct Channel {
        var io: VoiceIO
        var items: [Item] = []
        var task: Task<Void, Never>?
    }

    private var channels: [ChannelID: Channel] = [:]
    private let makeChannel: (() -> VoiceIO)?
    private var running = false
    private var cancelled = false

    public init(primary: VoiceIO, makeChannel: (() -> VoiceIO)? = nil) {
        self.channels[0] = Channel(io: primary, items: [], task: nil)
        self.makeChannel = makeChannel
    }

    // MARK: - Enqueue

    public func enqueue(_ item: Item, on channel: ChannelID = 0) {
        ensureChannel(channel)
        channels[channel]?.items.append(item)
    }

    @discardableResult
    public func enqueueSpeak(_ text: String, voiceID: String? = nil, on channel: ChannelID = 0) -> Self {
        enqueue(.speak(text: text, voiceID: voiceID), on: channel); return self
    }

    @discardableResult
    public func enqueueSFX(_ url: URL, gainDB: Float = 0, on channel: ChannelID = 0) -> Self {
        enqueue(.sfx(url: url, gainDB: gainDB), on: channel); return self
    }

    @discardableResult
    public func enqueuePause(_ seconds: TimeInterval, on channel: ChannelID = 0) -> Self {
        enqueue(.pause(seconds: seconds), on: channel); return self
    }

    // Parse "[sfx:name]" tokens and enqueue segments accordingly.
    // Example: "Hello [sfx:ding] world"
    @discardableResult
    public func enqueueParsingSFX(text: String, resolver: SFXResolver, defaultVoiceID: String? = nil, on channel: ChannelID = 0) -> Self {
        let parts = Self.parseTextForSFX(text)
        for part in parts {
            switch part {
            case .text(let s) where !s.isEmpty:
                enqueue(.speak(text: s, voiceID: defaultVoiceID), on: channel)
            case .sfx(let name):
                if let url = resolver(name) {
                    enqueue(.sfx(url: url, gainDB: 0), on: channel)
                }
            case .text: // empty text part; ignore
                break
            }
        }
        return self
    }

    public var isRunning: Bool { running }

    // MARK: - Control

    public func play() async {
        guard !running else { return }
        running = true
        cancelled = false

        // Launch a task per channel without mutating the dictionary while iterating it.
        let ids = Array(channels.keys)
        for id in ids {
            guard var ch = channels[id] else { continue }
            ch.task = Task { [weak self] in
                await self?.runChannel(id)
            }
            channels[id] = ch
        }

        // Await all channel tasks to finish
        for id in ids {
            _ = await channels[id]?.task?.value
        }

        running = false
    }

    public func cancelAll() {
        cancelled = true
        for (_, ch) in channels {
            ch.io.stopAll()
            ch.task?.cancel()
        }
    }

    // MARK: - Internals

    private func ensureChannel(_ id: ChannelID) {
        guard channels[id] == nil else { return }
        if let factory = makeChannel {
            channels[id] = Channel(io: factory(), items: [], task: nil)
        } else {
            // Without a factory we share channel 0 (serial behavior).
            if let ch0 = channels[0] {
                channels[id] = Channel(io: ch0.io, items: [], task: nil)
            }
        }
    }

    private func runChannel(_ id: ChannelID) async {
        guard var ch = channels[id] else { return }
        var i = 0

        while i < ch.items.count, !cancelled, !Task.isCancelled {
            let item = ch.items[i]
            let next = i + 1 < ch.items.count ? ch.items[i + 1] : nil

            switch item {
            case .speak(let text, let voiceID):
                // Pre-schedule SFX if next is a clip for near-zero gap.
                if case .sfx(let url, let gain)? = next {
                    try? await ch.io.prepareBoosted(url: url, gainDB: gain)
                }

                if let tts = ch.io as? TTSConfigurable {
                    await tts.speak(text, using: voiceID)
                } else {
                    await ch.io.speak(text)
                }

                if case .sfx = next {
                    // Await clip completion; RealVoiceIO auto-starts; others may need start call.
                    try? await ch.io.startPreparedBoosted()
                    i += 1 // consume the next .sfx
                }

            case .sfx(let url, let gain):
                try? await ch.io.playBoosted(url: url, gainDB: gain)

            case .pause(let seconds):
                try? await Task.sleep(nanoseconds: UInt64(max(0, seconds) * 1_000_000_000))
            }

            i += 1
        }

        // Clear consumed items on completion/cancel
        ch.items.removeAll()
        channels[id] = ch
    }

    // MARK: - Parsing

    enum Part {
        case text(String)
        case sfx(String)
    }

    static func parseTextForSFX(_ s: String) -> [Part] {
        // Pattern: [sfx:NAME] where NAME is [a-zA-Z0-9._-]+
        let pattern = #"\[sfx:([A-Za-z0-9._-]+)\]"#
        guard let regex = try? NSRegularExpression(pattern: pattern) else { return [.text(s)] }

        var parts: [Part] = []
        var idx = s.startIndex

        for match in regex.matches(in: s, range: NSRange(s.startIndex..<s.endIndex, in: s)) {
            guard let r = Range(match.range, in: s) else { continue }
            let before = String(s[idx..<r.lowerBound])
            if !before.isEmpty { parts.append(.text(before)) }

            if let nameRange = Range(match.range(at: 1), in: s) {
                parts.append(.sfx(String(s[nameRange])))
            }

            idx = r.upperBound
        }

        let tail = String(s[idx..<s.endIndex])
        if !tail.isEmpty { parts.append(.text(tail)) }
        if parts.isEmpty { parts = [.text(s)] }
        return parts
    }
}
```
Sources/VoiceKitUI/VoiceChorusPlayground.swift
```swift
//
//  VoiceChorusPlayground.swift
//  VoiceKitUI
//
//  Generated by GPT-5 (OpenAI) — collaborator: robert
//  date: 09-16-2025
//
import SwiftUI
import VoiceKitCore

struct VoiceChorusPlayground: View {
    @State private var selectedProfiles: [TTSVoiceProfile] = []
    @State private var pitch: Float = 1.0
    @State private var rate: Float = 1.0
    @State private var customText: String = "God in his wisdom made the fly\nAnd then forgot to tell us why."

    let chorus = VoiceChorus(makeEngine: { RealVoiceIO() })

    var body: some View {
        VStack {
            // Fixed control area
            VStack {
                Text("Voice Chorus Playground")
                    .font(.largeTitle)

                Button(action: startChorus) {
                    Text("Play Chorus")
                        .frame(maxWidth: .infinity)
                        .padding()
                        .background(Color.blue)
                        .foregroundColor(.white)
                        .cornerRadius(10)
                }
                .disabled(selectedProfiles.isEmpty)

                SliderView(title: "Pitch", value: $pitch, range: 0.5...2.0)
                SliderView(title: "Rate", value: $rate, range: 0.5...2.0)

                Section(header: Text("Custom Text")) {
                    TextEditor(text: $customText)
                        .frame(height: 100)
                        .border(Color.gray, width: 0.5)
                }
            }.padding()

            // Scrolling list area
            ScrollView {
                VoiceSelectionSection()
            }
        }
    }

    @ViewBuilder
    private func VoiceSelectionSection() -> some View {
        Section(header: Text("Voices")) {
            ForEach(availableVoices(), id: \.id) { voice in
                Toggle(voice.name, isOn: Binding(
                    get: { selectedProfiles.contains(where: { $0.id == voice.id }) },
                    set: { isSelected in
                        if isSelected {
                            selectedProfiles.append(
                                TTSVoiceProfile(id: voice.id,
                                                displayName: voice.name,
                                                rate: rate,
                                                pitch: pitch,
                                                volume: 1.0)
                            )
                        } else {
                            selectedProfiles.removeAll { $0.id == voice.id }
                        }
                    }
                ))
            }
        }
    }
    
    private func startChorus() {
        Task {
            await chorus.sing(customText, withVoiceProfiles: selectedProfiles)
        }
    }

    func availableVoices() -> [TTSVoiceInfo] {
        let io = RealVoiceIO()
        return io.availableVoices()
    }
}

struct SliderView: View {
    var title: String
    @Binding var value: Float
    var range: ClosedRange<Float>

    var body: some View {
        VStack {
            Text("\(title): \(value, specifier: "%.2f")")
            Slider(value: $value, in: range)
                .padding()
        }
    }
}

struct VoiceChorusPlayground_Previews: PreviewProvider {
    static var previews: some View {
        VoiceChorusPlayground()
    }
}

struct SliderView_Previews: PreviewProvider {
    static var previews: some View {
        SliderView(title: "Test Slider", value: .constant(1.0), range: 0.5...2.0)
        .previewLayout(.sizeThatFits)
    }
}
```
Sources/VoiceKitUI/VoicePickerView.swift
```swift
//
//  VoicePickerView.swift
//  VoiceKitUI
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-12-2025
//
//  Voice picker with:
//  - Filter by Current Language Name | All (button shows current language).
//  - Tap a row to hear a fixed sample.
//  - Live interrupting previews on slider changes with suffix.
//  - Multiple “Active” voices and optional Favorite star.
//  - Swipe to Hide/Unhide, plus “Show hidden voices” toggle.
//  - Master sliders: immediate updates, plus a debounced sample (favorite or random active).
//  - Labels use “range” instead of “variation” for simpler wording.
//
//  Package notes:
//  - This file lives in the VoiceKitUI target and depends on VoiceKitCore.
//  - All exported types are public so an app can use the picker directly.
//

import SwiftUI
import Combine
import Foundation
import VoiceKitCore

// MARK: - Persistence model

public struct VoiceProfilesFile: Codable {
    public var defaultVoiceID: String?
    public var master: TTSMasterControl
    public var profilesByID: [String: TTSVoiceProfile]
    public var activeVoiceIDs: [String]

    public init(defaultVoiceID: String? = nil,
                master: TTSMasterControl = .init(),
                profilesByID: [String: TTSVoiceProfile] = [:],
                activeVoiceIDs: [String] = []) {
        self.defaultVoiceID = defaultVoiceID
        self.master = master
        self.profilesByID = profilesByID
        self.activeVoiceIDs = activeVoiceIDs
    }
}

@MainActor
public final class VoiceProfilesStore: ObservableObject {
    @Published public var defaultVoiceID: String?
    @Published public var master: TTSMasterControl = .init()
    @Published public var profilesByID: [String: TTSVoiceProfile] = [:]
    @Published public var activeVoiceIDs: Set<String> = []

    private let fileURL: URL

    public init(filename: String = "voices.json") {
        let appSupport = FileManager.default.urls(for: .applicationSupportDirectory, in: .userDomainMask).first
            ?? FileManager.default.temporaryDirectory
        let dir = appSupport.appendingPathComponent("VoiceIO", isDirectory: true)
        try? FileManager.default.createDirectory(at: dir, withIntermediateDirectories: true)
        self.fileURL = dir.appendingPathComponent(filename)
        load()
    }

    public func load() {
        guard let data = try? Data(contentsOf: fileURL) else { return }
        if let decoded = try? JSONDecoder().decode(VoiceProfilesFile.self, from: data) {
            self.defaultVoiceID = decoded.defaultVoiceID
            self.master = decoded.master
            self.profilesByID = decoded.profilesByID
            self.activeVoiceIDs = Set(decoded.activeVoiceIDs)
        }
    }

    public func save() {
        let payload = VoiceProfilesFile(
            defaultVoiceID: defaultVoiceID,
            master: master,
            profilesByID: profilesByID,
            activeVoiceIDs: Array(activeVoiceIDs)
        )
        if let data = try? JSONEncoder().encode(payload) {
            try? data.write(to: fileURL, options: [.atomic])
        }
    }

    public func profile(for info: TTSVoiceInfo) -> TTSVoiceProfile {
        if let p = profilesByID[info.id] { return p }
        let p = TTSVoiceProfile(id: info.id, displayName: info.name, rate: 0.55, pitch: 1.0, volume: 0.9)
        profilesByID[info.id] = p
        return p
    }

    public func setProfile(_ p: TTSVoiceProfile) { profilesByID[p.id] = p }

    public func isActive(_ id: String) -> Bool { activeVoiceIDs.contains(id) }
    public func toggleActive(_ id: String) {
        if activeVoiceIDs.contains(id) { activeVoiceIDs.remove(id) }
        else { activeVoiceIDs.insert(id) }
        save()
    }

    public func isHidden(_ id: String) -> Bool { profilesByID[id]?.isHidden ?? false }
    public func setHidden(_ id: String, _ hidden: Bool) {
        if var p = profilesByID[id] {
            p.isHidden = hidden
            profilesByID[id] = p
            save()
        }
    }
}

// MARK: - ViewModel

@MainActor
public final class VoicePickerViewModel: ObservableObject {
    @Published public var voices: [TTSVoiceInfo] = []
    @Published public var showHidden: Bool = false

    public enum LanguageFilter: Hashable { case current, all }
    @Published public var languageFilter: LanguageFilter = .current

    private var previewTask: Task<Void, Never>?
    private var masterPreviewDebounce: Task<Void, Never>?

    public let tts: TTSConfigurable
    public let store: VoiceProfilesStore

    public init(tts: TTSConfigurable, store: VoiceProfilesStore) {
        self.tts = tts
        self.store = store
        refreshAvailableVoices()
        bootstrapDefaultsIfNeeded()
        applyToTTS()
    }

    public func refreshAvailableVoices() {
        voices = tts.availableVoices().sorted { a, b in
            if a.language == b.language { return a.name < b.name }
            return a.language < b.language
        }
    }

    private func languageCodePrefix() -> String {
        let tag = Locale.preferredLanguages.first ?? Locale.current.identifier
        let code = tag.split(separator: "-").first.map(String.init) ?? "en"
        return code.lowercased()
    }

    public var currentLanguageDisplayName: String {
        let code = languageCodePrefix()
        return Locale.current.localizedString(forLanguageCode: code)?.capitalized ?? code.uppercased()
    }

    public var filteredVoices: [TTSVoiceInfo] {
        let base: [TTSVoiceInfo]
        switch languageFilter {
        case .all: base = voices
        case .current:
            let pref = languageCodePrefix()
            base = voices.filter { $0.language.lowercased().hasPrefix(pref) }
        }
        return base.filter { info in showHidden || !store.isHidden(info.id) }
    }

    private func bootstrapDefaultsIfNeeded() {
        if store.defaultVoiceID == nil, let first = filteredVoices.first ?? voices.first {
            store.defaultVoiceID = first.id
        }
        for v in voices {
            _ = store.profile(for: v)
        }
        store.save()
    }

    public func applyToTTS() {
        for (_, p) in store.profilesByID { tts.setVoiceProfile(p) }
        if let id = store.defaultVoiceID, let p = store.profilesByID[id] {
            tts.setDefaultVoiceProfile(p)
        }
        tts.setMasterControl(store.master)
    }

    public func setDefaultVoice(id: String) {
        store.defaultVoiceID = id
        store.save()
        applyToTTS()
    }

    public func updateProfile(_ p: TTSVoiceProfile) {
        store.setProfile(p)
        store.save()
        if store.defaultVoiceID == p.id { tts.setDefaultVoiceProfile(p) }
        else { tts.setVoiceProfile(p) }
        tts.setMasterControl(store.master)
    }

    // Master: immediate apply + debounced preview on favorite/random active/random any
    public func updateMaster(_ m: TTSMasterControl, previewKind: String? = nil) {
        store.master = m
        store.save()
        tts.setMasterControl(m) // immediate so UI feels direct

        // Debounced preview announce (e.g., "at volume 1.10")
        if let kind = previewKind {
            masterPreviewDebounce?.cancel()
            masterPreviewDebounce = Task { [weak self] in
                guard let self else { return }
                try? await Task.sleep(nanoseconds: 160_000_000) // 160ms debounce
                let (id, profile) = self.pickPreviewVoice()
                let phrase = self.samplePhrase(for: profile, suffix: kind)
                self.playPreview(phrase: phrase, voiceID: id)
            }
        }
    }

    private func pickPreviewVoice() -> (String, TTSVoiceProfile) {
        if let fav = store.defaultVoiceID, let p = store.profilesByID[fav] { return (fav, p) }
        let active = filteredVoices.filter { store.isActive($0.id) }
        if let info = (active.isEmpty ? filteredVoices : active).randomElement() {
            return (info.id, store.profile(for: info))
        }
        // Fallback: any
        let any = voices.first!
        return (any.id, store.profile(for: any))
    }

    public func samplePhrase(for profile: TTSVoiceProfile, suffix: String = "") -> String {
        let who = profile.displayName.isEmpty ? "Alex" : profile.displayName
        let base = "My name is \(who). This is what my voice sounds like"
        return suffix.isEmpty ? base + "." : base + " " + suffix + "."
    }

    public func playPreview(phrase: String, voiceID: String) {
        previewTask?.cancel()
        tts.stopSpeakingNow()
        previewTask = Task { [tts] in
            if Task.isCancelled { return }
            await tts.speak(phrase, using: voiceID)
        }
    }

    public func stopPreview() {
        previewTask?.cancel()
        masterPreviewDebounce?.cancel()
        tts.stopSpeakingNow()
    }
}

// MARK: - View

public struct VoicePickerView: View {
    @ObservedObject private var store: VoiceProfilesStore
    @StateObject private var vm: VoicePickerViewModel

    @MainActor
    public init(tts: TTSConfigurable) {
        let s = VoiceProfilesStore()
        _store = ObservedObject(wrappedValue: s)
        _vm = StateObject(wrappedValue: VoicePickerViewModel(tts: tts, store: s))
    }

    @MainActor
    public init(tts: TTSConfigurable, store: VoiceProfilesStore) {
        _store = ObservedObject(wrappedValue: store)
        _vm = StateObject(wrappedValue: VoicePickerViewModel(tts: tts, store: store))
    }

    public var body: some View {
        NavigationView {
            Form {
                Section("Master") {
                    HStack {
                        Label("Volume", systemImage: "speaker.wave.2.fill")
                        Slider(
                            value: Binding(
                                get: { Double(store.master.volume) },
                                set: { newVal in
                                    vm.updateMaster(
                                        TTSMasterControl(volume: Float(newVal),
                                                         pitchVariation: store.master.pitchVariation,
                                                         rateVariation: store.master.rateVariation),
                                        previewKind: "at volume \(String(format: "%.2f", newVal))"
                                    )
                                }
                            ),
                            in: 0...2, step: 0.01
                        )
                        Text(String(format: "%.2f", store.master.volume))
                            .monospacedDigit()
                            .foregroundStyle(.secondary)
                            .frame(width: 52, alignment: .trailing)
                    }
                    HStack {
                        Label("Pitch range", systemImage: "waveform")
                        Slider(
                            value: Binding(
                                get: { Double(store.master.pitchVariation) },
                                set: { newVal in
                                    vm.updateMaster(
                                        TTSMasterControl(volume: store.master.volume,
                                                         pitchVariation: Float(newVal),
                                                         rateVariation: store.master.rateVariation),
                                        previewKind: "with pitch range \(String(format: "%.3f", newVal))"
                                    )
                                }
                            ),
                            in: 0...0.2, step: 0.001
                        )
                        Text(String(format: "%.3f", store.master.pitchVariation))
                            .monospacedDigit()
                            .foregroundStyle(.secondary)
                            .frame(width: 62, alignment: .trailing)
                    }
                    HStack {
                        Label("Speed range", systemImage: "speedometer")
                        Slider(
                            value: Binding(
                                get: { Double(store.master.rateVariation) },
                                set: { newVal in
                                    vm.updateMaster(
                                        TTSMasterControl(volume: store.master.volume,
                                                         pitchVariation: store.master.pitchVariation,
                                                         rateVariation: Float(newVal)),
                                        previewKind: "with speed range \(String(format: "%.3f", newVal))"
                                    )
                                }
                            ),
                            in: 0...0.2, step: 0.001
                        )
                        Text(String(format: "%.3f", store.master.rateVariation))
                            .monospacedDigit()
                            .foregroundStyle(.secondary)
                            .frame(width: 62, alignment: .trailing)
                    }
                }

                Section("Voices") {
                    HStack {
                        Picker("Filter", selection: $vm.languageFilter) {
                            Text(vm.currentLanguageDisplayName).tag(VoicePickerViewModel.LanguageFilter.current)
                            Text("All").tag(VoicePickerViewModel.LanguageFilter.all)
                        }
                        .pickerStyle(.segmented)
                        Toggle("Show hidden", isOn: $vm.showHidden)
                            .toggleStyle(.switch)
                    }

                    if vm.filteredVoices.isEmpty {
                        Text("No system voices available for this filter.")
                            .foregroundStyle(.secondary)
                    } else {
                        ForEach(vm.filteredVoices, id: \.id) { info in
                            VoiceRow(
                                info: info,
                                profile: store.profile(for: info),
                                isActive: store.isActive(info.id),
                                isFavorite: store.defaultVoiceID == info.id,
                                isHidden: store.isHidden(info.id),
                                onToggleActive: { store.toggleActive(info.id) },
                                onFavorite: { vm.setDefaultVoice(id: info.id) },
                                onHide: { store.setHidden(info.id, true) },
                                onUnhide: { store.setHidden(info.id, false) },
                                onChange: { updated, suffix in
                                    vm.updateProfile(updated)
                                    vm.playPreview(phrase: vm.samplePhrase(for: updated, suffix: suffix),
                                                   voiceID: info.id)
                                },
                                onTapRow: {
                                    let p = store.profile(for: info)
                                    vm.playPreview(phrase: vm.samplePhrase(for: p), voiceID: info.id)
                                }
                            )
                        }
                    }
                }
            }
            .navigationTitle("Voices")
        }
        .onDisappear {
            vm.stopPreview()
        }
    }
}

// MARK: - Components

private struct VoiceRow: View {
    let info: TTSVoiceInfo
    @State var profile: TTSVoiceProfile
    let isActive: Bool
    let isFavorite: Bool
    let isHidden: Bool
    let onToggleActive: () -> Void
    let onFavorite: () -> Void
    let onHide: () -> Void
    let onUnhide: () -> Void
    let onChange: (TTSVoiceProfile, String) -> Void
    let onTapRow: () -> Void

    @State private var expanded = false

    var body: some View {
        VStack(alignment: .leading, spacing: 8) {
            HStack(spacing: 12) {
                Button(action: onToggleActive) {
                    Image(systemName: isActive ? "checkmark.circle.fill" : "circle")
                        .foregroundStyle(isActive ? AnyShapeStyle(.tint) : AnyShapeStyle(.secondary))
                        .imageScale(.large)
                        .accessibilityLabel(isActive ? "Active" : "Make Active")
                }
                .buttonStyle(.plain)

                VStack(alignment: .leading) {
                    HStack {
                        Text(profile.displayName.isEmpty ? info.name : profile.displayName)
                            .font(.headline)
                        if isHidden {
                            Text("Hidden")
                                .font(.caption2)
                                .padding(.horizontal, 6)
                                .padding(.vertical, 2)
                                .background(.gray.opacity(0.2))
                                .clipShape(RoundedRectangle(cornerRadius: 4))
                        }
                    }
                    Text("\(info.language)")
                        .font(.caption)
                        .foregroundStyle(.secondary)
                }

                Spacer()

                Button(action: onFavorite) {
                    Image(systemName: isFavorite ? "star.fill" : "star")
                        .foregroundStyle(isFavorite ? AnyShapeStyle(.tint) : AnyShapeStyle(.secondary))
                        .imageScale(.large)
                        .accessibilityLabel(isFavorite ? "Favorite" : "Make Favorite")
                }
                .buttonStyle(.plain)

                Button {
                    withAnimation { expanded.toggle() }
                } label: {
                    Image(systemName: expanded ? "chevron.up" : "chevron.down")
                        .foregroundStyle(.secondary)
                }
                .buttonStyle(.plain)
            }
            .contentShape(Rectangle())
            .onTapGesture { onTapRow() }
            .swipeActions(edge: .trailing, allowsFullSwipe: false) {
                if isHidden {
                    Button("Unhide") { onUnhide() }.tint(.blue)
                } else {
                    Button("Hide") { onHide() }.tint(.red)
                }
            }

            if expanded {
                VStack(spacing: 10) {
                    HStack {
                        Text("Speed").frame(width: 70, alignment: .leading)
                        Slider(
                            value: Binding(
                                get: { Double(profile.rate) },
                                set: { newVal in
                                    profile.rate = Float(newVal)
                                    onChange(profile, "at speed \(String(format: "%.2f", profile.rate))")
                                }
                            ),
                            in: 0...1, step: 0.01
                        )
                        Text(String(format: "%.2f", profile.rate))
                            .monospacedDigit()
                            .foregroundStyle(.secondary)
                            .frame(width: 52, alignment: .trailing)
                    }

                    HStack {
                        Text("Pitch").frame(width: 70, alignment: .leading)
                        Slider(
                            value: Binding(
                                get: { Double(profile.pitch) },
                                set: { newVal in
                                    profile.pitch = Float(newVal)
                                    onChange(profile, "at pitch \(String(format: "%.2f", profile.pitch))")
                                }
                            ),
                            in: 0.5...2.0, step: 0.01
                        )
                        Text(String(format: "%.2f", profile.pitch))
                            .monospacedDigit()
                            .foregroundStyle(.secondary)
                            .frame(width: 52, alignment: .trailing)
                    }

                    HStack {
                        Text("Volume").frame(width: 70, alignment: .leading)
                        Slider(
                            value: Binding(
                                get: { Double(profile.volume) },
                                set: { newVal in
                                    profile.volume = Float(newVal)
                                    onChange(profile, "at volume \(String(format: "%.2f", profile.volume))")
                                }
                            ),
                            in: 0...1, step: 0.01
                        )
                        Text(String(format: "%.2f", profile.volume))
                            .monospacedDigit()
                            .foregroundStyle(.secondary)
                            .frame(width: 52, alignment: .trailing)
                    }
                }
                .padding(.top, 4)
            }
        }
        .padding(.vertical, 6)
    }
}
```
Tests/VoiceKitCoreTests/CoreSanityTests.swift
```swift
//
//  CoreSanityTests.swift
//  VoiceKit
//
//  Created by robert on 9/12/25.
//


//
//  CoreSanityTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-12-2025
//

import XCTest
import VoiceKitCore

@MainActor
final class CoreSanityTests: XCTestCase {

    func testCanCreateAndHardReset() async throws {
        // This should not trigger any permission UI by itself.
        // It validates actor-safety and basic initialization on main.
        let io = RealVoiceIO()
        io.hardReset()
        XCTAssertTrue(true)
    }

    func testPublicTypesAreReachable() {
        // Construct a few public types to confirm visibility and linkage
        let _ = RealVoiceIO.Config()
        let _ = RecognitionContext(expectation: .freeform)
        let _ = TTSMasterControl()
        let _ = TTSVoiceProfile(id: "v.test", displayName: "Test", rate: 0.5, pitch: 1.0, volume: 1.0)
        XCTAssertTrue(true)
    }
}
```
Tests/VoiceKitCoreTests/NameMatchMoreTests.swift
```swift
//
//  NameMatchMoreTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore

final class NameMatchMoreTests: XCTestCase {

    func testNormalizeRemovesInvisiblesAndUnifiesDashes() {
        // Insert zero-width space U+200B between letters; use en-dash/em-dash/minus variants
        let s = "Z\u{200B}e\u{200B}ro\u{200B} Width — En–Em − Minus"
        let norm = NameMatch.normalizeKey(s)
        // Expect invisibles gone, dash variants -> "-" and spacing collapsed
        XCTAssertEqual(norm, "zero width - en-em - minus")
    }

    func testNormalizeApostrophesAndCleanup() {
        XCTAssertEqual(NameMatch.normalizeKey("O’Connor"), "oconnor")
        XCTAssertEqual(NameMatch.normalizeKey("O'Connor"), "oconnor")
        XCTAssertEqual(NameMatch.normalizeKey("  -- hello --  "), "hello")
    }

    func testDistanceEmptyAndIdentity() {
        XCTAssertEqual(NameMatch.stringDistanceScore(a: "", b: ""), 0)
        XCTAssertEqual(NameMatch.stringDistanceScore(a: "a", b: "a"), 0)
        XCTAssertEqual(NameMatch.stringDistanceScore(a: "hello world", b: "hello world"), 0)
        XCTAssertEqual(NameMatch.stringDistanceScore(a: "a", b: ""), 1)
        XCTAssertEqual(NameMatch.stringDistanceScore(a: "", b: "b"), 1)
    }

    func testTokenPenaltyForMissingTokens() {
        let a = NameMatch.normalizeKey("john paul")
        let b = NameMatch.normalizeKey("john")
        let score = NameMatch.stringDistanceScore(a: a, b: b)
        XCTAssertGreaterThan(score, 0.05) // not zero because a token is missing
        XCTAssertLessThan(score, 0.7)     // but not maximal because first token matches well
    }

    func testCloseSpellingAndDifferentTokens() {
        let a = NameMatch.normalizeKey("kathryn")
        let b = NameMatch.normalizeKey("catherine")
        XCTAssertLessThan(NameMatch.stringDistanceScore(a: a, b: b), 0.5)

        let c = NameMatch.normalizeKey("bob dylan")
        let d = NameMatch.normalizeKey("alice cooper")
        XCTAssertGreaterThan(NameMatch.stringDistanceScore(a: c, b: d), 0.6)
    }
}
```
Tests/VoiceKitCoreTests/NameMatchTests.swift
```swift
//
//  NameMatchTests.swift
//  VoiceKit
//
//  Created by robert on 9/12/25.
//


//
//  NameMatchTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-12-2025
//

import XCTest
import VoiceKitCore

final class NameMatchTests: XCTestCase {

    func testNormalizeKeyHandlesDiacriticsLigaturesApostrophes() {
        // Zoë O’Connor → "zoe oconnor"
        let s = "Zoë O’Connor"
        let norm = NameMatch.normalizeKey(s)
        XCTAssertEqual(norm, "zoe oconnor")

        // Æthelred → "aethelred"
        XCTAssertEqual(NameMatch.normalizeKey("Æthelred"), "aethelred")

        // Mixed punctuation collapses and lowercases
        XCTAssertEqual(NameMatch.normalizeKey("  Jean‑Luc  Picard! "), "jean-luc picard")
    }

    func testStringDistanceScoreTokenAware() {
        // Close spellings should be < 0.35
        let a = NameMatch.normalizeKey("Katherine")
        let b = NameMatch.normalizeKey("Catherine")
        let score = NameMatch.stringDistanceScore(a: a, b: b)
        XCTAssertLessThan(score, 0.35)

        // Different names should be larger
        let c = NameMatch.normalizeKey("Bob")
        let d = NameMatch.normalizeKey("Alice")
        let score2 = NameMatch.stringDistanceScore(a: c, b: d)
        XCTAssertGreaterThan(score2, 0.35)
    }
}```
Tests/VoiceKitCoreTests/NameMatchTokenDistanceTests.swift
```swift
//
//  NameMatchTokenDistanceTests.swift
//  VoiceKit
//
//  Created by robert on 9/14/25.
//


//
//  NameMatchTokenDistanceTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore

final class NameMatchTokenDistanceTests: XCTestCase {

    func testIdenticalIsZero() {
        let key = NameMatch.normalizeKey("Jean-Luc Picard")
        XCTAssertEqual(NameMatch.stringDistanceScore(a: key, b: key), 0)
    }

    func testMissingTokenPenalizedButNotMax() {
        let a = NameMatch.normalizeKey("john paul jones")
        let b = NameMatch.normalizeKey("john jones")
        let score = NameMatch.stringDistanceScore(a: a, b: b)
        XCTAssertGreaterThan(score, 0.05)
        XCTAssertLessThan(score, 0.8)
    }

    func testCrossTokenMatchingPicksBestPairs() {
        let a = NameMatch.normalizeKey("katherine anne")
        let b = NameMatch.normalizeKey("catherine ann")
        let score = NameMatch.stringDistanceScore(a: a, b: b)
        XCTAssertLessThan(score, 0.35)
    }
}```
Tests/VoiceKitCoreTests/NameMatchUnicodeEdgeTests.swift
```swift
//
//  NameMatchUnicodeEdgeTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore

final class NameMatchUnicodeEdgeTests: XCTestCase {

    func testZeroWidthAndSoftHyphenAreRemoved() {
        let s = "Jo\u{200D}se\u{00AD}ph"
        XCTAssertEqual(NameMatch.normalizeKey(s), "joseph")
    }

    func testDashVariantsUnifyToAsciiHyphenAndSpacingCollapses() {
        // NameMatch.normalizeKey unifies dash variants to "-" and keeps hyphens.
        let s1 = "Jean–Luc — Picard − Captain"
        XCTAssertEqual(NameMatch.normalizeKey(s1), "jean-luc - picard - captain")

        let s2 = "Jean–Luc — Picard"
        XCTAssertEqual(NameMatch.normalizeKey(s2), "jean-luc - picard")
    }

    func testApostrophesRemovedAndLowercased() {
        XCTAssertEqual(NameMatch.normalizeKey("O’Connor"), "oconnor")
        XCTAssertEqual(NameMatch.normalizeKey("O'CONNOR"), "oconnor")
    }
}
```
Tests/VoiceKitCoreTests/NameResolverMoreTests.swift
```swift
//
//  NameResolverMoreTests.swift
//  VoiceKit
//
//  Created by robert on 9/14/25.
//


//
//  NameResolverMoreTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore

final class NameResolverMoreTests: XCTestCase {

    func testEmptyAndPunctuationOnlyReturnsEmptyWhenNoAllowed() {
        let r = NameResolver()
        XCTAssertEqual(r.resolve(transcript: "   ", allowed: nil), "")
        XCTAssertEqual(r.resolve(transcript: " !!! ", allowed: nil), "")
    }

    func testVariousDashesAndSpacesMatch() {
        let r = NameResolver()
        let allowed = ["Jean Luc"]
        // em dash, en dash, ascii dash, comma
        XCTAssertEqual(r.resolve(transcript: "Jean—Luc", allowed: allowed), "Jean Luc")
        XCTAssertEqual(r.resolve(transcript: "Jean–Luc", allowed: allowed), "Jean Luc")
        XCTAssertEqual(r.resolve(transcript: "Jean-Luc", allowed: allowed), "Jean Luc")
        XCTAssertEqual(r.resolve(transcript: "Jean,  Luc", allowed: allowed), "Jean Luc")
    }

    func testDiacriticsAndCaseFold() {
        let r = NameResolver()
        let allowed = ["Jose", "Ana Maria"]
        XCTAssertEqual(r.resolve(transcript: "José", allowed: allowed), "Jose")
        XCTAssertEqual(r.resolve(transcript: "ANA   MARÍA", allowed: allowed), "Ana Maria")
    }
}```
Tests/VoiceKitCoreTests/NameResolverTests.swift
```swift
//
//  NameResolverTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore

final class NameResolverTests: XCTestCase {

    func testReturnsCleanedRawWhenNoAllowedList() {
        let r = NameResolver()
        XCTAssertEqual(r.resolve(transcript: "  Zoë  ", allowed: nil), "Zoë")
        XCTAssertEqual(r.resolve(transcript: "  Max! ", allowed: []), "Max")
    }

    func testStrictExactMatchesWithDiacriticsAndCase() {
        let r = NameResolver()
        let allowed = ["Zoe", "Andre", "Ana Maria"]
        XCTAssertEqual(r.resolve(transcript: "Zoë", allowed: allowed), "Zoe")
        XCTAssertEqual(r.resolve(transcript: "  ANA   MARIA ", allowed: allowed), "Ana Maria")
        XCTAssertNil(r.resolve(transcript: "Ann", allowed: allowed))
    }

    func testPunctuationAndWhitespaceAreCollapsed() {
        let r = NameResolver()
        let allowed = ["Jean Luc"]
        XCTAssertEqual(r.resolve(transcript: "Jean‑Luc", allowed: allowed), "Jean Luc")
        XCTAssertEqual(r.resolve(transcript: "  Jean, Luc  ", allowed: allowed), "Jean Luc")
    }
}
```
Tests/VoiceKitCoreTests/RecognitionContextTests.swift
```swift
//
//  RecognitionContextTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore

final class RecognitionContextTests: XCTestCase {

    func testNumericContextualStringsContainDigitsAndWords() {
        let set = Set(RecognitionContext.numericContextualStrings)
        XCTAssertTrue(set.contains("7"))
        XCTAssertTrue(set.contains("seventy"))
        XCTAssertTrue(set.contains("90"))
        XCTAssertTrue(set.contains("hundred"))
        XCTAssertGreaterThan(set.count, 20)
    }
}
```
Tests/VoiceKitCoreTests/ScriptedVoiceIOBehaviorTests.swift
```swift
//
//  ScriptedVoiceIOBehaviorTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore

@MainActor
final class ScriptedVoiceIOBehaviorTests: XCTestCase {

    private func b64(_ array: [String]) -> String {
        let data = try! JSONSerialization.data(withJSONObject: array, options: [])
        return data.base64EncodedString()
    }

    func testSpeakEmitsPulseAndTogglesSpeaking() async throws {
        guard let io = ScriptedVoiceIO(fromBase64: b64([])) else { return XCTFail("init failed") }

        let started = expectation(description: "speaking started")
        let ended = expectation(description: "speaking ended")
        var sawPulse = false

        io.onTTSSpeakingChanged = { isSpeaking in
            if isSpeaking { started.fulfill() } else { ended.fulfill() }
        }
        io.onTTSPulse = { level in
            if level > 0 { sawPulse = true }
        }

        await io.speak("hello")
        await fulfillment(of: [started, ended], timeout: 1.0)
        XCTAssertTrue(sawPulse, "Expected at least one non-zero pulse")
    }

    func testListenDequeuesAndFiresCallbacks() async throws {
        guard let io = ScriptedVoiceIO(fromBase64: b64(["first"])) else { return XCTFail("init failed") }

        let began = expectation(description: "listening began")
        let updated = expectation(description: "transcript updated")

        io.onListeningChanged = { if $0 { began.fulfill() } }
        io.onTranscriptChanged = { if $0 == "first" { updated.fulfill() } }

        let res = try await io.listen(timeout: 1, inactivity: 0.3, record: false)
        XCTAssertEqual(res.transcript, "first")
        await fulfillment(of: [began, updated], timeout: 1.0)
    }

    func testStopAllCancelsInFlightListenEpoch() async throws {
        guard let io = ScriptedVoiceIO(fromBase64: b64(["will-cancel"])) else { return XCTFail("init failed") }

        let begin = expectation(description: "listening began")
        io.onListeningChanged = { if $0 { begin.fulfill() } }

        // Run the listen task on the main actor to keep isolation simple.
        let task = Task { @MainActor in
            try? await io.listen(timeout: 2, inactivity: 1, record: false)
        }

        await fulfillment(of: [begin], timeout: 0.5)
        // Cancel the in-flight listen quickly to flip epoch
        io.stopAll()

        _ = await task.value
        // If we reached here without crash, epoch-based cancel worked.
        XCTAssertTrue(true)
    }
}
```
Tests/VoiceKitCoreTests/ScriptedVoiceIOEdgeTests.swift
```swift
//
//  ScriptedVoiceIOEdgeTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore

@MainActor
final class ScriptedVoiceIOEdgeTests: XCTestCase {

    private func base64(of strings: [String]) -> String {
        let data = try! JSONSerialization.data(withJSONObject: strings, options: [])
        return data.base64EncodedString()
    }

    func testListenReturnsEmptyWhenQueueEmpty() async throws {
        guard let voice = ScriptedVoiceIO(fromBase64: base64(of: [])) else {
            return XCTFail("Failed to build ScriptedVoiceIO")
        }

        let began = expectation(description: "listening began")
        let ended = expectation(description: "listening ended")
        voice.onListeningChanged = { isOn in
            if isOn { began.fulfill() } else { ended.fulfill() }
        }

        let result = try await voice.listen(timeout: 0.8, inactivity: 0.2, record: false)
        XCTAssertEqual(result.transcript, "")
        await fulfillment(of: [began, ended], timeout: 1.0)
    }

    func testTwoSpeaksToggleSpeakingAndEmitPulse() async throws {
        guard let voice = ScriptedVoiceIO(fromBase64: base64(of: [])) else {
            return XCTFail("Failed to build ScriptedVoiceIO")
        }

        let startedFirst = expectation(description: "speak 1 started")
        let endedFirst = expectation(description: "speak 1 ended")
        let startedSecond = expectation(description: "speak 2 started")
        let endedSecond = expectation(description: "speak 2 ended")

        var startCount = 0
        var endCount = 0
        var sawAnyPulse = false

        voice.onTTSSpeakingChanged = { speaking in
            if speaking {
                if startCount == 0 { startedFirst.fulfill() }
                else if startCount == 1 { startedSecond.fulfill() }
                startCount += 1
            } else {
                if endCount == 0 { endedFirst.fulfill() }
                else if endCount == 1 { endedSecond.fulfill() }
                endCount += 1
            }
        }
        voice.onTTSPulse = { level in if level > 0 { sawAnyPulse = true } }

        await voice.speak("first")
        await voice.speak("second")

        await fulfillment(of: [startedFirst, endedFirst, startedSecond, endedSecond], timeout: 2.0)
        XCTAssertTrue(sawAnyPulse, "Expected at least one non-zero pulse across speaks")
    }
}
```
Tests/VoiceKitCoreTests/ScriptedVoiceIOPackageTests.swift
```swift
//
//  ScriptedVoiceIOPackageTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore

@MainActor
final class ScriptedVoiceIOPackageTests: XCTestCase {

    private func base64(of strings: [String]) -> String {
        let data = try! JSONSerialization.data(withJSONObject: strings, options: [])
        return data.base64EncodedString()
    }

    func testInitFromBase64() {
        XCTAssertNotNil(ScriptedVoiceIO(fromBase64: base64(of: ["hello"])))
        XCTAssertNil(ScriptedVoiceIO(fromBase64: "not-base64"))
    }

    func testListenDequeuesInOrder() async throws {
        guard let voice = ScriptedVoiceIO(fromBase64: base64(of: ["alpha", "beta"])) else {
            return XCTFail("Failed to construct ScriptedVoiceIO")
        }
        let firstResult = try await voice.listen(timeout: 1.5, inactivity: 0.4, record: false)
        XCTAssertEqual(firstResult.transcript, "alpha")
        let secondResult = try await voice.listen(timeout: 1.5, inactivity: 0.4, record: false)
        XCTAssertEqual(secondResult.transcript, "beta")
    }
}
```
Tests/VoiceKitCoreTests/VoiceChorusTests.swift
```swift
//
//  VoiceChorusTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-16-2025
//

import XCTest
import VoiceKitCore
import CoreGraphics

@MainActor
final class VoiceChorusTests: XCTestCase {

    @MainActor
    final class MockEngine: VoiceIO, TTSConfigurable {
        var onListeningChanged: ((Bool) -> Void)?
        var onTranscriptChanged: ((String) -> Void)?
        var onLevelChanged: ((CGFloat) -> Void)?
        var onTTSSpeakingChanged: ((Bool) -> Void)?
        var onTTSPulse: ((CGFloat) -> Void)?
        var onStatusMessageChanged: ((String?) -> Void)?

        var log: [String] = []
        var stopped = false
        var speakDelay: UInt64 = 80_000_000 // Default 80ms delay for speaking

        // Track profiles to test custom voice configurations
        var appliedProfiles: [String: TTSVoiceProfile] = [:]

        func ensurePermissions() async throws {}
        func configureSessionIfNeeded() async throws {}

        func speak(_ text: String) async {
            log.append("speak:\(text):noID")
            try? await Task.sleep(nanoseconds: speakDelay)
        }

        // Track speak with ID or profile for test validation
        func speak(_ text: String, using voiceID: String?) async {
            log.append("speak:\(text):\(voiceID ?? "nil")")
            try? await Task.sleep(nanoseconds: speakDelay)
        }
        
        func setVoiceProfile(_ profile: TTSVoiceProfile) {
            appliedProfiles[profile.id] = profile
        }

        func availableVoices() -> [TTSVoiceInfo] { [] }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { nil }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) {}
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { nil }
        func setMasterControl(_ control: TTSMasterControl) {}
        func getMasterControl() -> TTSMasterControl { .init() }
        func stopSpeakingNow() {}
        func prepareBoosted(url: URL, gainDB: Float) async throws {}
        func startPreparedBoosted() async throws {}
        func playBoosted(url: URL, gainDB: Float) async throws {}
        func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
            VoiceResult(transcript: "", recordingURL: nil)
        }

        func stopAll() { stopped = true; log.append("stopAll") }
        func hardReset() {}
    }
/*
    func testChorusRunsEnginesInParallel() async {
        let chorus = VoiceChorus { MockEngine() }
        let start = Date()
        
        let voices = [
            SingableVoice(voiceID: "com.apple.voice.alex"),
            SingableVoice(profile: TTSVoiceProfile(id: "1", displayName: "Alex Custom", rate: 0.5, pitch: 1.2, volume: 1)),
            SingableVoice(voiceID: "com.apple.voice.emma")
        ]
        await chorus.sing("Hello, World!", with: voices)

        let elapsed = Date().timeIntervalSince(start)

        // Parallel engines should complete significantly faster than the sum of individual delays
        XCTAssertLessThan(elapsed, 0.22)
        print("Engines ran in parallel and completed in ~\(elapsed) seconds.")
    }

    func testStopCancelsParallelEngineTasks() async {
        let chorus = VoiceChorus { MockEngine() }
        let tasks = Task {
            await chorus.sing("Testing stop...", with: [
                SingableVoice(voiceID: "com.apple.voice.joe"),
                SingableVoice(voiceID: "com.apple.voice.susan")
            ])
        }
        
        // Allow some time for tasks to start
        try? await Task.sleep(nanoseconds: 20_000_000)

        chorus.stop()  // Should stop all ongoing speeches
        await tasks.value

        // We can't directly inspect the internal engine state, but test passes if no crash and `stopAll` was called
        XCTAssertTrue(true, "Chorus stopped without crashing.")
    }
 */
}
```
Tests/VoiceKitCoreTests/VoiceOpGateForceClearTests.swift
```swift
//
//  VoiceOpGateForceClearTests.swift
//  VoiceKit
//
//  Created by robert on 9/14/25.
//


//
//  VoiceOpGateForceClearTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore

final class VoiceOpGateForceClearTests: XCTestCase {

    func testForceClearAllowsImmediateAcquire() async {
        let gate = VoiceOpGate()
        await gate.acquire()
        await gate.forceClear() // forget lock without waiting
        // Should not block
        await gate.acquire()
        await gate.release()
        XCTAssertTrue(true)
    }
}```
Tests/VoiceKitCoreTests/VoiceOpGateTests.swift
```swift
//
//  VoiceOpGateTests.swift
//  VoiceKit
//
//  Created by robert on 9/13/25.
//


//
//  VoiceOpGateTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore

final class VoiceOpGateTests: XCTestCase {

    func testGateSerializesAccess() async throws {
        let gate = VoiceOpGate()
        let start = Date()

        async let first: Void = {
            await gate.acquire()
            try? await Task.sleep(nanoseconds: 120_000_000)
            await gate.release()
        }()

        // Stagger second acquire slightly later; it must wait for the first to release.
        try? await Task.sleep(nanoseconds: 10_000_000)
        async let second: Void = {
            await gate.acquire()
            // Record elapsed once we enter; it should be >= ~120ms.
            let elapsed = Date().timeIntervalSince(start)
            XCTAssertGreaterThanOrEqual(elapsed, 0.11, "Second acquire should wait for first to release")
            await gate.release()
        }()

        _ = await (first, second)
    }
}```
Tests/VoiceKitCoreTests/VoiceQueueTests.swift
```swift
//
//  VoiceQueueTests.swift
//  VoiceKitCoreTests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-16-2025
//

import XCTest
import VoiceKitCore

@MainActor
final class VoiceQueueTests: XCTestCase {

    // A tiny fake engine to validate sequencing without AV/Speech.
    @MainActor
    final class FakeIO: VoiceIO, TTSConfigurable {
        var onListeningChanged: ((Bool) -> Void)?
        var onTranscriptChanged: ((String) -> Void)?
        var onLevelChanged: ((CGFloat) -> Void)?
        var onTTSSpeakingChanged: ((Bool) -> Void)?
        var onTTSPulse: ((CGFloat) -> Void)?
        var onStatusMessageChanged: ((String?) -> Void)?

        var log: [String] = []
        var stopped = false

        func ensurePermissions() async throws {}
        func configureSessionIfNeeded() async throws {}

        func speak(_ text: String) async {
            log.append("speak:\(text)")
            try? await Task.sleep(nanoseconds: 40_000_000)
        }

        // TTSConfigurable
        func availableVoices() -> [TTSVoiceInfo] { [] }
        func setVoiceProfile(_ profile: TTSVoiceProfile) {}
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { nil }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) {}
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { nil }
        func setMasterControl(_ master: TTSMasterControl) {}
        func getMasterControl() -> TTSMasterControl { .init() }
        func speak(_ text: String, using voiceID: String?) async {
            log.append("speak:\(text):\(voiceID ?? "nil")")
            try? await Task.sleep(nanoseconds: 40_000_000)
        }
        func stopSpeakingNow() { log.append("stopSpeakingNow") }

        func prepareBoosted(url: URL, gainDB: Float) async throws { log.append("prepare:\(url.lastPathComponent):\(gainDB)") }
        func startPreparedBoosted() async throws { log.append("startPrepared") }
        func playBoosted(url: URL, gainDB: Float) async throws { log.append("play:\(url.lastPathComponent):\(gainDB)") }

        func listen(timeout: TimeInterval, inactivity: TimeInterval, record: Bool) async throws -> VoiceResult {
            return VoiceResult(transcript: "", recordingURL: nil)
        }

        func stopAll() { stopped = true; log.append("stopAll") }
        func hardReset() { log.append("hardReset") }
    }

    private func tempURL(_ name: String) -> URL {
        FileManager.default.temporaryDirectory.appendingPathComponent(name + ".caf")
    }

    func testSpeakThenSFXIsPreScheduledAndStarted() async {
        let io = FakeIO()
        let q = VoiceQueue(primary: io)

        let url = tempURL("ding")
        q.enqueue(.speak(text: "Hello", voiceID: "v1"))
        q.enqueue(.sfx(url: url, gainDB: 6))

        await q.play()
        // Expect prepare before speak, then startPrepared after speak
        XCTAssertEqual(io.log, [
            "prepare:ding.caf:6.0",
            "speak:Hello:v1",
            "startPrepared"
        ])
    }

    func testSFXWithoutPriorSpeakUsesPlayBoosted() async {
        let io = FakeIO()
        let q = VoiceQueue(primary: io)
        let url = tempURL("pop")

        q.enqueue(.sfx(url: url, gainDB: 3))
        await q.play()

        XCTAssertEqual(io.log, ["play:pop.caf:3.0"])
    }

    func testPauseWaitsBetweenItems() async {
        let io = FakeIO()
        let q = VoiceQueue(primary: io)
        let start = Date()
        q.enqueue(.speak(text: "A", voiceID: nil))
        q.enqueue(.pause(seconds: 0.08))
        q.enqueue(.speak(text: "B", voiceID: nil))
        await q.play()
        let elapsed = Date().timeIntervalSince(start)
        XCTAssertGreaterThanOrEqual(elapsed, 0.11)
    }

    func testMultiChannelRunsConcurrentlyWhenFactoryProvided() async {
        let io0 = FakeIO()
        let q = VoiceQueue(primary: io0) {
            return FakeIO()
        }
        q.enqueue(.speak(text: "ch0-1", voiceID: nil), on: 0)
        q.enqueue(.speak(text: "ch0-2", voiceID: nil), on: 0)

        q.enqueue(.speak(text: "ch1-1", voiceID: nil), on: 1)
        q.enqueue(.speak(text: "ch1-2", voiceID: nil), on: 1)

        let t = Date()
        await q.play()
        let elapsed = Date().timeIntervalSince(t)
        // Two channels of ~80ms total each should overlap, completing under ~0.18s
        XCTAssertLessThan(elapsed, 0.18)
    }

    func testCancelAllStopsFurtherProcessing() async {
        let io = FakeIO()
        let q = VoiceQueue(primary: io)
        let url = tempURL("bell")

        q.enqueue(.speak(text: "A", voiceID: nil))
        q.enqueue(.sfx(url: url, gainDB: 4))
        q.enqueue(.speak(text: "B", voiceID: nil))

        // Start playback and cancel quickly
        Task { await q.play() }
        try? await Task.sleep(nanoseconds: 20_000_000)
        q.cancelAll()

        // Allow tasks to unwind
        try? await Task.sleep(nanoseconds: 80_000_000)

        // We should see stopAll and not necessarily the full sequence
        XCTAssertTrue(io.log.contains("stopAll"))
    }

    func testEmbeddedSFXParsingQueuesParts() async {
        let io = FakeIO()
        let q = VoiceQueue(primary: io)
        let ding = tempURL("ding")
        let resolver: VoiceQueue.SFXResolver = { name in
            if name == "ding" { return ding }
            return nil
        }

        q.enqueueParsingSFX(text: "Hello [sfx:ding] world", resolver: resolver, defaultVoiceID: "vX")
        await q.play()

        // Should be: prepare(ding) -> speak("Hello ") -> startPrepared -> speak(" world")
        XCTAssertEqual(io.log, [
            "prepare:ding.caf:0.0",
            "speak:Hello :vX",
            "startPrepared",
            "speak: world:vX"
        ])
    }
}
```
Tests/VoiceKitUITests/VoicePickerApplyAndPersistTests.swift
```swift
//
//  VoicePickerApplyAndPersistTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerApplyAndPersistTests: XCTestCase {

    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        var profiles: [String: TTSVoiceProfile] = [:]
        var defaultProfile: TTSVoiceProfile?
        var master: TTSMasterControl = .init()
        var setVoiceProfileCalls: [String] = []
        var setDefaultProfileCalls: [String] = []

        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) { profiles[profile.id] = profile; setVoiceProfileCalls.append(profile.id) }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { profiles[id] }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile; setDefaultProfileCalls.append(profile.id) }
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
        func setMasterControl(_ master: TTSMasterControl) { self.master = master }
        func getMasterControl() -> TTSMasterControl { master }
        func speak(_ text: String, using voiceID: String?) async {}
        func stopSpeakingNow() {}
    }

    private func currentLangPrefix() -> String {
        let tag = Locale.preferredLanguages.first ?? Locale.current.identifier
        return tag.split(separator: "-").first.map(String.init)?.lowercased() ?? "en"
    }

    func testUpdateProfilePersistsAndAppliesToDefault() {
        let lang = currentLangPrefix()
        let tts = FakeTTS()
        tts.voices = [ TTSVoiceInfo(id: "voice1", name: "Alpha", language: "\(lang)-US") ]

        let store = VoiceProfilesStore(filename: "apply-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        vm.setDefaultVoice(id: "voice1")
        var updated = store.profile(for: tts.voices[0])
        updated.rate = 0.77
        vm.updateProfile(updated)

        guard let stored = store.profilesByID["voice1"] else {
            return XCTFail("Missing stored profile for voice1")
        }
        XCTAssertEqual(stored.rate, 0.77 as Float, accuracy: 0.0001 as Float)

        guard let applied = tts.defaultProfile else {
            return XCTFail("Missing default profile applied to TTS")
        }
        XCTAssertEqual(applied.rate, 0.77 as Float, accuracy: 0.0001 as Float)
        XCTAssertTrue(tts.setDefaultProfileCalls.contains("voice1"))
    }

    func testUpdateProfileAppliesToNonDefaultViaSetVoiceProfile() {
        let lang = currentLangPrefix()
        let tts = FakeTTS()
        tts.voices = [
            TTSVoiceInfo(id: "voice1", name: "Alpha", language: "\(lang)-US"),
            TTSVoiceInfo(id: "voice2", name: "Beta",  language: "\(lang)-US")
        ]
        let store = VoiceProfilesStore(filename: "apply2-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        vm.setDefaultVoice(id: "voice1")
        var nonDefaultProfile = store.profile(for: tts.voices[1])
        nonDefaultProfile.pitch = 1.33
        vm.updateProfile(nonDefaultProfile)

        guard let stored = store.profilesByID["voice2"] else {
            return XCTFail("Missing stored profile for voice2")
        }
        XCTAssertEqual(stored.pitch, 1.33 as Float, accuracy: 0.0001 as Float)
        XCTAssertTrue(tts.setVoiceProfileCalls.contains("voice2"))
    }
}
```
Tests/VoiceKitUITests/VoicePickerBootstrappingTests.swift
```swift
//
//  VoicePickerBootstrappingTests.swift
//  VoiceKit
//
//  Created by robert on 9/14/25.
//


//
//  VoicePickerBootstrappingTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerBootstrappingTests: XCTestCase {

    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        var profiles: [String: TTSVoiceProfile] = [:]
        var defaultProfile: TTSVoiceProfile?
        var master: TTSMasterControl = .init()
        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) { profiles[profile.id] = profile }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { profiles[id] }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile }
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
        func setMasterControl(_ master: TTSMasterControl) { self.master = master }
        func getMasterControl() -> TTSMasterControl { master }
        func speak(_ text: String, using voiceID: String?) async {}
        func stopSpeakingNow() {}
    }

    private func currentLangPrefix() -> String {
        let tag = Locale.preferredLanguages.first ?? Locale.current.identifier
        return tag.split(separator: "-").first.map(String.init)?.lowercased() ?? "en"
    }

    func testBootstrapSetsDefaultToFirstFilteredVoice() {
        let lang = currentLangPrefix()
        let tts = FakeTTS()
        tts.voices = [
            TTSVoiceInfo(id: "vA", name: "Alpha", language: "\(lang)-US"),
            TTSVoiceInfo(id: "vB", name: "Beta", language: "\(lang)-GB")
        ]
        let store = VoiceProfilesStore(filename: "boot-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        XCTAssertEqual(vm.voices.count, 2)
        XCTAssertNotNil(store.defaultVoiceID)
        XCTAssertEqual(store.defaultVoiceID, vm.filteredVoices.first?.id)
    }

    func testCurrentLanguageDisplayNameIsNonEmptyHumanReadable() {
        let tts = FakeTTS()
        let store = VoiceProfilesStore(filename: "boot2-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        XCTAssertFalse(vm.currentLanguageDisplayName.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty)
    }

    func testSamplePhraseFallsBackToAlexWhenNameEmpty() {
        let tts = FakeTTS()
        let store = VoiceProfilesStore(filename: "boot3-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        let nameless = TTSVoiceProfile(id: "v", displayName: "", rate: 0.5, pitch: 1.0, volume: 1.0)
        let phrase = vm.samplePhrase(for: nameless)
        XCTAssertTrue(phrase.contains("My name is Alex"), "Expected fallback name 'Alex' in sample phrase")
    }
}```
Tests/VoiceKitUITests/VoicePickerFilteringTests.swift
```swift
//
//  VoicePickerFilteringTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerFilteringTests: XCTestCase {

    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) {}
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { nil }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) {}
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { nil }
        func setMasterControl(_ master: TTSMasterControl) {}
        func getMasterControl() -> TTSMasterControl { .init() }
        func speak(_ text: String, using voiceID: String?) async {}
        func stopSpeakingNow() {}
    }

    private func currentLangPrefix() -> String {
        let tag = Locale.preferredLanguages.first ?? Locale.current.identifier
        return tag.split(separator: "-").first.map(String.init)?.lowercased() ?? "en"
    }

    func testLanguageFilterAndHidden() async {
        let lang = currentLangPrefix()
        let other = lang == "en" ? "fr" : "en"

        let fake = FakeTTS()
        fake.voices = [
            TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(lang)-US"),
            TTSVoiceInfo(id: "v2", name: "Beta", language: "\(lang)-GB"),
            TTSVoiceInfo(id: "v3", name: "Gamma", language: "\(other)-FR")
        ]

        let store = VoiceProfilesStore(filename: "filter-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: fake, store: store)

        // Default filter is .current
        XCTAssertEqual(vm.filteredVoices.count, 2, "Should show only current-language voices")

        // Hide one of the current-language voices
        store.setHidden("v2", true)
        XCTAssertEqual(vm.filteredVoices.map(\.id).sorted(), ["v1"], "Hidden voice should be excluded by default")

        // Show hidden toggle reveals it
        vm.showHidden = true
        XCTAssertEqual(Set(vm.filteredVoices.map(\.id)), Set(["v1","v2"]), "Hidden voice should appear when showHidden is true")

        // Switch to .all shows all three
        vm.languageFilter = .all
        XCTAssertEqual(Set(vm.filteredVoices.map(\.id)), Set(["v1","v2","v3"]))
    }
}
```
Tests/VoiceKitUITests/VoicePickerPreviewBehaviorTests.swift
```swift
//
//  VoicePickerPreviewBehaviorTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerPreviewBehaviorTests: XCTestCase {

    // Minimal fake TTS engine that records calls
    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        var lastSpeakText: String?
        var lastSpeakVoiceID: String?
        var stopSpeakingCount = 0
        var profiles: [String: TTSVoiceProfile] = [:]
        var defaultProfile: TTSVoiceProfile?
        var master: TTSMasterControl = .init()

        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) { profiles[profile.id] = profile }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { profiles[id] }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile }
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
        func setMasterControl(_ master: TTSMasterControl) { self.master = master }
        func getMasterControl() -> TTSMasterControl { master }
        func speak(_ text: String, using voiceID: String?) async { lastSpeakText = text; lastSpeakVoiceID = voiceID }
        func stopSpeakingNow() { stopSpeakingCount += 1 }
    }

    private func currentLanguagePrefix() -> String {
        let preferred = Locale.preferredLanguages.first ?? Locale.current.identifier
        return preferred.split(separator: "-").first.map(String.init)?.lowercased() ?? "en"
    }

    // Small async polling helper to wait for a condition without flakiness.
    private func waitFor(_ what: String, timeout: TimeInterval = 1.0, poll: TimeInterval = 0.02, until condition: @escaping () -> Bool) async {
        let deadline = Date().addingTimeInterval(timeout)
        while Date() < deadline {
            if condition() { return }
            try? await Task.sleep(nanoseconds: UInt64(poll * 1_000_000_000))
        }
        XCTFail("Timed out waiting for \(what)")
    }

    func testFavoriteVoiceUsedForPreview() async {
        let lang = currentLanguagePrefix()
        let tts = FakeTTS()
        tts.voices = [
            TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(lang)-US"),
            TTSVoiceInfo(id: "v2", name: "Beta", language: "\(lang)-US")
        ]
        let store = VoiceProfilesStore(filename: "preview-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        vm.setDefaultVoice(id: "v2")
        vm.updateMaster(TTSMasterControl(volume: 1.1, pitchVariation: 0, rateVariation: 0),
                        previewKind: "at volume 1.10")

        await waitFor("debounced preview speak") {
            tts.lastSpeakVoiceID == "v2" && (tts.lastSpeakText?.contains("at volume 1.10") == true)
        }
        XCTAssertEqual(tts.lastSpeakVoiceID, "v2")
        XCTAssertTrue(tts.lastSpeakText?.contains("at volume 1.10") == true)
    }

    func testPlayPreviewCancelsPrevious() async {
        let lang = currentLanguagePrefix()
        let tts = FakeTTS()
        tts.voices = [ TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(lang)-US") ]
        let store = VoiceProfilesStore(filename: "preview2-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        vm.playPreview(phrase: "One", voiceID: "v1")
        await waitFor("first speak") { tts.lastSpeakText == "One" }

        vm.playPreview(phrase: "Two", voiceID: "v1")
        await waitFor("second speak") { tts.lastSpeakText == "Two" }

        // Each playPreview issues a stop + new speak
        XCTAssertEqual(tts.stopSpeakingCount, 2)
        XCTAssertEqual(tts.lastSpeakText, "Two")
    }

    func testDebounceTakesLastUpdate() async {
        let lang = currentLanguagePrefix()
        let tts = FakeTTS()
        tts.voices = [ TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(lang)-US") ]
        let store = VoiceProfilesStore(filename: "preview3-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        vm.updateMaster(TTSMasterControl(volume: 1.0, pitchVariation: 0, rateVariation: 0), previewKind: "first")
        vm.updateMaster(TTSMasterControl(volume: 1.2, pitchVariation: 0, rateVariation: 0), previewKind: "second")

        await waitFor("debounced last preview") { tts.lastSpeakText?.contains("second") == true }
        XCTAssertTrue(tts.lastSpeakText?.contains("second") == true)
    }
}
```
Tests/VoiceKitUITests/VoicePickerPreviewSelectionTests.swift
```swift
//
//  VoicePickerPreviewSelectionTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerPreviewSelectionTests: XCTestCase {

    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        var lastSpeakText: String?
        var lastSpeakVoiceID: String?
        var profiles: [String: TTSVoiceProfile] = [:]
        var defaultProfile: TTSVoiceProfile?
        var master: TTSMasterControl = .init()
        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) { profiles[profile.id] = profile }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { profiles[id] }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile }
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
        func setMasterControl(_ master: TTSMasterControl) { self.master = master }
        func getMasterControl() -> TTSMasterControl { master }
        func speak(_ text: String, using voiceID: String?) async { lastSpeakText = text; lastSpeakVoiceID = voiceID }
        func stopSpeakingNow() {}
    }

    private func currentLanguagePrefix() -> String {
        let preferred = Locale.preferredLanguages.first ?? Locale.current.identifier
        return preferred.split(separator: "-").first.map(String.init)?.lowercased() ?? "en"
    }

    func testShowHiddenAffectsFilteredVoices() async {
        let lang = currentLanguagePrefix()
        let tts = FakeTTS()
        tts.voices = [
            TTSVoiceInfo(id: "vh", name: "Hidden", language: "\(lang)-US"),
            TTSVoiceInfo(id: "vs", name: "Shown",  language: "\(lang)-US")
        ]
        let store = VoiceProfilesStore(filename: "hidden-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        store.setHidden("vh", true)
        // Default is showHidden = false, so only vs appears
        XCTAssertEqual(vm.filteredVoices.map(\.id), ["vs"])

        // Now allow hidden items to be shown
        vm.showHidden = true
        XCTAssertEqual(Set(vm.filteredVoices.map(\.id)), Set(["vh","vs"]))
    }
}
```
Tests/VoiceKitUITests/VoicePickerViewModelExtrasTests.swift
```swift
//
//  VoicePickerViewModelExtrasTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerViewModelExtrasTests: XCTestCase {

    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        var profiles: [String: TTSVoiceProfile] = [:]
        var defaultProfile: TTSVoiceProfile?
        var master: TTSMasterControl = .init()
        var stopSpeakingCount = 0

        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) { profiles[profile.id] = profile }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { profiles[id] }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile }
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
        func setMasterControl(_ master: TTSMasterControl) { self.master = master }
        func getMasterControl() -> TTSMasterControl { master }
        func speak(_ text: String, using voiceID: String?) async { }
        func stopSpeakingNow() { stopSpeakingCount += 1 }
    }

    private func currentLanguagePrefix() -> String {
        let preferred = Locale.preferredLanguages.first ?? Locale.current.identifier
        return preferred.split(separator: "-").first.map(String.init)?.lowercased() ?? "en"
    }

    func testSamplePhraseIncludesDisplayNameAndSuffix() {
        let tts = FakeTTS()
        let store = VoiceProfilesStore(filename: "extras-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        let profile = TTSVoiceProfile(id: "v", displayName: "Charlie", rate: 0.5, pitch: 1.0, volume: 1.0)
        let phrase = vm.samplePhrase(for: profile, suffix: "at speed 0.50")
        XCTAssertTrue(phrase.contains("Charlie"))
        XCTAssertTrue(phrase.contains("at speed 0.50"))
    }

    func testStopPreviewStopsSpeaking() {
        let lang = currentLanguagePrefix()
        let tts = FakeTTS()
        tts.voices = [ TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(lang)-US") ]
        let store = VoiceProfilesStore(filename: "extras2-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: tts, store: store)

        vm.stopPreview()
        XCTAssertEqual(tts.stopSpeakingCount, 1)
    }
}
```
Tests/VoiceKitUITests/VoicePickerViewModelTests.swift
```swift
//
//  VoicePickerViewModelTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoicePickerViewModelTests: XCTestCase {

    // Fake TTS engine conforming to TTSConfigurable
    @MainActor
    final class FakeTTS: TTSConfigurable {
        var voices: [TTSVoiceInfo] = []
        var profiles: [String: TTSVoiceProfile] = [:]
        var defaultProfile: TTSVoiceProfile?
        var master: TTSMasterControl = .init()

        var lastSpeakText: String?
        var lastSpeakVoiceID: String?
        var setDefaultCalls: [String] = []
        var setProfileCalls: [String] = []
        var setMasterCalls = 0
        var stopSpeakingCount = 0

        func availableVoices() -> [TTSVoiceInfo] { voices }
        func setVoiceProfile(_ profile: TTSVoiceProfile) { profiles[profile.id] = profile; setProfileCalls.append(profile.id) }
        func getVoiceProfile(id: String) -> TTSVoiceProfile? { profiles[id] }
        func setDefaultVoiceProfile(_ profile: TTSVoiceProfile) { defaultProfile = profile; setDefaultCalls.append(profile.id) }
        func getDefaultVoiceProfile() -> TTSVoiceProfile? { defaultProfile }
        func setMasterControl(_ master: TTSMasterControl) { self.master = master; setMasterCalls += 1 }
        func getMasterControl() -> TTSMasterControl { master }
        func speak(_ text: String, using voiceID: String?) async {
            lastSpeakText = text
            lastSpeakVoiceID = voiceID
        }
        func stopSpeakingNow() { stopSpeakingCount += 1 }
    }

    private func currentLang() -> String {
        let tag = Locale.preferredLanguages.first ?? Locale.current.identifier
        let code = tag.split(separator: "-").first.map(String.init) ?? "en"
        return code.lowercased()
    }

    func testBootstrapAndApplyToTTS() async {
        let langPrefix = currentLang()
        let fake = FakeTTS()
        fake.voices = [
            TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(langPrefix)-US"),
            TTSVoiceInfo(id: "v2", name: "Beta", language: "\(langPrefix)-GB")
        ]

        let store = VoiceProfilesStore(filename: "vm-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: fake, store: store)

        // Should load voices and set up profiles
        XCTAssertEqual(vm.voices.count, 2)
        XCTAssertNotNil(store.defaultVoiceID)

        // One setMasterControl call happened during init via applyToTTS()
        XCTAssertEqual(fake.setMasterCalls, 1)

        // Apply changes to TTS again
        vm.applyToTTS()
        XCTAssertGreaterThan(fake.setProfileCalls.count, 0)
        XCTAssertEqual(fake.setMasterCalls, 2)

        if let defID = store.defaultVoiceID {
            XCTAssertTrue(fake.setDefaultCalls.contains(defID))
        }
    }

    func testUpdateProfileAndDefault() async {
        let langPrefix = currentLang()
        let fake = FakeTTS()
        fake.voices = [ TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(langPrefix)-US") ]

        let store = VoiceProfilesStore(filename: "vm-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: fake, store: store)

        // Make v1 default
        vm.setDefaultVoice(id: "v1")
        XCTAssertTrue(fake.setDefaultCalls.contains("v1"))

        // Change the profile; should call setDefaultVoiceProfile on fake for the default
        var p = store.profile(for: fake.voices[0])
        p.rate = 0.66
        vm.updateProfile(p)

        guard let rate: Float = fake.defaultProfile?.rate else {
            return XCTFail("Missing default profile after update")
        }
        XCTAssertEqual(rate, 0.66 as Float, accuracy: 0.0001 as Float)
    }

    func testUpdateMasterTriggersDebouncedPreviewSpeak() async {
        let langPrefix = currentLang()
        let fake = FakeTTS()
        fake.voices = [
            TTSVoiceInfo(id: "v1", name: "Alpha", language: "\(langPrefix)-US")
        ]

        let store = VoiceProfilesStore(filename: "vm-\(UUID().uuidString).json")
        let vm = VoicePickerViewModel(tts: fake, store: store)

        // Trigger master update with previewKind suffix text
        vm.updateMaster(TTSMasterControl(volume: 1.1, pitchVariation: 0.0, rateVariation: 0.0),
                        previewKind: "at volume 1.10")

        // Debounce is ~160ms; wait slightly longer
        try? await Task.sleep(nanoseconds: 250_000_000)
        XCTAssertNotNil(fake.lastSpeakText)
        XCTAssertTrue(fake.lastSpeakText?.contains("at volume 1.10") == true)
        XCTAssertEqual(fake.lastSpeakVoiceID, store.defaultVoiceID)
    }
}
```
Tests/VoiceKitUITests/VoiceProfilesStoreBootstrapTests.swift
```swift
//
//  VoiceProfilesStoreBootstrapTests.swift
//  VoiceKit
//
//  Created by robert on 9/14/25.
//


//
//  VoiceProfilesStoreBootstrapTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoiceProfilesStoreBootstrapTests: XCTestCase {

    func testProfileForCreatesAndPersists() {
        let filename = "bootstrap-\(UUID().uuidString).json"
        let store1 = VoiceProfilesStore(filename: filename)
        let voiceInfo = TTSVoiceInfo(id: "v.bootstrap", name: "Bootstrap Voice", language: "en-US")

        // First access creates a default profile
        var profile = store1.profile(for: voiceInfo)
        XCTAssertEqual(profile.displayName, "Bootstrap Voice")
        profile.displayName = "Custom Name"
        profile.rate = 0.62
        profile.pitch = 1.15
        profile.volume = 0.75
        store1.setProfile(profile)
        store1.toggleActive(voiceInfo.id)
        store1.setHidden(voiceInfo.id, true)
        store1.save()

        // Reload and verify
        let store2 = VoiceProfilesStore(filename: filename)
        guard let loaded = store2.profilesByID[voiceInfo.id] else {
            return XCTFail("Profile did not persist")
        }
        XCTAssertEqual(loaded.displayName, "Custom Name")
        XCTAssertEqual(loaded.rate, 0.62 as Float, accuracy: 0.0001 as Float)
        XCTAssertEqual(loaded.pitch, 1.15 as Float, accuracy: 0.0001 as Float)
        XCTAssertEqual(loaded.volume, 0.75 as Float, accuracy: 0.0001 as Float)
        XCTAssertTrue(store2.isActive(voiceInfo.id))
        XCTAssertTrue(store2.isHidden(voiceInfo.id))
    }
}```
Tests/VoiceKitUITests/VoiceProfilesStoreTests.swift
```swift
//
//  VoiceProfilesStoreTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-13-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoiceProfilesStoreTests: XCTestCase {

    func testSaveAndLoadProfilesAndFlags() {
        let filename = "voices-\(UUID().uuidString).json"
        let store1 = VoiceProfilesStore(filename: filename)

        let info = TTSVoiceInfo(id: "v.test", name: "Test Voice", language: "en-US")
        var profile = store1.profile(for: info)
        profile.displayName = "Custom Name"
        profile.rate = 0.7
        profile.pitch = 1.25
        profile.volume = 0.8
        store1.setProfile(profile)

        store1.defaultVoiceID = info.id
        store1.toggleActive(info.id)
        store1.setHidden(info.id, true)
        store1.master = TTSMasterControl(volume: 1.2, pitchVariation: 0.01, rateVariation: 0.02)
        store1.save()

        // Load into a new store from the same file
        let store2 = VoiceProfilesStore(filename: filename)
        XCTAssertEqual(store2.defaultVoiceID, info.id)
        XCTAssertTrue(store2.isActive(info.id))
        XCTAssertTrue(store2.isHidden(info.id))

        guard let p2 = store2.profilesByID[info.id] else {
            return XCTFail("Missing profile after reload")
        }
        XCTAssertEqual(p2.displayName, "Custom Name")
        XCTAssertEqual(p2.rate, 0.7 as Float, accuracy: 0.0001 as Float)
        XCTAssertEqual(p2.pitch, 1.25 as Float, accuracy: 0.0001 as Float)
        XCTAssertEqual(p2.volume, 0.8 as Float, accuracy: 0.0001 as Float)

        XCTAssertEqual(store2.master.volume, 1.2 as Float, accuracy: 0.0001 as Float)
        XCTAssertEqual(store2.master.pitchVariation, 0.01 as Float, accuracy: 0.000001 as Float)
        XCTAssertEqual(store2.master.rateVariation, 0.02 as Float, accuracy: 0.000001 as Float)
    }
}
```
Tests/VoiceKitUITests/VoiceProfilesStoreToggleTests.swift
```swift
//
//  VoiceProfilesStoreToggleTests.swift
//  VoiceKit
//
//  Created by robert on 9/14/25.
//


//
//  VoiceProfilesStoreToggleTests.swift
//  VoiceKitUITests
//
//  Generated by GPT-5 (OpenAI) — collaborator: rdoggett
//  date: 09-14-2025
//

import XCTest
import VoiceKitCore
import VoiceKitUI

@MainActor
final class VoiceProfilesStoreToggleTests: XCTestCase {

    func testActiveAndHiddenPersist() {
        let filename = "store-\(UUID().uuidString).json"
        let store1 = VoiceProfilesStore(filename: filename)
        let info1 = TTSVoiceInfo(id: "v.a", name: "Alpha", language: "en-US")
        let info2 = TTSVoiceInfo(id: "v.b", name: "Beta", language: "en-US")

        // Accessing profile should create defaults
        let p1 = store1.profile(for: info1)
        XCTAssertEqual(p1.displayName, "Alpha")

        // Toggle active and hidden
        store1.toggleActive(info1.id) // add
        store1.toggleActive(info2.id) // add
        store1.toggleActive(info2.id) // remove
        store1.setHidden(info1.id, true)

        store1.defaultVoiceID = info1.id
        store1.save()

        // Reload
        let store2 = VoiceProfilesStore(filename: filename)
        XCTAssertTrue(store2.isActive(info1.id))
        XCTAssertFalse(store2.isActive(info2.id))
        XCTAssertTrue(store2.isHidden(info1.id))
        XCTAssertEqual(store2.defaultVoiceID, info1.id)

        // Unhide and save again
        store2.setHidden(info1.id, false)
        store2.save()
        let store3 = VoiceProfilesStore(filename: filename)
        XCTAssertFalse(store3.isHidden(info1.id))
    }
}
```
